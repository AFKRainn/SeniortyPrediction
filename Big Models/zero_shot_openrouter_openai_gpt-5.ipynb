{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Zero-shot seniority prediction (OpenRouter)\n",
        "\n",
        "**Model**: `openai/gpt-5`\n",
        "\n",
        "This notebook runs **zero-shot** classification on a **stratified 20% test split** from `cleaned_resumes.csv`.\n",
        "\n",
        "- **Input**: the same feature columns used in your baseline (summary/experience/education/skills/projects/certs + experience time + job title)\n",
        "- **Output per row**: exactly one word: `junior` / `mid` / `senior`\n",
        "- **Metrics**: classification report + confusion matrix\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports\n",
        "\n",
        "This cell imports the minimal libraries needed for:\n",
        "- loading/splitting the data\n",
        "- calling OpenRouter\n",
        "- evaluating + plotting results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import time\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Config\n",
        "\n",
        "This cell sets:\n",
        "- the model name\n",
        "- where the CSV is\n",
        "- OpenRouter settings (API key is read from `OPENROUTER_API_KEY`)\n",
        "- batch/concurrency knobs (to avoid waiting forever)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model to evaluate (OpenRouter model id)\n",
        "MODEL = \"openai/gpt-5\"\n",
        "\n",
        "# Data (expected to be in the same folder as this notebook)\n",
        "DATA_PATH = Path(\"cleaned_resumes.csv\")\n",
        "TARGET = \"experience_level\"\n",
        "LABELS = [\"junior\", \"mid\", \"senior\"]\n",
        "\n",
        "# Use the same baseline feature columns for the LLM prompt\n",
        "FEATURE_COLS = [\n",
        "    \"summary\",\n",
        "    \"experience\",\n",
        "    \"education\",\n",
        "    \"skills\",\n",
        "    \"projects\",\n",
        "    \"certifications\",\n",
        "    \"last_experience_only\",\n",
        "    \"total_experience_time\",\n",
        "    \"last_experience_time\",\n",
        "    \"job title\",\n",
        "]\n",
        "\n",
        "# Exclude obvious IDs/metadata (baseline-like)\n",
        "ID_COLS = {\"name\", \"email\", \"linkedin\", \"github\", \"summary_count\", \"target_experience_text\"}\n",
        "\n",
        "# OpenRouter\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
        "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "\n",
        "if not OPENROUTER_API_KEY:\n",
        "    raise ValueError(\"Missing env var OPENROUTER_API_KEY\")\n",
        "\n",
        "# Speed knobs\n",
        "BATCH_SIZE = 20          # how many resumes we process at a time\n",
        "MAX_WORKERS = 8          # parallel requests inside each batch\n",
        "MAX_RETRIES = 4          # retry on transient API errors\n",
        "REQUEST_TIMEOUT_S = 90\n",
        "\n",
        "# If the model returns something invalid after retries, we use this fallback\n",
        "FALLBACK_LABEL = \"mid\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load data + build the stratified test split (20%)\n",
        "\n",
        "This mirrors your baseline setup:\n",
        "- read `cleaned_resumes.csv`\n",
        "- auto-pick feature columns (exclude IDs + target)\n",
        "- create a **stratified** 20% test split with `random_state=42`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (2100, 17)\n",
            "Selected feature columns: ['summary', 'experience', 'education', 'skills', 'projects', 'certifications', 'summary_count', 'last_experience_only', 'total_experience_time', 'last_experience_time', 'job title', 'target_experience_text']\n",
            "Test rows: 420\n",
            "experience_level\n",
            "mid       140\n",
            "junior    140\n",
            "senior    140\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "\n",
        "if len(df) != 2100:\n",
        "    raise ValueError(f\"Expected 2100 rows, got {len(df)}\")\n",
        "\n",
        "if TARGET not in df.columns:\n",
        "    raise ValueError(f\"Missing target column: {TARGET}\")\n",
        "\n",
        "# Normalize labels\n",
        "df[TARGET] = df[TARGET].astype(str).str.strip().str.lower()\n",
        "\n",
        "bad_labels = sorted(set(df[TARGET].unique()) - set(LABELS))\n",
        "if bad_labels:\n",
        "    raise ValueError(f\"Unexpected labels in {TARGET}: {bad_labels} (expected only {LABELS})\")\n",
        "\n",
        "missing_features = [c for c in FEATURE_COLS if c not in df.columns]\n",
        "if missing_features:\n",
        "    raise ValueError(f\"Missing required feature columns: {missing_features}\")\n",
        "\n",
        "# Make sure features are strings (safe for prompt building)\n",
        "for col in FEATURE_COLS:\n",
        "    df[col] = df[col].astype(str).fillna(\"\")\n",
        "\n",
        "# Stratified 20% test split (no training used)\n",
        "_, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=df[TARGET]\n",
        ")\n",
        "\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "print(\"Test rows:\", len(test_df))\n",
        "print(test_df[TARGET].value_counts())\n",
        "\n",
        "print(\"Prompt feature columns:\", FEATURE_COLS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prompt + output parsing\n",
        "\n",
        "We build a single prompt per resume and force the model to answer with **one word only**: `junior`, `mid`, or `senior`.\n",
        "\n",
        "Then we parse the returned text and normalize it to one of those 3 labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYSTEM_MSG = (\n",
        "    \"You are a strict classifier. \"\n",
        "    \"Return ONLY one word: junior, mid, or senior. \"\n",
        "    \"No punctuation. No extra text.\"\n",
        ")\n",
        "\n",
        "def build_user_msg(row: pd.Series) -> str:\n",
        "    parts = []\n",
        "    for col in FEATURE_COLS:\n",
        "        val = str(row.get(col, \"\")).strip()\n",
        "        if val:\n",
        "            parts.append(f\"{col}: {val}\")\n",
        "    resume_text = \"\\n\".join(parts)\n",
        "\n",
        "    return (\n",
        "        \"Classify the candidate's overall seniority into exactly one of: junior, mid, senior.\\n\"\n",
        "        \"Answer with ONE word only.\\n\\n\"\n",
        "        f\"{resume_text}\\n\\n\"\n",
        "        \"Answer:\"\n",
        "    )\n",
        "\n",
        "_label_re = re.compile(r\"\\b(junior|mid|senior)\\b\", re.IGNORECASE)\n",
        "\n",
        "def parse_label(text: str) -> str:\n",
        "    if not text:\n",
        "        return FALLBACK_LABEL\n",
        "\n",
        "    t = text.strip().lower()\n",
        "    # common cleanup\n",
        "    t = t.replace(\".\", \" \").replace(\",\", \" \").replace(\"\\n\", \" \")\n",
        "\n",
        "    # if the first token is a label, use it\n",
        "    first = t.split()[0] if t.split() else \"\"\n",
        "    if first in LABELS:\n",
        "        return first\n",
        "\n",
        "    # otherwise try to find any label in the output\n",
        "    m = _label_re.search(t)\n",
        "    if m:\n",
        "        return m.group(1).lower()\n",
        "\n",
        "    return FALLBACK_LABEL\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
