{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee5159d6",
   "metadata": {},
   "source": [
    "# Resume Data Cleaning Pipeline\n",
    "\n",
    "This notebook processes raw resume data from `master_resumes_original.jsonl` and produces `cleaned_resumes.csv` for model training.\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "1. **Inspect structure**: examine nested JSON of one resume\n",
    "2. **Flatten data**: convert nested JSON to DataFrame columns (personal_info → name/email/summary, experience/education/skills/projects → JSON strings)\n",
    "3. **Filter invalid dates**: remove resumes with placeholder start/end dates\n",
    "4. **Extract seniority**: calculate total experience time, identify target job's level, clear that level from data (avoid leakage)\n",
    "5. **Parse to text**: convert JSON fields to readable strings\n",
    "6. **Generate summaries**: use LLM to create professional summaries, remove seniority keywords\n",
    "7. **Balance dataset**: sample 700 per class (junior/mid/senior)\n",
    "\n",
    "## Imports\n",
    "\n",
    "- **`json`**: parse JSONL records\n",
    "- **`pandas`**: DataFrame operations\n",
    "- **`datetime`**: calculate experience durations\n",
    "- **`re`**: remove seniority keywords\n",
    "- **`urllib.request`**: call OpenRouter API\n",
    "- **`ThreadPoolExecutor`**: parallelize API calls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f540e3",
   "metadata": {},
   "source": [
    "# 1. Inspect Raw Data Structure\n",
    "\n",
    "Read the first resume and print its nested structure to understand what fields we need to flatten.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2045b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key: personal_info\n",
      "Type: dict\n",
      "Dict keys: ['name', 'email', 'phone', 'location', 'summary', 'linkedin', 'github']\n",
      "  - name: str\n",
      "  - email: str\n",
      "  - phone: str\n",
      "  - location: dict\n",
      "    Dict keys: ['city', 'country', 'remote_preference']\n",
      "  - summary: str\n",
      "  - linkedin: str\n",
      "  - github: str\n",
      "\n",
      "Key: experience\n",
      "Type: list\n",
      "List length: 1\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['company', 'company_info', 'title', 'level', 'employment_type', 'dates', 'responsibilities', 'technical_environment']\n",
      "  - company: str\n",
      "  - company_info: dict\n",
      "    Dict keys: ['industry', 'size']\n",
      "  - title: str\n",
      "  - level: str\n",
      "  - employment_type: str\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'end', 'duration']\n",
      "  - responsibilities: list\n",
      "    List length: 1, first item type: str\n",
      "  - technical_environment: dict\n",
      "    Dict keys: ['technologies', 'methodologies', 'tools']\n",
      "\n",
      "Key: education\n",
      "Type: list\n",
      "List length: 2\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['degree', 'institution', 'dates', 'achievements']\n",
      "  - degree: dict\n",
      "    Dict keys: ['level', 'field', 'major']\n",
      "  - institution: dict\n",
      "    Dict keys: ['name', 'location', 'accreditation']\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'expected_graduation']\n",
      "  - achievements: dict\n",
      "    Dict keys: ['gpa', 'honors', 'relevant_coursework']\n",
      "Item 1 type: dict\n",
      "Item 1 keys: ['degree', 'institution', 'dates', 'achievements']\n",
      "  - degree: dict\n",
      "    Dict keys: ['level', 'field', 'major']\n",
      "  - institution: dict\n",
      "    Dict keys: ['name', 'location', 'accreditation']\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'expected_graduation']\n",
      "  - achievements: dict\n",
      "    Dict keys: ['gpa', 'honors', 'relevant_coursework']\n",
      "\n",
      "Key: skills\n",
      "Type: dict\n",
      "Dict keys: ['technical', 'languages']\n",
      "  - technical: dict\n",
      "    Dict keys: ['programming_languages', 'frameworks', 'databases', 'cloud']\n",
      "  - languages: list\n",
      "    List length: 1, first item type: dict\n",
      "    First item keys: ['name', 'level']\n",
      "\n",
      "Key: projects\n",
      "Type: list\n",
      "List length: 1\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['name', 'description', 'technologies', 'role', 'url', 'impact']\n",
      "  - name: str\n",
      "  - description: str\n",
      "  - technologies: list\n",
      "    List length: 1, first item type: str\n",
      "  - role: str\n",
      "  - url: str\n",
      "  - impact: str\n",
      "\n",
      "Key: certifications\n",
      "Type: str\n",
      "Value preview: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('master_resumes_original.jsonl', 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    record = json.loads(first_line.strip())\n",
    "    \n",
    "    for key in record.keys():\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        value = record[key]\n",
    "        print(f\"Type: {type(value).__name__}\")\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            print(f\"Dict keys: {list(value.keys())}\")\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"  - {sub_key}: {type(sub_value).__name__}\")\n",
    "                if isinstance(sub_value, dict):\n",
    "                    print(f\"    Dict keys: {list(sub_value.keys())}\")\n",
    "                elif isinstance(sub_value, list) and sub_value:\n",
    "                    print(f\"    List length: {len(sub_value)}, first item type: {type(sub_value[0]).__name__}\")\n",
    "                    if isinstance(sub_value[0], dict):\n",
    "                        print(f\"    First item keys: {list(sub_value[0].keys())}\")\n",
    "        \n",
    "        elif isinstance(value, list):\n",
    "            print(f\"List length: {len(value)}\")\n",
    "            if value:\n",
    "                for idx, item in enumerate(value):\n",
    "                    print(f\"Item {idx} type: {type(item).__name__}\")\n",
    "                    if isinstance(item, dict):\n",
    "                        print(f\"Item {idx} keys: {list(item.keys())}\")\n",
    "                        for sub_key, sub_value in item.items():\n",
    "                            print(f\"  - {sub_key}: {type(sub_value).__name__}\")\n",
    "                            if isinstance(sub_value, dict):\n",
    "                                print(f\"    Dict keys: {list(sub_value.keys())}\")\n",
    "                            elif isinstance(sub_value, list) and sub_value:\n",
    "                                print(f\"    List length: {len(sub_value)}, first item type: {type(sub_value[0]).__name__}\")\n",
    "        else:\n",
    "            print(f\"Value preview: {str(value)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe847252",
   "metadata": {},
   "source": [
    "# 2. Flatten Data to DataFrame\n",
    "\n",
    "`clean_resume_data`: extracts personal_info fields (name, email, summary, linkedin, github), converts nested fields (experience, education, skills, projects, certifications) to JSON strings. Reads all JSONL records, creates DataFrame, adds `summary_count`, drops `phone`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a19494b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4817 records\n",
      "Created 11 columns\n",
      "\n",
      "Column names: ['name', 'email', 'summary', 'linkedin', 'github', 'experience', 'education', 'skills', 'projects', 'certifications', 'summary_count']\n",
      "\n",
      "First few rows:\n",
      "           name         email  \\\n",
      "0       Unknown       Unknown   \n",
      "1       Unknown       Unknown   \n",
      "2  Not Provided  Not Provided   \n",
      "3       Unknown       Unknown   \n",
      "4                               \n",
      "\n",
      "                                             summary      linkedin  \\\n",
      "0  Python Developer with experience in Python, Te...       Unknown   \n",
      "1  Experienced Operations Manager with expertise ...           NaN   \n",
      "2  Software Proficiency in various languages and ...  Not Provided   \n",
      "3  Experienced Operations Manager with expertise ...           NaN   \n",
      "4                                                                    \n",
      "\n",
      "         github                                         experience  \\\n",
      "0       Unknown  [{\"company\": \"Fresher\", \"company_info\": {\"indu...   \n",
      "1           NaN  [{\"company\": \"Delta Controls, Dubai FZCO\", \"co...   \n",
      "2  Not Provided  [{\"company\": \"Parkar Consulting and Labs\", \"co...   \n",
      "3           NaN  [{\"company\": \"Delta Controls, Dubai FZCO\", \"co...   \n",
      "4                [{\"company\": \"Atos Syntel\", \"company_info\": {\"...   \n",
      "\n",
      "                                           education  \\\n",
      "0  [{\"degree\": {\"level\": \"ME\", \"field\": \"Computer...   \n",
      "1  [{\"degree\": {\"level\": \"B.E\", \"field\": \"Electro...   \n",
      "2  [{\"degree\": {\"level\": \"B.E.\", \"field\": \"Not Pr...   \n",
      "3  [{\"degree\": {\"level\": \"B.E\", \"field\": \"Electro...   \n",
      "4  [{\"degree\": {\"level\": \"Bachelor of Engineering...   \n",
      "\n",
      "                                              skills  \\\n",
      "0  {\"technical\": {\"programming_languages\": [{\"nam...   \n",
      "1  {\"technical\": {\"project_management\": [{\"name\":...   \n",
      "2  {\"technical\": {\"programming_languages\": [{\"nam...   \n",
      "3  {\"technical\": {\"project_management\": [{\"name\":...   \n",
      "4  {\"technical\": {\"programming_languages\": [{\"nam...   \n",
      "\n",
      "                                            projects  \\\n",
      "0  [{\"name\": \"Unknown\", \"description\": \"Unknown\",...   \n",
      "1  [{\"name\": \"FGP/WPMP\", \"description\": \"Led syst...   \n",
      "2  [{\"name\": \"FPGA Implementation\", \"description\"...   \n",
      "3  [{\"name\": \"FGP/WPMP\", \"description\": \"Led syst...   \n",
      "4                                                 []   \n",
      "\n",
      "                                      certifications  summary_count  \n",
      "0                                                 \"\"             14  \n",
      "1                                                 \"\"             27  \n",
      "2                                                 \"\"             22  \n",
      "3                                                 \"\"             27  \n",
      "4  \"{\\\"name\\\": \\\"ESD Program\\\", \\\"issuer\\\": \\\"Zen...              0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_resume_data(record):\n",
    "    cleaned = {}\n",
    "    personal_info = record.get('personal_info', {})\n",
    "    if isinstance(personal_info, dict):\n",
    "        for key, value in personal_info.items():\n",
    "            if key == 'location':\n",
    "                continue\n",
    "            if isinstance(value, (str, int, float, bool, type(None))):\n",
    "                cleaned[key] = value\n",
    "\n",
    "    experience = record.get('experience', [])\n",
    "    cleaned['experience'] = json.dumps(experience)\n",
    "    education = record.get('education', [])\n",
    "    cleaned['education'] = json.dumps(education)\n",
    "    skills = record.get('skills')\n",
    "    cleaned['skills'] = json.dumps(skills) if skills is not None else json.dumps({})\n",
    "    projects = record.get('projects', [])\n",
    "    cleaned['projects'] = json.dumps(projects)\n",
    "    certifications = record.get('certifications')\n",
    "    cleaned['certifications'] = json.dumps(certifications) if certifications is not None else json.dumps([])\n",
    "    return cleaned\n",
    "\n",
    "all_records = []\n",
    "with open('master_resumes_original.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            record = json.loads(line.strip())\n",
    "            cleaned_record = clean_resume_data(record)\n",
    "            all_records.append(cleaned_record)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "df = pd.DataFrame(all_records)\n",
    "df['summary_count'] = df['summary'].fillna('').apply(lambda x: len(str(x).split()))\n",
    "df = df.drop(columns=['phone'])\n",
    "\n",
    "print(f\"Processed {len(all_records)} records\")\n",
    "print(f\"Created {len(df.columns)} columns\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0009a",
   "metadata": {},
   "source": [
    "# 3. Filter Invalid Dates\n",
    "\n",
    "Remove resumes where experience has placeholder dates (\"Unknown\", \"Not Provided\", \"N/A\", \"\"). `has_invalid_start_end` checks each experience's start/end dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3940143",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_values = {\"Unknown\", \"Not Provided\", \"Not Available\", \"N/A\", \"unknown\", \"not provided\", \"\"}\n",
    "\n",
    "def has_invalid_start_end(experience_json):\n",
    "    try:\n",
    "        exp_list = json.loads(experience_json)\n",
    "        for exp in exp_list:\n",
    "            dates = exp.get(\"dates\", {})\n",
    "            start = dates.get(\"start\", \"\")\n",
    "            end = dates.get(\"end\", \"\")\n",
    "            if start in invalid_values or end in invalid_values:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        return True\n",
    "\n",
    "df = df[~df[\"experience\"].apply(has_invalid_start_end)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e01a4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows left after deletion: 4634\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total rows left after deletion: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656be01a",
   "metadata": {},
   "source": [
    "# 4. Extract Seniority and Experience Time\n",
    "\n",
    "We define helper functions and extract seniority data:\n",
    "\n",
    "- `parse_date`: converts date strings (or \"present\") to datetime objects\n",
    "- `calculate_duration`: computes years between start/end dates\n",
    "- `extract_seniority_and_experience`: for each resume, we:\n",
    "  - Calculate duration for each job and store it in `duration_calc`\n",
    "  - Normalize `level` and `title` to lowercase\n",
    "  - Identify the target experience (the one with \"present\" end date, or max end date, or highest level if same title)\n",
    "  - Extract `experience_level` from the target job **before clearing it** (to avoid leakage)\n",
    "  - Return: `experience_level`, `last_experience_only`, `total_experience_time`, `last_experience_time`, `target_job_title`, and updated experience JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79be9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "PRESENT_DATE = \"2025-12-12\"\n",
    "\n",
    "def parse_date(date_str):\n",
    "    if isinstance(date_str, str) and date_str.strip().lower() == \"present\":\n",
    "        try:\n",
    "            return datetime.strptime(PRESENT_DATE, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_duration(start_date, end_date):\n",
    "    start = parse_date(start_date)\n",
    "    end = parse_date(end_date)\n",
    "    if start and end:\n",
    "        duration_years = (end - start).days / 365.25\n",
    "        return max(0, duration_years)\n",
    "    return 0\n",
    "\n",
    "level_map = {\"junior\": 1, \"mid\": 2, \"senior\": 3}\n",
    "\n",
    "def extract_seniority_and_experience(exp_json):\n",
    "    try:\n",
    "        experiences = json.loads(exp_json)\n",
    "    except:\n",
    "        return {\n",
    "            \"experience_level\": \"unknown\",\n",
    "            \"last_experience_only\": \"N/A\",\n",
    "            \"total_experience_time\": \"0 Years\",\n",
    "            \"last_experience_time\": \"0 Years\",\n",
    "            \"target_job_title\": \"\",\n",
    "            \"updated_experience_json\": exp_json\n",
    "        }\n",
    "\n",
    "    if not isinstance(experiences, list) or len(experiences) == 0:\n",
    "        return {\n",
    "            \"experience_level\": \"unknown\",\n",
    "            \"last_experience_only\": \"N/A\",\n",
    "            \"total_experience_time\": \"0 Years\",\n",
    "            \"last_experience_time\": \"0 Years\",\n",
    "            \"target_job_title\": \"\",\n",
    "            \"updated_experience_json\": exp_json\n",
    "        }\n",
    "\n",
    "    for exp in experiences:\n",
    "        dates = exp.get('dates', {})\n",
    "        start = dates.get('start', '')\n",
    "        end = dates.get('end', '')\n",
    "        duration_val = calculate_duration(start, end)\n",
    "        exp['duration_calc'] = duration_val\n",
    "        if 'dates' not in exp:\n",
    "            exp['dates'] = {}\n",
    "        exp['dates']['duration'] = f\"{duration_val:.2f} years\"\n",
    "        if 'level' in exp and isinstance(exp['level'], str):\n",
    "            exp['level'] = exp['level'].lower()\n",
    "        if 'title' in exp and exp['title'] is not None:\n",
    "            exp['title'] = str(exp['title']).strip().lower()\n",
    "        else:\n",
    "            exp['title'] = \"\"\n",
    "\n",
    "    if len(experiences) == 1:\n",
    "        last_experience_only_flag = \"Only last Experience listed\"\n",
    "    else:\n",
    "        last_experience_only_flag = \"Multiple Experiences listed\"\n",
    "\n",
    "    if len(experiences) == 1:\n",
    "        most_recent_exp = experiences[0]\n",
    "    else:\n",
    "        # More than one experience; find which is the \"most recent\" (should be the one with \"present\", if any)\n",
    "        def is_present(exp):\n",
    "            end = exp.get('dates', {}).get('end', '')\n",
    "            return isinstance(end, str) and end.strip().lower() == \"present\"\n",
    "        # 'present' always marks the last experience\n",
    "        present_exps = [exp for exp in experiences if is_present(exp)]\n",
    "        if present_exps:\n",
    "            most_recent_exp = present_exps[0]\n",
    "        else:\n",
    "            # Fall back to max end date\n",
    "            def exp_end_date(exp):\n",
    "                return parse_date(exp.get('dates', {}).get('end', '')) or datetime.min\n",
    "            most_recent_exp = max(experiences, key=exp_end_date)\n",
    "\n",
    "        temp_exp = most_recent_exp\n",
    "        temp_title = temp_exp.get('title', '').lower()\n",
    "        temp_level = temp_exp.get('level', '').lower()\n",
    "        temp_level_rank = level_map.get(temp_level, 0)\n",
    "\n",
    "        for exp in experiences:\n",
    "            if exp is temp_exp:\n",
    "                continue\n",
    "            exp_title = exp.get('title', '').lower()\n",
    "            exp_level = exp.get('level', '').lower()\n",
    "            exp_level_rank = level_map.get(exp_level, 0)\n",
    "            if exp_title == temp_title and exp_level_rank > temp_level_rank:\n",
    "                temp_level_rank = exp_level_rank\n",
    "                temp_exp = exp\n",
    "                temp_level = exp_level\n",
    "        \n",
    "        most_recent_exp = temp_exp\n",
    "\n",
    "    experience_level = most_recent_exp.get('level', 'unknown').lower()\n",
    "    target_job_title = most_recent_exp.get('title', '')\n",
    "    most_recent_exp['level'] = \"\"\n",
    "    \n",
    "    total_experience_years = sum(exp.get('duration_calc', 0) for exp in experiences)\n",
    "    last_experience_years = most_recent_exp.get('duration_calc', 0)\n",
    "\n",
    "    # Keep only title and responsibilities for each experience\n",
    "    # cleaned_experiences = []\n",
    "    # for exp in experiences:\n",
    "    #     cleaned_exp = {}\n",
    "    #     if 'title' in exp:\n",
    "    #         cleaned_exp['title'] = exp['title']\n",
    "    #     if 'responsibilities' in exp:\n",
    "    #         cleaned_exp['responsibilities'] = exp['responsibilities']\n",
    "    #     cleaned_experiences.append(cleaned_exp)\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"experience_level\": experience_level,\n",
    "        \"last_experience_only\": last_experience_only_flag,\n",
    "        \"total_experience_time\": f\"{round(total_experience_years, 2)} Years\",\n",
    "        \"last_experience_time\": f\"{round(last_experience_years, 2)} Years\",\n",
    "        \"target_job_title\": target_job_title,\n",
    "        \"updated_experience_json\": json.dumps(experiences)\n",
    "    }\n",
    "\n",
    "df_results = df['experience'].apply(extract_seniority_and_experience)\n",
    "df['experience_level'] = df_results.apply(lambda x: x['experience_level'])\n",
    "df['last_experience_only'] = df_results.apply(lambda x: x['last_experience_only'])\n",
    "df['total_experience_time'] = df_results.apply(lambda x: x['total_experience_time'])\n",
    "df['last_experience_time'] = df_results.apply(lambda x: x['last_experience_time'])\n",
    "df['job title'] = df_results.apply(lambda x: x['target_job_title'])\n",
    "df['experience'] = df_results.apply(lambda x: x.get('updated_experience_json', df['experience'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f7d69bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "def _parse_date_flexible(date_str):\n",
    "    if not isinstance(date_str, str):\n",
    "        return None\n",
    "    s = date_str.strip()\n",
    "    if not s:\n",
    "        return None\n",
    "    if s.lower() == \"present\":\n",
    "        try:\n",
    "            return datetime.strptime(PRESENT_DATE, \"%Y-%m-%d\")\n",
    "        except Exception:\n",
    "            return None\n",
    "    for fmt in (\"%Y-%m-%d\", \"%Y/%m/%d\", \"%Y-%m\", \"%Y/%m\", \"%Y\", \"%b %Y\", \"%B %Y\"):\n",
    "        try:\n",
    "            return datetime.strptime(s, fmt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return None\n",
    "\n",
    "def _format_like(dt, template):\n",
    "    if not isinstance(template, str):\n",
    "        return dt.strftime(\"%Y-%m-%d\")\n",
    "    t = template.strip()\n",
    "    if t.lower() == \"present\":\n",
    "        return \"Present\"\n",
    "    if len(t) == 4 and t.isdigit():\n",
    "        return dt.strftime(\"%Y\")\n",
    "    if \"-\" in t:\n",
    "        parts = t.split(\"-\")\n",
    "        if len(parts) == 2:\n",
    "            return dt.strftime(\"%Y-%m\")\n",
    "        if len(parts) >= 3:\n",
    "            return dt.strftime(\"%Y-%m-%d\")\n",
    "    if \"/\" in t:\n",
    "        parts = t.split(\"/\")\n",
    "        if len(parts) == 2:\n",
    "            return dt.strftime(\"%Y/%m\")\n",
    "        if len(parts) >= 3:\n",
    "            return dt.strftime(\"%Y/%m/%d\")\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def _fix_experience_json(experience_json):\n",
    "    try:\n",
    "        experiences = json.loads(experience_json)\n",
    "    except Exception:\n",
    "        return experience_json\n",
    "\n",
    "    if not isinstance(experiences, list) or len(experiences) == 0:\n",
    "        return experience_json\n",
    "\n",
    "    for exp in experiences:\n",
    "        if not isinstance(exp, dict):\n",
    "            continue\n",
    "        exp.pop(\"employment_type\", None)\n",
    "        exp.pop(\"employmentType\", None)\n",
    "\n",
    "    for i in range(len(experiences) - 2, -1, -1):\n",
    "        cur = experiences[i]\n",
    "        nxt = experiences[i + 1]\n",
    "        if not isinstance(cur, dict) or not isinstance(nxt, dict):\n",
    "            continue\n",
    "\n",
    "        cur_dates = cur.get(\"dates\") if isinstance(cur.get(\"dates\"), dict) else {}\n",
    "        nxt_dates = nxt.get(\"dates\") if isinstance(nxt.get(\"dates\"), dict) else {}\n",
    "        cur[\"dates\"] = cur_dates\n",
    "        nxt[\"dates\"] = nxt_dates\n",
    "\n",
    "        cur_end_str = cur_dates.get(\"end\", \"\")\n",
    "        nxt_start_str = nxt_dates.get(\"start\", \"\")\n",
    "\n",
    "        cur_end_dt = _parse_date_flexible(cur_end_str)\n",
    "        nxt_start_dt = _parse_date_flexible(nxt_start_str)\n",
    "        if not cur_end_dt or not nxt_start_dt:\n",
    "            continue\n",
    "\n",
    "        if cur_end_dt > nxt_start_dt:\n",
    "            dur_years = cur.get(\"duration_calc\")\n",
    "            if not isinstance(dur_years, (int, float)) or dur_years <= 0:\n",
    "                cur_start_dt = _parse_date_flexible(cur_dates.get(\"start\", \"\"))\n",
    "                if not cur_start_dt:\n",
    "                    continue\n",
    "                dur_years = max(0, (cur_end_dt - cur_start_dt).days / 365.25)\n",
    "\n",
    "            new_end_dt = nxt_start_dt\n",
    "            new_start_dt = new_end_dt - timedelta(days=int(round(dur_years * 365.25)))\n",
    "            if new_start_dt > new_end_dt:\n",
    "                new_start_dt = new_end_dt\n",
    "\n",
    "            cur_start_str = cur_dates.get(\"start\", \"\")\n",
    "            cur_dates[\"end\"] = _format_like(new_end_dt, cur_end_str)\n",
    "            cur_dates[\"start\"] = _format_like(new_start_dt, cur_start_str)\n",
    "\n",
    "    return json.dumps(experiences)\n",
    "\n",
    "\n",
    "df[\"experience\"] = df[\"experience\"].apply(_fix_experience_json)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1cf5b5",
   "metadata": {},
   "source": [
    "# 5. Parse JSON Fields to Text\n",
    "\n",
    "We convert JSON fields to readable text strings for model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7817397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_experience_to_text(experience_json):\n",
    "    try:\n",
    "        experiences = json.loads(experience_json)\n",
    "        if not experiences or not isinstance(experiences, list):\n",
    "            return \"\"\n",
    "        text_parts = []\n",
    "        for i, exp in enumerate(experiences, 1):\n",
    "            exp_text = f\"Experience {i}: \"\n",
    "            if exp.get('company'):\n",
    "                exp_text += f\"Company: {exp['company']}. \"\n",
    "            if exp.get('title'):\n",
    "                exp_text += f\"Title: {exp['title']}. \"\n",
    "            if exp.get('level'):\n",
    "                exp_text += f\"Level: {exp['level']}. \"\n",
    "            if exp.get('dates'):\n",
    "                dates = exp['dates']\n",
    "                if dates.get('start'):\n",
    "                    exp_text += f\"Start Date: {dates['start']}. \"\n",
    "                if dates.get('end'):\n",
    "                    exp_text += f\"End Date: {dates['end']}. \"\n",
    "                if dates.get('duration'):\n",
    "                    exp_text += f\"Duration: {dates['duration']}. \"\n",
    "            if exp.get('responsibilities') and isinstance(exp['responsibilities'], list):\n",
    "                responsibilities = ' '.join(exp['responsibilities'])\n",
    "                exp_text += f\"Responsibilities: {responsibilities}. \"\n",
    "            if exp.get('technical_environment'):\n",
    "                tech_env = exp['technical_environment']\n",
    "                if tech_env.get('technologies') and isinstance(tech_env['technologies'], list):\n",
    "                    technologies = ', '.join(tech_env['technologies'])\n",
    "                    exp_text += f\"Technologies: {technologies}. \"\n",
    "                if tech_env.get('methodologies') and isinstance(tech_env['methodologies'], list):\n",
    "                    methodologies = ', '.join(tech_env['methodologies'])\n",
    "                    exp_text += f\"Methodologies: {methodologies}. \"\n",
    "                if tech_env.get('tools') and isinstance(tech_env['tools'], list):\n",
    "                    tools = ', '.join(tech_env['tools'])\n",
    "                    exp_text += f\"Tools: {tools}. \"\n",
    "            if exp.get('company_info'):\n",
    "                company_info = exp['company_info']\n",
    "                if company_info.get('industry'):\n",
    "                    exp_text += f\"Industry: {company_info['industry']}. \"\n",
    "                if company_info.get('size'):\n",
    "                    exp_text += f\"Company Size: {company_info['size']}. \"\n",
    "            text_parts.append(exp_text.strip())\n",
    "        return ' '.join(text_parts)\n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "df['experience'] = df['experience'].apply(parse_experience_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d308bdc",
   "metadata": {},
   "source": [
    "`parse_education_to_text`: converts education JSON to text (degree, institution, dates, achievements).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d26c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_education_to_text(education_str):\n",
    "    try:\n",
    "        if pd.isna(education_str) or education_str == '':\n",
    "            return \"\"\n",
    "        education_list = json.loads(education_str)\n",
    "        if not isinstance(education_list, list):\n",
    "            return \"\"\n",
    "        text_parts = []\n",
    "        for idx, edu in enumerate(education_list, 1):\n",
    "            if not isinstance(edu, dict):\n",
    "                continue\n",
    "            edu_text = f\"Education {idx}: \"\n",
    "            if edu.get('degree'):\n",
    "                degree = edu['degree']\n",
    "                if degree.get('level'):\n",
    "                    edu_text += f\"Degree Level: {degree['level']}. \"\n",
    "                if degree.get('field'):\n",
    "                    edu_text += f\"Field: {degree['field']}. \"\n",
    "                if degree.get('major'):\n",
    "                    edu_text += f\"Major: {degree['major']}. \"\n",
    "            if edu.get('institution'):\n",
    "                institution = edu['institution']\n",
    "                if institution.get('name'):\n",
    "                    edu_text += f\"Institution: {institution['name']}. \"\n",
    "                if institution.get('location'):\n",
    "                    edu_text += f\"Location: {institution['location']}. \"\n",
    "                if institution.get('accreditation'):\n",
    "                    edu_text += f\"Accreditation: {institution['accreditation']}. \"\n",
    "            if edu.get('dates'):\n",
    "                dates = edu['dates']\n",
    "                if dates.get('start'):\n",
    "                    edu_text += f\"Start Date: {dates['start']}. \"\n",
    "                if dates.get('end'):\n",
    "                    edu_text += f\"End Date: {dates['end']}. \"\n",
    "                if dates.get('expected_graduation'):\n",
    "                    edu_text += f\"Expected Graduation: {dates['expected_graduation']}. \"\n",
    "            if edu.get('achievements'):\n",
    "                achievements = edu['achievements']\n",
    "                if achievements.get('gpa'):\n",
    "                    edu_text += f\"GPA: {achievements['gpa']}. \"\n",
    "                if achievements.get('honors'):\n",
    "                    edu_text += f\"Honors: {achievements['honors']}. \"\n",
    "                if achievements.get('relevant_coursework') and isinstance(achievements['relevant_coursework'], list):\n",
    "                    coursework = ', '.join(achievements['relevant_coursework'])\n",
    "                    if coursework:\n",
    "                        edu_text += f\"Relevant Coursework: {coursework}. \"\n",
    "            text_parts.append(edu_text.strip())\n",
    "        return ' '.join(text_parts)\n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "df['education'] = df['education'].apply(parse_education_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a991bc3",
   "metadata": {},
   "source": [
    "`parse_skills_to_text`: converts skills JSON to text (programming languages, frameworks, databases, cloud, spoken languages).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a8cf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_skills_to_text(skills_json):\n",
    "    if pd.isna(skills_json) or skills_json == '':\n",
    "        return \"\"\n",
    "    try:\n",
    "        if isinstance(skills_json, str):\n",
    "            skills = json.loads(skills_json)\n",
    "        else:\n",
    "            skills = skills_json\n",
    "        text_parts = []\n",
    "        if skills.get('technical'):\n",
    "            technical = skills['technical']\n",
    "            if technical.get('programming_languages') and isinstance(technical['programming_languages'], list):\n",
    "                for lang in technical['programming_languages']:\n",
    "                    if lang.get('name'):\n",
    "                        lang_text = f\"Programming Language: {lang['name']}\"\n",
    "                        if lang.get('level'):\n",
    "                            lang_text += f\" (Level: {lang['level']})\"\n",
    "                        text_parts.append(lang_text + \". \")\n",
    "            if technical.get('frameworks') and isinstance(technical['frameworks'], list):\n",
    "                for framework in technical['frameworks']:\n",
    "                    if framework.get('name'):\n",
    "                        framework_text = f\"Framework: {framework['name']}\"\n",
    "                        if framework.get('level'):\n",
    "                            framework_text += f\" (Level: {framework['level']})\"\n",
    "                        text_parts.append(framework_text + \". \")\n",
    "            if technical.get('databases') and isinstance(technical['databases'], list):\n",
    "                for db in technical['databases']:\n",
    "                    if db.get('name'):\n",
    "                        db_text = f\"Database: {db['name']}\"\n",
    "                        if db.get('level'):\n",
    "                            db_text += f\" (Level: {db['level']})\"\n",
    "                        text_parts.append(db_text + \". \")\n",
    "            if technical.get('cloud') and isinstance(technical['cloud'], list):\n",
    "                for cloud in technical['cloud']:\n",
    "                    if isinstance(cloud, dict) and cloud.get('name'):\n",
    "                        cloud_text = f\"Cloud: {cloud['name']}\"\n",
    "                        if cloud.get('level'):\n",
    "                            cloud_text += f\" (Level: {cloud['level']})\"\n",
    "                        text_parts.append(cloud_text + \". \")\n",
    "                    elif isinstance(cloud, str):\n",
    "                        text_parts.append(f\"Cloud: {cloud}. \")\n",
    "        if skills.get('languages') and isinstance(skills['languages'], list):\n",
    "            for language in skills['languages']:\n",
    "                if isinstance(language, dict) and language.get('name'):\n",
    "                    lang_text = f\"Language: {language['name']}\"\n",
    "                    if language.get('level'):\n",
    "                        lang_text += f\" (Level: {language['level']})\"\n",
    "                    text_parts.append(lang_text + \". \")\n",
    "                elif isinstance(language, str):\n",
    "                    text_parts.append(f\"Language: {language}. \")\n",
    "        return ' '.join(text_parts).strip()\n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "df['skills'] = df['skills'].apply(parse_skills_to_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2ec547",
   "metadata": {},
   "source": [
    "`parse_projects_to_text`: converts projects JSON to text (name, description, technologies, role, URL, impact). Returns \"No Projects made\" if empty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdb668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_projects_to_text(projects_str):\n",
    "    try:\n",
    "        if pd.isna(projects_str) or str(projects_str).strip() == '' or str(projects_str).strip() == '[]':\n",
    "            return \"No Projects made\"\n",
    "        projects = json.loads(projects_str)\n",
    "        if not projects or not isinstance(projects, list) or len(projects) == 0:\n",
    "            return \"No Projects made\"\n",
    "        text_parts = []\n",
    "        for idx, project in enumerate(projects, 1):\n",
    "            if not isinstance(project, dict):\n",
    "                continue\n",
    "            project_text = f\"Project {idx}: \"\n",
    "            if project.get('name'):\n",
    "                project_text += f\"{project['name']}. \"\n",
    "            if project.get('description'):\n",
    "                project_text += f\"Description: {project['description']}. \"\n",
    "            if project.get('technologies') and isinstance(project['technologies'], list):\n",
    "                tech_list = [tech for tech in project['technologies'] if tech]\n",
    "                if tech_list:\n",
    "                    project_text += f\"Technologies: {', '.join(tech_list)}. \"\n",
    "            if project.get('role'):\n",
    "                project_text += f\"Role: {project['role']}. \"\n",
    "            if project.get('url'):\n",
    "                project_text += f\"URL: {project['url']}. \"\n",
    "            if project.get('impact'):\n",
    "                project_text += f\"Impact: {project['impact']}. \"\n",
    "            \n",
    "            text_parts.append(project_text.strip())\n",
    "        if not text_parts:\n",
    "            return \"No Projects made\"\n",
    "        return ' '.join(text_parts).strip()\n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"No Projects made\"\n",
    "\n",
    "df['projects'] = df['projects'].apply(parse_projects_to_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4956216",
   "metadata": {},
   "source": [
    "# 6. Clean Up\n",
    "\n",
    "We drop the `certifications` column (no data) and remove rows with empty `summary`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b553f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('certifications', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bdb164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4626\n"
     ]
    }
   ],
   "source": [
    "df = df[df['summary'].astype(str).str.strip() != '']\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912de9e",
   "metadata": {},
   "source": [
    "# 7. Generate Professional Summaries Using LLM\n",
    "\n",
    "We generate new summaries using OpenRouter API (Mistral 3B) based on resume content - **excluding personal info and seniority level**. Summaries are cached in `generated_summaries.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89fc0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will generate new summaries (file not found or LOAD_EXISTING_SUMMARIES=False)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "LOAD_EXISTING_SUMMARIES = True\n",
    "summaries_file = 'generated_summaries.json'\n",
    "\n",
    "if LOAD_EXISTING_SUMMARIES and os.path.exists(summaries_file):\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as f:\n",
    "        loaded_summaries = json.load(f)\n",
    "    new_summaries = {int(k): v for k, v in loaded_summaries.items()}\n",
    "    print(f\"Loaded {len(new_summaries)} existing summaries from {summaries_file}\")\n",
    "    SKIP_GENERATION = True\n",
    "else:\n",
    "    SKIP_GENERATION = False\n",
    "    print(f\"Will generate new summaries (file not found or LOAD_EXISTING_SUMMARIES=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MODEL = \"google/gemma-3n-e4b-it\"\n",
    "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "MAX_WORKERS = 10\n",
    "EXCLUDE_COLS = {\"name\", \"email\", \"linkedin\", \"github\", \"experience_level\", \"summary\", \"summary_count\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4209d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_prompt(row):\n",
    "    parts = []\n",
    "    for col in row.index:\n",
    "        if col not in EXCLUDE_COLS:\n",
    "            val = str(row[col]).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(f\"{col}: {val}\")\n",
    "    resume_text = \"\\n\".join(parts)\n",
    "    return f\"\"\"You are the person described in this resume. Write a professional summary for your resume in first person that captures who you are as a professional.\n",
    "\n",
    "    {resume_text}\n",
    "\n",
    "    Focus on what makes your experience distinctive. Let the summary flow naturally - if you have deep expertise in one area, lean into that. If your career spans diverse technologies, show that breadth. If you've worked on interesting projects, bring them to life. The summary should feel authentic to your specific background, not generic.\n",
    "\n",
    "    Write only the summary paragraph in text format only. min words 20, max words 70. Try to base the lengh on the resume data itself, if there is a lot to talk about, mention them, if not, try to be as short and concise as possible. for example, one expereince is not something to talk about much, but multiple can get you to talk more. be as simular as possible to the data and nothing from outside.\"\"\"\n",
    "\n",
    "def call_api(prompt):\n",
    "    payload = json.dumps({\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        # \"max_tokens\": 150\n",
    "    }).encode(\"utf-8\")\n",
    "    req = urllib.request.Request(\n",
    "        OPENROUTER_URL,\n",
    "        data=payload,\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    )\n",
    "    with urllib.request.urlopen(req, timeout=60) as resp:\n",
    "        result = json.loads(resp.read().decode(\"utf-8\"))\n",
    "    return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def generate_summary_for_row(idx_row):\n",
    "    idx, row = idx_row\n",
    "    try:\n",
    "        prompt = build_summary_prompt(row)\n",
    "        summary = call_api(prompt)\n",
    "        return idx, summary\n",
    "    except Exception as e:\n",
    "        print(f\"Row {idx} failed: {e}\")\n",
    "        return idx, row.get(\"summary\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ddad489a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries for 4626 rows...\n",
      "Processed 100/4626\n",
      "Processed 200/4626\n",
      "Processed 300/4626\n",
      "Processed 400/4626\n",
      "Processed 500/4626\n",
      "Processed 600/4626\n",
      "Processed 700/4626\n",
      "Processed 800/4626\n",
      "Processed 900/4626\n",
      "Processed 1000/4626\n",
      "Processed 1100/4626\n",
      "Processed 1200/4626\n",
      "Processed 1300/4626\n",
      "Processed 1400/4626\n",
      "Processed 1500/4626\n",
      "Processed 1600/4626\n",
      "Processed 1700/4626\n",
      "Processed 1800/4626\n",
      "Processed 1900/4626\n",
      "Processed 2000/4626\n",
      "Row 2025 failed: IncompleteRead(11 bytes read)\n",
      "Processed 2100/4626\n",
      "Processed 2200/4626\n",
      "Processed 2300/4626\n",
      "Row 2409 failed: IncompleteRead(11 bytes read)\n",
      "Processed 2400/4626\n",
      "Processed 2500/4626\n",
      "Processed 2600/4626\n",
      "Processed 2700/4626\n",
      "Processed 2800/4626\n",
      "Processed 2900/4626\n",
      "Processed 3000/4626\n",
      "Processed 3100/4626\n",
      "Processed 3200/4626\n",
      "Processed 3300/4626\n",
      "Processed 3400/4626\n",
      "Processed 3500/4626\n",
      "Processed 3600/4626\n",
      "Processed 3700/4626\n",
      "Processed 3800/4626\n",
      "Processed 3900/4626\n",
      "Processed 4000/4626\n",
      "Processed 4100/4626\n",
      "Processed 4200/4626\n",
      "Processed 4300/4626\n",
      "Processed 4400/4626\n",
      "Processed 4500/4626\n",
      "Processed 4600/4626\n",
      "Done! Generated 4626 summaries\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_GENERATION:\n",
    "    print(f\"Generating summaries for {len(df)} rows...\")\n",
    "    new_summaries = {}\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(generate_summary_for_row, (idx, row)): idx for idx, row in df.iterrows()}\n",
    "        for i, future in enumerate(as_completed(futures), 1):\n",
    "            idx, summary = future.result()\n",
    "            new_summaries[idx] = summary\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processed {i}/{len(df)}\")\n",
    "    print(f\"Done! Generated {len(new_summaries)} summaries\")\n",
    "else:\n",
    "    print(\"Skipped generation - using loaded summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3f71b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4626 summaries to generated_summaries.json\n"
     ]
    }
   ],
   "source": [
    "if not SKIP_GENERATION:\n",
    "    summaries_file = 'generated_summaries.json'\n",
    "    with open(summaries_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump({str(k): v for k, v in new_summaries.items()}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(new_summaries)} summaries to {summaries_file}\")\n",
    "else:\n",
    "    print(\"Summaries loaded from file - no need to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5df53335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample generated summaries:\n",
      "================================================================================\n",
      "\n",
      "[1] Highly motivated and results-oriented Java Developer with 10.45 years of experience in developing and maintaining robust Java-based web applications. Proven ability to leverage Spring, Hibernate, and MySQL within Agile environments.  I am proficient in front-end development using various technologie...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2] Highly accomplished Project Manager with 11+ years at AT&T, leading complex transition and operational projects within the telecommunications industry. Proven expertise in vendor management, risk mitigation, and driving quality through automation and process improvements. Skilled in Agile methodolog...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3] Highly accomplished Advocate with 13.32 years of experience advising clients on legal rights and representing them in courts across various jurisdictions, including international companies. Proven expertise in legal research, document drafting (bail petitions, appeals, contracts, etc.), and complex ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df[\"summary\"] = df.index.map(new_summaries)\n",
    "df[\"summary_count\"] = df[\"summary\"].fillna(\"\").apply(len)\n",
    "\n",
    "print(\"Sample generated summaries:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n[{i+1}] {df.iloc[i]['summary'][:300]}...\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a31edb56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed seniority keywords from summaries\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_seniority_keywords(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return text\n",
    "    text = re.sub(r'\\b(junior|senior|mid)\\b', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"summary\"] = df[\"summary\"].apply(remove_seniority_keywords)\n",
    "print(\"Removed seniority keywords from summaries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb470c2",
   "metadata": {},
   "source": [
    "# 9. Reorder Columns\n",
    "\n",
    "We put informative features first (experience, projects, skills, summary, education), noise last (name, email, linkedin, github)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e42c98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns reordered:\n",
      "['experience', 'projects', 'skills', 'summary', 'education', 'job title', 'total_experience_time', 'last_experience_time', 'summary_count', 'last_experience_only', 'experience_level', 'name', 'email', 'linkedin', 'github']\n"
     ]
    }
   ],
   "source": [
    "preferred_first = [\n",
    "    \"experience\", \"projects\", \"skills\", \"summary\", \"education\",\n",
    "    \"certifications\", \"job title\",\n",
    "    \"total_experience_time\", \"last_experience_time\", \"summary_count\", \"last_experience_only\"\n",
    "]\n",
    "preferred_last = [\"name\", \"email\", \"linkedin\", \"github\"]\n",
    "\n",
    "all_columns = df.columns.tolist()\n",
    "first_cols = [col for col in preferred_first if col in all_columns]\n",
    "last_cols = [col for col in preferred_last if col in all_columns]\n",
    "middle_cols = [col for col in all_columns if col not in first_cols and col not in last_cols]\n",
    "\n",
    "new_column_order = first_cols + middle_cols + last_cols\n",
    "df = df[new_column_order]\n",
    "\n",
    "print(\"Columns reordered:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6323f8a9",
   "metadata": {},
   "source": [
    "# 10. Save Full Dataset\n",
    "\n",
    "We save the complete cleaned dataset to `cleaned_resumes.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e562e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned_resumes.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee73585",
   "metadata": {},
   "source": [
    "# 11. Balance Dataset (700 per class)\n",
    "\n",
    "We select 700 samples per class: for **seniors/mids** take top 700 most experienced, for **juniors** random sample. Save to all model directories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d045df8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_years(time_str):\n",
    "    try:\n",
    "        return float(str(time_str).replace(' Years', '').strip())\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "senior_df = df[df['experience_level'] == 'senior'].copy()\n",
    "mid_df = df[df['experience_level'] == 'mid'].copy()\n",
    "junior_df = df[df['experience_level'] == 'junior']\n",
    "\n",
    "senior_df['_exp_years'] = senior_df['total_experience_time'].apply(extract_years)\n",
    "mid_df['_exp_years'] = mid_df['total_experience_time'].apply(extract_years)\n",
    "\n",
    "senior_sample = senior_df.sort_values('_exp_years', ascending=False).head(700).drop(columns=['_exp_years'])\n",
    "mid_sample = mid_df.sort_values('_exp_years', ascending=False).head(700).drop(columns=['_exp_years'])\n",
    "junior_sample = junior_df.sample(n=min(700, len(junior_df)), random_state=42)\n",
    "\n",
    "balanced_df = pd.concat([senior_sample, mid_sample, junior_sample], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b3ed737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2100 records to ./Baseline/cleaned_resumes.csv\n",
      "\n",
      "Experience level distribution:\n",
      "experience_level\n",
      "senior    700\n",
      "mid       700\n",
      "junior    700\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "balanced_df.to_csv('./Baseline/cleaned_resumes.csv', index=False)\n",
    "balanced_df.to_csv('./Smaller Models/cleaned_resumes.csv', index=False)\n",
    "balanced_df.to_csv('./Big Models/cleaned_resumes.csv', index=False)\n",
    "\n",
    "print(f\"Saved {len(balanced_df)} records to ./Baseline/cleaned_resumes.csv\")\n",
    "print(f\"\\nExperience level distribution:\")\n",
    "print(balanced_df['experience_level'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
