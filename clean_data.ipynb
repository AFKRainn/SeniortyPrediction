{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2045b438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Key: personal_info\n",
      "Type: dict\n",
      "Dict keys: ['name', 'email', 'phone', 'location', 'summary', 'linkedin', 'github']\n",
      "  - name: str\n",
      "  - email: str\n",
      "  - phone: str\n",
      "  - location: dict\n",
      "    Dict keys: ['city', 'country', 'remote_preference']\n",
      "  - summary: str\n",
      "  - linkedin: str\n",
      "  - github: str\n",
      "\n",
      "Key: experience\n",
      "Type: list\n",
      "List length: 1\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['company', 'company_info', 'title', 'level', 'employment_type', 'dates', 'responsibilities', 'technical_environment']\n",
      "  - company: str\n",
      "  - company_info: dict\n",
      "    Dict keys: ['industry', 'size']\n",
      "  - title: str\n",
      "  - level: str\n",
      "  - employment_type: str\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'end', 'duration']\n",
      "  - responsibilities: list\n",
      "    List length: 1, first item type: str\n",
      "  - technical_environment: dict\n",
      "    Dict keys: ['technologies', 'methodologies', 'tools']\n",
      "\n",
      "Key: education\n",
      "Type: list\n",
      "List length: 2\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['degree', 'institution', 'dates', 'achievements']\n",
      "  - degree: dict\n",
      "    Dict keys: ['level', 'field', 'major']\n",
      "  - institution: dict\n",
      "    Dict keys: ['name', 'location', 'accreditation']\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'expected_graduation']\n",
      "  - achievements: dict\n",
      "    Dict keys: ['gpa', 'honors', 'relevant_coursework']\n",
      "Item 1 type: dict\n",
      "Item 1 keys: ['degree', 'institution', 'dates', 'achievements']\n",
      "  - degree: dict\n",
      "    Dict keys: ['level', 'field', 'major']\n",
      "  - institution: dict\n",
      "    Dict keys: ['name', 'location', 'accreditation']\n",
      "  - dates: dict\n",
      "    Dict keys: ['start', 'expected_graduation']\n",
      "  - achievements: dict\n",
      "    Dict keys: ['gpa', 'honors', 'relevant_coursework']\n",
      "\n",
      "Key: skills\n",
      "Type: dict\n",
      "Dict keys: ['technical', 'languages']\n",
      "  - technical: dict\n",
      "    Dict keys: ['programming_languages', 'frameworks', 'databases', 'cloud']\n",
      "  - languages: list\n",
      "    List length: 1, first item type: dict\n",
      "    First item keys: ['name', 'level']\n",
      "\n",
      "Key: projects\n",
      "Type: list\n",
      "List length: 1\n",
      "Item 0 type: dict\n",
      "Item 0 keys: ['name', 'description', 'technologies', 'role', 'url', 'impact']\n",
      "  - name: str\n",
      "  - description: str\n",
      "  - technologies: list\n",
      "    List length: 1, first item type: str\n",
      "  - role: str\n",
      "  - url: str\n",
      "  - impact: str\n",
      "\n",
      "Key: certifications\n",
      "Type: str\n",
      "Value preview: \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Read the JSONL file and inspect the structure of each key\n",
    "with open('master_resumes_original.jsonl', 'r', encoding='utf-8') as f:\n",
    "    first_line = f.readline()\n",
    "    record = json.loads(first_line.strip())\n",
    "    \n",
    "    for key in record.keys():\n",
    "        print(f\"\\nKey: {key}\")\n",
    "        value = record[key]\n",
    "        \n",
    "        # Show the type and structure\n",
    "        print(f\"Type: {type(value).__name__}\")\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            print(f\"Dict keys: {list(value.keys())}\")\n",
    "            # Show a sample of nested structure\n",
    "            for sub_key, sub_value in value.items():\n",
    "                print(f\"  - {sub_key}: {type(sub_value).__name__}\")\n",
    "                if isinstance(sub_value, dict):\n",
    "                    print(f\"    Dict keys: {list(sub_value.keys())}\")\n",
    "                elif isinstance(sub_value, list) and sub_value:\n",
    "                    print(f\"    List length: {len(sub_value)}, first item type: {type(sub_value[0]).__name__}\")\n",
    "                    if isinstance(sub_value[0], dict):\n",
    "                        print(f\"    First item keys: {list(sub_value[0].keys())}\")\n",
    "        \n",
    "        elif isinstance(value, list):\n",
    "            print(f\"List length: {len(value)}\")\n",
    "            if value:\n",
    "                # Show all items in the list, not just the first one\n",
    "                for idx, item in enumerate(value):\n",
    "                    print(f\"Item {idx} type: {type(item).__name__}\")\n",
    "                    if isinstance(item, dict):\n",
    "                        print(f\"Item {idx} keys: {list(item.keys())}\")\n",
    "                        # Show nested structure of each item\n",
    "                        for sub_key, sub_value in item.items():\n",
    "                            print(f\"  - {sub_key}: {type(sub_value).__name__}\")\n",
    "                            if isinstance(sub_value, dict):\n",
    "                                print(f\"    Dict keys: {list(sub_value.keys())}\")\n",
    "                            elif isinstance(sub_value, list) and sub_value:\n",
    "                                print(f\"    List length: {len(sub_value)}, first item type: {type(sub_value[0]).__name__}\")\n",
    "        \n",
    "        else:\n",
    "            print(f\"Value preview: {str(value)[:100]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a19494b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclean_resume_data\u001b[39m(record):\n\u001b[32m      4\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m    Extract and clean resume data - just parse fields simply.\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_resume_data(record):\n",
    "    \"\"\"\n",
    "    Extract and clean resume data - just parse fields simply.\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "\n",
    "    # Process personal_info\n",
    "    personal_info = record.get('personal_info', {})\n",
    "    if isinstance(personal_info, dict):\n",
    "        for key, value in personal_info.items():\n",
    "            # Skip location - we don't want it\n",
    "            if key == 'location':\n",
    "                continue\n",
    "            # Add simple values directly\n",
    "            if isinstance(value, (str, int, float, bool, type(None))):\n",
    "                cleaned[key] = value\n",
    "\n",
    "    # Simply process experience - just put the value as JSON string, no extra extraction\n",
    "    experience = record.get('experience', [])\n",
    "    cleaned['experience'] = json.dumps(experience)\n",
    "\n",
    "    # Process education - convert to JSON string\n",
    "    education = record.get('education', [])\n",
    "    cleaned['education'] = json.dumps(education)\n",
    "\n",
    "    # Process skills - convert to JSON string\n",
    "    skills = record.get('skills')\n",
    "    cleaned['skills'] = json.dumps(skills) if skills is not None else json.dumps({})\n",
    "\n",
    "    # Process projects - convert to JSON string\n",
    "    projects = record.get('projects', [])\n",
    "    cleaned['projects'] = json.dumps(projects)\n",
    "\n",
    "    # Process certifications - convert to JSON string\n",
    "    certifications = record.get('certifications')\n",
    "    cleaned['certifications'] = json.dumps(certifications) if certifications is not None else json.dumps([])\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# Read all records from JSONL and clean them\n",
    "all_records = []\n",
    "with open('master_resumes_original.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        try:\n",
    "            record = json.loads(line.strip())\n",
    "            cleaned_record = clean_resume_data(record)\n",
    "            all_records.append(cleaned_record)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_records)\n",
    "\n",
    "# get length of the 'summary' column and create a new column with summary_count\n",
    "df['summary_count'] = df['summary'].fillna('').apply(len)\n",
    "# remove the phone column\n",
    "df = df.drop(columns=['phone'])\n",
    "\n",
    "print(f\"Processed {len(all_records)} records\")\n",
    "print(f\"Created {len(df.columns)} columns\")\n",
    "print(f\"\\nColumn names: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3940143",
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_values = {\"Unknown\", \"Not Provided\", \"Not Available\", \"N/A\", \"unknown\", \"not provided\", \"\"}\n",
    "\n",
    "def has_invalid_start_end(experience_json):\n",
    "    try:\n",
    "        exp_list = json.loads(experience_json)\n",
    "        for exp in exp_list:\n",
    "            dates = exp.get(\"dates\", {})\n",
    "            start = dates.get(\"start\", \"\")\n",
    "            end = dates.get(\"end\", \"\")\n",
    "            if start in invalid_values or end in invalid_values:\n",
    "                return True\n",
    "        return False\n",
    "    except Exception:\n",
    "        # If can't parse or not list, err on the side of exclusion\n",
    "        return True\n",
    "\n",
    "df = df[~df[\"experience\"].apply(has_invalid_start_end)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total rows left after deletion: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79be9273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "PRESENT_DATE = \"2025-12-12\"\n",
    "\n",
    "def parse_date(date_str):\n",
    "    # Assumes date format is YYYY-MM-DD or \"present\"\n",
    "    if isinstance(date_str, str) and date_str.strip().lower() == \"present\":\n",
    "        try:\n",
    "            return datetime.strptime(PRESENT_DATE, \"%Y-%m-%d\")\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_duration(start_date, end_date):\n",
    "    start = parse_date(start_date)\n",
    "    end = parse_date(end_date)\n",
    "    if start and end:\n",
    "        duration_years = (end - start).days / 365.25\n",
    "        return max(0, duration_years)\n",
    "    return 0\n",
    "\n",
    "level_map = {\"junior\": 1, \"mid\": 2, \"senior\": 3}\n",
    "\n",
    "def extract_seniority_and_experience(exp_json):\n",
    "    try:\n",
    "        experiences = json.loads(exp_json)\n",
    "    except:\n",
    "        return {\n",
    "            \"experience_level\": \"unknown\",\n",
    "            \"last_experience_only\": \"N/A\",\n",
    "            \"total_experience_time\": \"0 Years\",\n",
    "            \"last_experience_time\": \"0 Years\",\n",
    "            \"target_job_title\": \"\",\n",
    "            \"updated_experience_json\": exp_json,\n",
    "            \"target_experience_json\": \"\"\n",
    "        }\n",
    "\n",
    "    if not isinstance(experiences, list) or len(experiences) == 0:\n",
    "        return {\n",
    "            \"experience_level\": \"unknown\",\n",
    "            \"last_experience_only\": \"N/A\",\n",
    "            \"total_experience_time\": \"0 Years\",\n",
    "            \"last_experience_time\": \"0 Years\",\n",
    "            \"target_job_title\": \"\",\n",
    "            \"updated_experience_json\": exp_json,\n",
    "            \"target_experience_json\": \"\"\n",
    "        }\n",
    "\n",
    "    # Preprocess: normalize levels, calculate durations and update duration field\n",
    "    for exp in experiences:\n",
    "        dates = exp.get('dates', {})\n",
    "        start = dates.get('start', '')\n",
    "        end = dates.get('end', '')\n",
    "        \n",
    "        duration_val = calculate_duration(start, end)\n",
    "        exp['duration_calc'] = duration_val\n",
    "        \n",
    "        # Update the duration field in the dates object\n",
    "        if 'dates' not in exp:\n",
    "            exp['dates'] = {}\n",
    "        exp['dates']['duration'] = f\"{duration_val:.2f} years\"\n",
    "        \n",
    "        # Lowercase `level` for robustness\n",
    "        if 'level' in exp and isinstance(exp['level'], str):\n",
    "            exp['level'] = exp['level'].lower()\n",
    "        if 'title' in exp and exp['title'] is not None:\n",
    "            exp['title'] = str(exp['title']).strip().lower()\n",
    "        else:\n",
    "            exp['title'] = \"\"\n",
    "\n",
    "    # Determine last_experience_only flag based on number of experiences\n",
    "    if len(experiences) == 1:\n",
    "        last_experience_only_flag = \"Only last Experience listed\"\n",
    "    else:\n",
    "        last_experience_only_flag = \"Multiple Experiences listed\"\n",
    "\n",
    "    # Selection Logic\n",
    "    if len(experiences) == 1:\n",
    "        most_recent_exp = experiences[0]\n",
    "    else:\n",
    "        # More than one experience; find which is the \"most recent\" (should be the one with \"present\", if any)\n",
    "        def is_present(exp):\n",
    "            end = exp.get('dates', {}).get('end', '')\n",
    "            return isinstance(end, str) and end.strip().lower() == \"present\"\n",
    "        # 'present' always marks the last experience\n",
    "        present_exps = [exp for exp in experiences if is_present(exp)]\n",
    "        if present_exps:\n",
    "            most_recent_exp = present_exps[0]\n",
    "        else:\n",
    "            # Fall back to max end date\n",
    "            def exp_end_date(exp):\n",
    "                return parse_date(exp.get('dates', {}).get('end', '')) or datetime.min\n",
    "            most_recent_exp = max(experiences, key=exp_end_date)\n",
    "\n",
    "        temp_exp = most_recent_exp\n",
    "        temp_title = temp_exp.get('title', '').lower()\n",
    "        temp_level = temp_exp.get('level', '').lower()\n",
    "        temp_level_rank = level_map.get(temp_level, 0)\n",
    "\n",
    "        for exp in experiences:\n",
    "            if exp is temp_exp:\n",
    "                continue\n",
    "            exp_title = exp.get('title', '').lower()\n",
    "            exp_level = exp.get('level', '').lower()\n",
    "            exp_level_rank = level_map.get(exp_level, 0)\n",
    "            if exp_title == temp_title and exp_level_rank > temp_level_rank:\n",
    "                temp_level_rank = exp_level_rank\n",
    "                temp_exp = exp\n",
    "                temp_level = exp_level\n",
    "        \n",
    "        most_recent_exp = temp_exp\n",
    "\n",
    "    # Extract target level BEFORE clearing it\n",
    "    experience_level = most_recent_exp.get('level', 'unknown').lower()\n",
    "    \n",
    "    # Extract target job title BEFORE any modifications\n",
    "    target_job_title = most_recent_exp.get('title', '')\n",
    "    \n",
    "    # Create a copy of the target experience WITHOUT the level field for the target_experience column\n",
    "    target_experience_copy = {}\n",
    "    if 'title' in most_recent_exp:\n",
    "        target_experience_copy['title'] = most_recent_exp['title']\n",
    "    if 'responsibilities' in most_recent_exp:\n",
    "        target_experience_copy['responsibilities'] = most_recent_exp['responsibilities']\n",
    "    \n",
    "    # -------------------------------------------------------------\n",
    "    # Clear the 'level' ONLY from the chosen experience\n",
    "    # -------------------------------------------------------------\n",
    "    most_recent_exp['level'] = \"\"\n",
    "    \n",
    "    total_experience_years = sum(exp.get('duration_calc', 0) for exp in experiences)\n",
    "    last_experience_years = most_recent_exp.get('duration_calc', 0)\n",
    "\n",
    "    # Keep only title and responsibilities for each experience\n",
    "    cleaned_experiences = []\n",
    "    for exp in experiences:\n",
    "        cleaned_exp = {}\n",
    "        if 'title' in exp:\n",
    "            cleaned_exp['title'] = exp['title']\n",
    "        if 'responsibilities' in exp:\n",
    "            cleaned_exp['responsibilities'] = exp['responsibilities']\n",
    "        cleaned_experiences.append(cleaned_exp)\n",
    "\n",
    "    # Return updated structure including the modified list (with CLEARED level for target ONLY)\n",
    "    return {\n",
    "        \"experience_level\": experience_level,\n",
    "        \"last_experience_only\": last_experience_only_flag,\n",
    "        \"total_experience_time\": f\"{round(total_experience_years, 2)} Years\",\n",
    "        \"last_experience_time\": f\"{round(last_experience_years, 2)} Years\",\n",
    "        \"target_job_title\": target_job_title,\n",
    "        \"updated_experience_json\": json.dumps(cleaned_experiences),\n",
    "        \"target_experience_json\": json.dumps(target_experience_copy)\n",
    "    }\n",
    "\n",
    "# Now apply to the DataFrame\n",
    "\n",
    "df_results = df['experience'].apply(extract_seniority_and_experience)\n",
    "df['experience_level'] = df_results.apply(lambda x: x['experience_level'])\n",
    "df['last_experience_only'] = df_results.apply(lambda x: x['last_experience_only'])\n",
    "df['total_experience_time'] = df_results.apply(lambda x: x['total_experience_time'])\n",
    "df['last_experience_time'] = df_results.apply(lambda x: x['last_experience_time'])\n",
    "df['job title'] = df_results.apply(lambda x: x['target_job_title'])\n",
    "# Update the experience column with the modified JSON (calculated duration + cleared level)\n",
    "df['experience'] = df_results.apply(lambda x: x.get('updated_experience_json', df['experience'].iloc[0]))\n",
    "# Add the target experience in a separate column\n",
    "df['target_experience'] = df_results.apply(lambda x: x.get('target_experience_json', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7817397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the experience JSON and convert to readable text format\n",
    "def parse_experience_to_text(experience_json):\n",
    "    \"\"\"\n",
    "    Parse the experience JSON and convert it to a readable text format.\n",
    "    Each experience is labeled as \"Experience 1:\", \"Experience 2:\", etc.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        experiences = json.loads(experience_json)\n",
    "        if not experiences or not isinstance(experiences, list):\n",
    "            return \"\"\n",
    "        \n",
    "        text_parts = []\n",
    "        for i, exp in enumerate(experiences, 1):\n",
    "            exp_text = f\"Experience {i}: \"\n",
    "            \n",
    "            # Add company\n",
    "            if exp.get('company'):\n",
    "                exp_text += f\"Company: {exp['company']}. \"\n",
    "            \n",
    "            # Add title\n",
    "            if exp.get('title'):\n",
    "                exp_text += f\"Title: {exp['title']}. \"\n",
    "            \n",
    "            # Add level (if present)\n",
    "            if exp.get('level'):\n",
    "                exp_text += f\"Level: {exp['level']}. \"\n",
    "            \n",
    "            # Add employment type\n",
    "            if exp.get('employment_type'):\n",
    "                exp_text += f\"Employment Type: {exp['employment_type']}. \"\n",
    "            \n",
    "            # Add dates\n",
    "            if exp.get('dates'):\n",
    "                dates = exp['dates']\n",
    "                if dates.get('start'):\n",
    "                    exp_text += f\"Start Date: {dates['start']}. \"\n",
    "                if dates.get('end'):\n",
    "                    exp_text += f\"End Date: {dates['end']}. \"\n",
    "                if dates.get('duration'):\n",
    "                    exp_text += f\"Duration: {dates['duration']}. \"\n",
    "            \n",
    "            # Add responsibilities\n",
    "            if exp.get('responsibilities') and isinstance(exp['responsibilities'], list):\n",
    "                responsibilities = ' '.join(exp['responsibilities'])\n",
    "                exp_text += f\"Responsibilities: {responsibilities}. \"\n",
    "            \n",
    "            # Add technical environment\n",
    "            if exp.get('technical_environment'):\n",
    "                tech_env = exp['technical_environment']\n",
    "                if tech_env.get('technologies') and isinstance(tech_env['technologies'], list):\n",
    "                    technologies = ', '.join(tech_env['technologies'])\n",
    "                    exp_text += f\"Technologies: {technologies}. \"\n",
    "                if tech_env.get('methodologies') and isinstance(tech_env['methodologies'], list):\n",
    "                    methodologies = ', '.join(tech_env['methodologies'])\n",
    "                    exp_text += f\"Methodologies: {methodologies}. \"\n",
    "                if tech_env.get('tools') and isinstance(tech_env['tools'], list):\n",
    "                    tools = ', '.join(tech_env['tools'])\n",
    "                    exp_text += f\"Tools: {tools}. \"\n",
    "            \n",
    "            # Add company info\n",
    "            if exp.get('company_info'):\n",
    "                company_info = exp['company_info']\n",
    "                if company_info.get('industry'):\n",
    "                    exp_text += f\"Industry: {company_info['industry']}. \"\n",
    "                if company_info.get('size'):\n",
    "                    exp_text += f\"Company Size: {company_info['size']}. \"\n",
    "            \n",
    "            text_parts.append(exp_text.strip())\n",
    "        \n",
    "        return ' '.join(text_parts)\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "# Apply the parsing function to the experience column\n",
    "df['experience'] = df['experience'].apply(parse_experience_to_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d26c162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse education JSON to text\n",
    "def parse_education_to_text(education_str):\n",
    "    \"\"\"\n",
    "    Parse the education JSON string and convert it to a readable text format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(education_str) or education_str == '':\n",
    "            return \"\"\n",
    "        \n",
    "        education_list = json.loads(education_str)\n",
    "        if not isinstance(education_list, list):\n",
    "            return \"\"\n",
    "        \n",
    "        text_parts = []\n",
    "        for idx, edu in enumerate(education_list, 1):\n",
    "            if not isinstance(edu, dict):\n",
    "                continue\n",
    "            \n",
    "            edu_text = f\"Education {idx}: \"\n",
    "            \n",
    "            # Add degree information\n",
    "            if edu.get('degree'):\n",
    "                degree = edu['degree']\n",
    "                if degree.get('level'):\n",
    "                    edu_text += f\"Degree Level: {degree['level']}. \"\n",
    "                if degree.get('field'):\n",
    "                    edu_text += f\"Field: {degree['field']}. \"\n",
    "                if degree.get('major'):\n",
    "                    edu_text += f\"Major: {degree['major']}. \"\n",
    "            \n",
    "            # Add institution information\n",
    "            if edu.get('institution'):\n",
    "                institution = edu['institution']\n",
    "                if institution.get('name'):\n",
    "                    edu_text += f\"Institution: {institution['name']}. \"\n",
    "                if institution.get('location'):\n",
    "                    edu_text += f\"Location: {institution['location']}. \"\n",
    "                if institution.get('accreditation'):\n",
    "                    edu_text += f\"Accreditation: {institution['accreditation']}. \"\n",
    "            \n",
    "            # Add dates\n",
    "            if edu.get('dates'):\n",
    "                dates = edu['dates']\n",
    "                if dates.get('start'):\n",
    "                    edu_text += f\"Start Date: {dates['start']}. \"\n",
    "                if dates.get('end'):\n",
    "                    edu_text += f\"End Date: {dates['end']}. \"\n",
    "                if dates.get('expected_graduation'):\n",
    "                    edu_text += f\"Expected Graduation: {dates['expected_graduation']}. \"\n",
    "            \n",
    "            # Add achievements\n",
    "            if edu.get('achievements'):\n",
    "                achievements = edu['achievements']\n",
    "                if achievements.get('gpa'):\n",
    "                    edu_text += f\"GPA: {achievements['gpa']}. \"\n",
    "                if achievements.get('honors'):\n",
    "                    edu_text += f\"Honors: {achievements['honors']}. \"\n",
    "                if achievements.get('relevant_coursework') and isinstance(achievements['relevant_coursework'], list):\n",
    "                    coursework = ', '.join(achievements['relevant_coursework'])\n",
    "                    if coursework:\n",
    "                        edu_text += f\"Relevant Coursework: {coursework}. \"\n",
    "            \n",
    "            text_parts.append(edu_text.strip())\n",
    "        \n",
    "        return ' '.join(text_parts)\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "# Apply the parsing function to the education column\n",
    "df['education'] = df['education'].apply(parse_education_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cf8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse skills JSON to text\n",
    "def parse_skills_to_text(skills_json):\n",
    "    \"\"\"\n",
    "    Parse the skills JSON structure into a readable text format.\n",
    "    \"\"\"\n",
    "    if pd.isna(skills_json) or skills_json == '':\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        if isinstance(skills_json, str):\n",
    "            skills = json.loads(skills_json)\n",
    "        else:\n",
    "            skills = skills_json\n",
    "        \n",
    "        text_parts = []\n",
    "        \n",
    "        # Parse technical skills\n",
    "        if skills.get('technical'):\n",
    "            technical = skills['technical']\n",
    "            \n",
    "            # Programming languages\n",
    "            if technical.get('programming_languages') and isinstance(technical['programming_languages'], list):\n",
    "                for lang in technical['programming_languages']:\n",
    "                    if lang.get('name'):\n",
    "                        lang_text = f\"Programming Language: {lang['name']}\"\n",
    "                        if lang.get('level'):\n",
    "                            lang_text += f\" (Level: {lang['level']})\"\n",
    "                        text_parts.append(lang_text + \". \")\n",
    "            \n",
    "            # Frameworks\n",
    "            if technical.get('frameworks') and isinstance(technical['frameworks'], list):\n",
    "                for framework in technical['frameworks']:\n",
    "                    if framework.get('name'):\n",
    "                        framework_text = f\"Framework: {framework['name']}\"\n",
    "                        if framework.get('level'):\n",
    "                            framework_text += f\" (Level: {framework['level']})\"\n",
    "                        text_parts.append(framework_text + \". \")\n",
    "            \n",
    "            # Databases\n",
    "            if technical.get('databases') and isinstance(technical['databases'], list):\n",
    "                for db in technical['databases']:\n",
    "                    if db.get('name'):\n",
    "                        db_text = f\"Database: {db['name']}\"\n",
    "                        if db.get('level'):\n",
    "                            db_text += f\" (Level: {db['level']})\"\n",
    "                        text_parts.append(db_text + \". \")\n",
    "            \n",
    "            # Cloud\n",
    "            if technical.get('cloud') and isinstance(technical['cloud'], list):\n",
    "                for cloud in technical['cloud']:\n",
    "                    if isinstance(cloud, dict) and cloud.get('name'):\n",
    "                        cloud_text = f\"Cloud: {cloud['name']}\"\n",
    "                        if cloud.get('level'):\n",
    "                            cloud_text += f\" (Level: {cloud['level']})\"\n",
    "                        text_parts.append(cloud_text + \". \")\n",
    "                    elif isinstance(cloud, str):\n",
    "                        text_parts.append(f\"Cloud: {cloud}. \")\n",
    "        \n",
    "        # Parse languages\n",
    "        if skills.get('languages') and isinstance(skills['languages'], list):\n",
    "            for language in skills['languages']:\n",
    "                if isinstance(language, dict) and language.get('name'):\n",
    "                    lang_text = f\"Language: {language['name']}\"\n",
    "                    if language.get('level'):\n",
    "                        lang_text += f\" (Level: {language['level']})\"\n",
    "                    text_parts.append(lang_text + \". \")\n",
    "                elif isinstance(language, str):\n",
    "                    text_parts.append(f\"Language: {language}. \")\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"\"\n",
    "\n",
    "# Apply the parsing function to the skills column\n",
    "df['skills'] = df['skills'].apply(parse_skills_to_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdb668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse projects column from JSON to text\n",
    "def parse_projects_to_text(projects_str):\n",
    "    \"\"\"\n",
    "    Parse the projects JSON string into a readable text format.\n",
    "    If empty or invalid, return \"No Projects made\".\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(projects_str) or str(projects_str).strip() == '' or str(projects_str).strip() == '[]':\n",
    "            return \"No Projects made\"\n",
    "        \n",
    "        projects = json.loads(projects_str)\n",
    "        \n",
    "        if not projects or not isinstance(projects, list) or len(projects) == 0:\n",
    "            return \"No Projects made\"\n",
    "        \n",
    "        text_parts = []\n",
    "        \n",
    "        for idx, project in enumerate(projects, 1):\n",
    "            if not isinstance(project, dict):\n",
    "                continue\n",
    "            \n",
    "            project_text = f\"Project {idx}: \"\n",
    "            \n",
    "            # Project name\n",
    "            if project.get('name'):\n",
    "                project_text += f\"{project['name']}. \"\n",
    "            \n",
    "            # Description\n",
    "            if project.get('description'):\n",
    "                project_text += f\"Description: {project['description']}. \"\n",
    "            \n",
    "            # Technologies\n",
    "            if project.get('technologies') and isinstance(project['technologies'], list):\n",
    "                tech_list = [tech for tech in project['technologies'] if tech]\n",
    "                if tech_list:\n",
    "                    project_text += f\"Technologies: {', '.join(tech_list)}. \"\n",
    "            \n",
    "            # Role\n",
    "            if project.get('role'):\n",
    "                project_text += f\"Role: {project['role']}. \"\n",
    "            \n",
    "            # URL\n",
    "            if project.get('url'):\n",
    "                project_text += f\"URL: {project['url']}. \"\n",
    "            \n",
    "            # Impact\n",
    "            if project.get('impact'):\n",
    "                project_text += f\"Impact: {project['impact']}. \"\n",
    "            \n",
    "            text_parts.append(project_text.strip())\n",
    "        \n",
    "        if not text_parts:\n",
    "            return \"No Projects made\"\n",
    "        \n",
    "        return ' '.join(text_parts).strip()\n",
    "    \n",
    "    except (json.JSONDecodeError, TypeError, AttributeError):\n",
    "        return \"No Projects made\"\n",
    "\n",
    "# Apply the parsing function to the projects column\n",
    "df['projects'] = df['projects'].apply(parse_projects_to_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553f6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove certifications column since no one has certifications\n",
    "df = df.drop('certifications', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912de9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all rows where the 'summary' column is empty or contains only whitespace\n",
    "df = df[df['summary'].astype(str).str.strip() != '']\n",
    "\n",
    "# Show the number of rows left\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fc0ce5",
   "metadata": {},
   "source": [
    "### Generate Better Summaries Using LLM\n",
    "\n",
    "The original summaries are generic. We'll generate new ones based on actual resume content (experience, skills, projects, education, job title, experience times) - excluding personal info and seniority level.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58811a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load existing summaries if available (set to True to skip regeneration)\n",
    "LOAD_EXISTING_SUMMARIES = True\n",
    "summaries_file = 'generated_summaries.json'\n",
    "\n",
    "if LOAD_EXISTING_SUMMARIES and os.path.exists(summaries_file):\n",
    "    with open(summaries_file, 'r', encoding='utf-8') as f:\n",
    "        loaded_summaries = json.load(f)\n",
    "    # Convert string keys back to integers\n",
    "    new_summaries = {int(k): v for k, v in loaded_summaries.items()}\n",
    "    print(f\"Loaded {len(new_summaries)} existing summaries from {summaries_file}\")\n",
    "    SKIP_GENERATION = True\n",
    "else:\n",
    "    SKIP_GENERATION = False\n",
    "    print(f\"Will generate new summaries (file not found or LOAD_EXISTING_SUMMARIES=False)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4209d884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "MODEL = \"mistralai/ministral-3b-2512\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "MAX_WORKERS = 10  # parallel API calls\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Set OPENROUTER_API_KEY environment variable\")\n",
    "\n",
    "# Columns to use for generating summary (exclude personal info and target)\n",
    "EXCLUDE_COLS = {\"name\", \"email\", \"linkedin\", \"github\", \"experience_level\", \"summary\", \"summary_count\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddad489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_summary_prompt(row):\n",
    "    \"\"\"Build prompt from resume data to generate a summary.\"\"\"\n",
    "    parts = []\n",
    "    for col in row.index:\n",
    "        if col not in EXCLUDE_COLS:\n",
    "            val = str(row[col]).strip()\n",
    "            if val and val.lower() != \"nan\":\n",
    "                parts.append(f\"{col}: {val}\")\n",
    "    resume_text = \"\\n\".join(parts)\n",
    "    \n",
    "    return f\"\"\"Based on the following resume information, write a professional summary that highlights the candidate's key strengths, experience, and expertise that summarizes it as if you are the person writing it. Be specific and use details from the resume. Do NOT seniority level.\n",
    "\n",
    "{resume_text}\n",
    "\n",
    "Write ONLY the summary, nothing else:\"\"\"\n",
    "\n",
    "def call_api(prompt):\n",
    "    \"\"\"Call OpenRouter API and return response text.\"\"\"\n",
    "    payload = json.dumps({\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 150\n",
    "    }).encode(\"utf-8\")\n",
    "    \n",
    "    req = urllib.request.Request(\n",
    "        OPENROUTER_URL,\n",
    "        data=payload,\n",
    "        headers={\n",
    "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with urllib.request.urlopen(req, timeout=60) as resp:\n",
    "        result = json.loads(resp.read().decode(\"utf-8\"))\n",
    "    \n",
    "    return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "\n",
    "def generate_summary_for_row(idx_row):\n",
    "    \"\"\"Generate summary for a single row. Returns (index, new_summary).\"\"\"\n",
    "    idx, row = idx_row\n",
    "    try:\n",
    "        prompt = build_summary_prompt(row)\n",
    "        summary = call_api(prompt)\n",
    "        return idx, summary\n",
    "    except Exception as e:\n",
    "        print(f\"Row {idx} failed: {e}\")\n",
    "        return idx, row.get(\"summary\", \"\")  # fallback to original\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f71b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKIP_GENERATION:\n",
    "    print(f\"Generating summaries for {len(df)} rows...\")\n",
    "\n",
    "    new_summaries = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(generate_summary_for_row, (idx, row)): idx for idx, row in df.iterrows()}\n",
    "        \n",
    "        for i, future in enumerate(as_completed(futures), 1):\n",
    "            idx, summary = future.result()\n",
    "            new_summaries[idx] = summary\n",
    "            if i % 100 == 0:\n",
    "                print(f\"Processed {i}/{len(df)}\")\n",
    "\n",
    "    print(f\"Done! Generated {len(new_summaries)} summaries\")\n",
    "else:\n",
    "    print(\"Skipped generation - using loaded summaries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df53335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save generated summaries to a JSON file for reuse (only if newly generated)\n",
    "if not SKIP_GENERATION:\n",
    "    summaries_file = 'generated_summaries.json'\n",
    "    with open(summaries_file, 'w', encoding='utf-8') as f:\n",
    "        # Convert keys to strings for JSON compatibility\n",
    "        json.dump({str(k): v for k, v in new_summaries.items()}, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved {len(new_summaries)} summaries to {summaries_file}\")\n",
    "else:\n",
    "    print(\"Summaries loaded from file - no need to save\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd7bffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the summary column with new generated summaries\n",
    "df[\"summary\"] = df.index.map(new_summaries)\n",
    "\n",
    "# Update summary_count with new lengths\n",
    "df[\"summary_count\"] = df[\"summary\"].fillna(\"\").apply(len)\n",
    "\n",
    "# Show a few examples\n",
    "print(\"Sample generated summaries:\")\n",
    "print(\"=\"*80)\n",
    "for i in range(3):\n",
    "    print(f\"\\n[{i+1}] {df.iloc[i]['summary'][:300]}...\")\n",
    "    print(\"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31edb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove seniority level keywords from summaries to avoid data leakage\n",
    "import re\n",
    "\n",
    "def remove_seniority_keywords(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return text\n",
    "    # Use word boundaries to match whole words only (case-insensitive)\n",
    "    text = re.sub(r'\\b(junior|senior|mid)\\b', '', text, flags=re.IGNORECASE)\n",
    "    # Clean up any extra whitespace that may result\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"summary\"] = df[\"summary\"].apply(remove_seniority_keywords)\n",
    "print(\"Removed seniority keywords from summaries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to put most informative features first\n",
    "# Put most informative resume parts first (only if they exist)\n",
    "preferred_first = [\n",
    "    \"experience\", \"projects\", \"skills\", \"summary\", \"education\",\n",
    "    \"certifications\", \"job title\",\n",
    "    \"total_experience_time\", \"last_experience_time\", \"summary_count\", \"last_experience_only\"\n",
    "]\n",
    "\n",
    "# Stuff that is usually pure noise for prediction\n",
    "preferred_last = [\"name\", \"email\", \"linkedin\", \"github\"]\n",
    "\n",
    "# Get all columns\n",
    "all_columns = df.columns.tolist()\n",
    "\n",
    "# Separate columns into three groups\n",
    "first_cols = [col for col in preferred_first if col in all_columns]\n",
    "last_cols = [col for col in preferred_last if col in all_columns]\n",
    "middle_cols = [col for col in all_columns if col not in first_cols and col not in last_cols]\n",
    "\n",
    "# Reorder the dataframe\n",
    "new_column_order = first_cols + middle_cols + last_cols\n",
    "df = df[new_column_order]\n",
    "\n",
    "print(\"Columns reordered:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e562e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df.to_csv('cleaned_resumes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ed737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick 700 records of a senior, 700 of a mid, 700 of a junior, and save that file in ./Baseline/cleaned_resumes.csv\n",
    "# For seniors and mids: pick the top 700 most experienced (by total_experience_time)\n",
    "# For juniors: random sample (only ~750 total)\n",
    "\n",
    "# Helper function to extract numeric years from \"X.XX Years\" format\n",
    "def extract_years(time_str):\n",
    "    try:\n",
    "        return float(str(time_str).replace(' Years', '').strip())\n",
    "    except:\n",
    "        return 0.0\n",
    "\n",
    "senior_df = df[df['experience_level'] == 'senior'].copy()\n",
    "mid_df = df[df['experience_level'] == 'mid'].copy()\n",
    "junior_df = df[df['experience_level'] == 'junior']\n",
    "\n",
    "# Sort seniors and mids by total experience (descending) and take top 700\n",
    "senior_df['_exp_years'] = senior_df['total_experience_time'].apply(extract_years)\n",
    "mid_df['_exp_years'] = mid_df['total_experience_time'].apply(extract_years)\n",
    "\n",
    "senior_sample = senior_df.sort_values('_exp_years', ascending=False).head(700).drop(columns=['_exp_years'])\n",
    "mid_sample = mid_df.sort_values('_exp_years', ascending=False).head(700).drop(columns=['_exp_years'])\n",
    "\n",
    "# Juniors: random sample (they're only ~750 total)\n",
    "junior_sample = junior_df.sample(n=min(700, len(junior_df)), random_state=42)\n",
    "\n",
    "# Combine the samples\n",
    "balanced_df = pd.concat([senior_sample, mid_sample, junior_sample], ignore_index=True)\n",
    "\n",
    "print(f\"Senior experience range: {senior_sample['total_experience_time'].iloc[-1]} to {senior_sample['total_experience_time'].iloc[0]}\")\n",
    "print(f\"Mid experience range: {mid_sample['total_experience_time'].iloc[-1]} to {mid_sample['total_experience_time'].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b68f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to the specified location\n",
    "balanced_df.to_csv('./Baseline/cleaned_resumes.csv', index=False)\n",
    "balanced_df.to_csv('./Smaller Models/cleaned_resumes.csv', index=False)\n",
    "balanced_df.to_csv('./Big Models/cleaned_resumes.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Saved {len(balanced_df)} records to ./Baseline/cleaned_resumes.csv\")\n",
    "print(f\"\\nExperience level distribution:\")\n",
    "print(balanced_df['experience_level'].value_counts())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
