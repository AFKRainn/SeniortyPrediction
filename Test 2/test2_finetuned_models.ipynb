{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Finetuned Models (DistilBERT & RoBERTa)\n",
    "\n",
    "This notebook tests if finetuned transformer models have **bias towards resume writing style**:\n",
    "- Do overstated resumes get higher seniority predictions?\n",
    "- Do understated resumes get lower seniority predictions?\n",
    "- Which words influence the model's decisions?\n",
    "\n",
    "**Models tested:** DistilBERT, RoBERTa (run in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 120 resume sets (each has neutral, overstated, understated versions)\n",
      "Seniority distribution: {'junior': 40, 'mid': 40, 'senior': 40}\n"
     ]
    }
   ],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "print(f\"Loaded {len(df)} resume sets (each has neutral, overstated, understated versions)\")\n",
    "print(f\"Seniority distribution: {df['seniority'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading distilbert... Done\n",
      "Loading roberta... "
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, path \u001b[38;5;129;01min\u001b[39;00m MODEL_PATHS.items():\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m, end=\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     tokenizers[name] = \u001b[43mAutoTokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device)\n\u001b[32m     14\u001b[39m     models[name].eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1175\u001b[39m, in \u001b[36mAutoTokenizer.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m tokenizer_class_py, tokenizer_class_fast = TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[32m   1174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2113\u001b[39m, in \u001b[36mPreTrainedTokenizerBase.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2110\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2111\u001b[39m         logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m2113\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_is_local\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_local\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2124\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2125\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2151\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2148\u001b[39m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[32m   2149\u001b[39m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[32m   2150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (from_slow \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_tokenizer_file) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.slow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[32m-> \u001b[39m\u001b[32m2151\u001b[39m     slow_tokenizer = \u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mslow_tokenizer_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_from_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_vocab_files\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_configuration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2155\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2158\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2159\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2160\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2161\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2163\u001b[39m     slow_tokenizer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2359\u001b[39m, in \u001b[36mPreTrainedTokenizerBase._from_pretrained\u001b[39m\u001b[34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[39m\n\u001b[32m   2357\u001b[39m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[32m   2358\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2359\u001b[39m     tokenizer = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m import_protobuf_decode_error():\n\u001b[32m   2361\u001b[39m     logger.info(\n\u001b[32m   2362\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUnable to load tokenizer model from SPM, loading from TikToken will be attempted instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2363\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(Google protobuf error: Tried to load SPM model with non-SPM vocab file).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2364\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\roberta\\tokenization_roberta.py:187\u001b[39m, in \u001b[36mRobertaTokenizer.__init__\u001b[39m\u001b[34m(self, vocab_file, merges_file, errors, bos_token, eos_token, sep_token, cls_token, unk_token, pad_token, mask_token, add_prefix_space, **kwargs)\u001b[39m\n\u001b[32m    179\u001b[39m mask_token = (\n\u001b[32m    180\u001b[39m     AddedToken(mask_token, lstrip=\u001b[38;5;28;01mTrue\u001b[39;00m, rstrip=\u001b[38;5;28;01mFalse\u001b[39;00m, normalized=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mask_token, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    182\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m mask_token\n\u001b[32m    183\u001b[39m )\n\u001b[32m    185\u001b[39m \u001b[38;5;66;03m# these special tokens are not part of the vocab.json, let's add them in the correct order\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m vocab_handle:\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.encoder = json.load(vocab_handle)\n\u001b[32m    189\u001b[39m \u001b[38;5;28mself\u001b[39m.decoder = {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.encoder.items()}\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not NoneType"
     ]
    }
   ],
   "source": [
    "# Load both models\n",
    "MODEL_PATHS = {\n",
    "    'distilbert': '../Smaller Models/distilbert_resume_level',\n",
    "    'roberta': '../Smaller Models/roberta_resume_level'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    print(f\"Loading {name}...\", end=\" \")\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device)\n",
    "    models[name].eval()\n",
    "    print(\"Done\")\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: 'junior', 1: 'mid', 2: 'senior'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, text):\n",
    "    \"\"\"Get prediction from a model.\"\"\"\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return id2label[pred_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predictions (Parallel)\n",
    "\n",
    "Run both models on all resume versions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions for all resumes and all styles\n",
    "results = []\n",
    "\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "model_names = list(models.keys())\n",
    "\n",
    "total = len(df) * len(styles) * len(model_names)\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    true_seniority = row['seniority']\n",
    "    \n",
    "    for style in styles:\n",
    "        resume_text = str(row[style])\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            pred = predict(model_name, resume_text)\n",
    "            \n",
    "            results.append({\n",
    "                'idx': idx,\n",
    "                'true_seniority': true_seniority,\n",
    "                'style': style,\n",
    "                'model': model_name,\n",
    "                'prediction': pred,\n",
    "                'correct': pred == true_seniority,\n",
    "                'resume_text': resume_text\n",
    "            })\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} resumes ({count}/{total} predictions)\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nTotal predictions: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later analysis\n",
    "results_df.to_csv(\"finetuned_predictions.csv\", index=False)\n",
    "print(\"Saved to finetuned_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 1: Accuracy by Style\n",
    "\n",
    "Does writing style affect prediction accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ACCURACY BY STYLE AND MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    for style in ['neutral', 'overstated', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        acc = style_df['correct'].mean()\n",
    "        print(f\"  {style:<15}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 2: Style Bias Detection\n",
    "\n",
    "Key question: Do overstated resumes get predicted as higher seniority than they actually are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map seniority to numeric for comparison\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "results_df['true_rank'] = results_df['true_seniority'].map(seniority_rank)\n",
    "results_df['pred_rank'] = results_df['prediction'].map(seniority_rank)\n",
    "results_df['rank_diff'] = results_df['pred_rank'] - results_df['true_rank']  # +ve = overestimate\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION BIAS BY STYLE (rank_diff: +ve = overestimate, -ve = underestimate)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Style':<15} {'Avg Rank Diff':<18} {'Overestimates':<15} {'Underestimates'}\")\n",
    "    for style in ['overstated', 'neutral', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        avg_diff = style_df['rank_diff'].mean()\n",
    "        over = (style_df['rank_diff'] > 0).sum()\n",
    "        under = (style_df['rank_diff'] < 0).sum()\n",
    "        print(f\"{style:<15} {avg_diff:+.3f}{'':>12} {over:<15} {under}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BIAS CHECK: Do overstated resumes get 'promoted'?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    # Check: Junior resumes predicted as senior\n",
    "    for true_level in ['junior', 'mid']:\n",
    "        for style in ['overstated', 'neutral', 'understated']:\n",
    "            subset = model_df[(model_df['true_seniority'] == true_level) & (model_df['style'] == style)]\n",
    "            senior_preds = (subset['prediction'] == 'senior').sum()\n",
    "            total = len(subset)\n",
    "            pct = senior_preds / total * 100 if total > 0 else 0\n",
    "            print(f\"  {true_level.upper()} + {style:<12} â†’ predicted SENIOR: {senior_preds}/{total} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 3: Same Person, Different Predictions?\n",
    "\n",
    "For each person: do their 3 resume versions get different predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION CONSISTENCY: Same person, different styles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    inconsistent = 0\n",
    "    style_changes = []\n",
    "    \n",
    "    for idx in df.index:\n",
    "        person_preds = model_df[model_df['idx'] == idx].set_index('style')['prediction']\n",
    "        \n",
    "        if len(set(person_preds)) > 1:  # Different predictions for same person\n",
    "            inconsistent += 1\n",
    "            style_changes.append({\n",
    "                'idx': idx,\n",
    "                'true': df.loc[idx, 'seniority'],\n",
    "                'neutral': person_preds.get('neutral', '?'),\n",
    "                'overstated': person_preds.get('overstated', '?'),\n",
    "                'understated': person_preds.get('understated', '?')\n",
    "            })\n",
    "    \n",
    "    print(f\"Resumes with INCONSISTENT predictions across styles: {inconsistent}/{len(df)} ({inconsistent/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if style_changes:\n",
    "        changes_df = pd.DataFrame(style_changes[:10])  # Show first 10\n",
    "        print(\"\\nSample inconsistent predictions:\")\n",
    "        print(changes_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 4: Word Influence Analysis\n",
    "\n",
    "Which words appear more in resumes that got predicted as Senior vs Junior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power words and humble words from our validation metric\n",
    "POWER_WORDS = {\n",
    "    'spearheaded', 'revolutionized', 'pioneered', 'orchestrated', 'architected',\n",
    "    'transformed', 'drove', 'championed', 'accelerated', 'maximized',\n",
    "    'optimized', 'elevated', 'propelled', 'commanded', 'masterminded',\n",
    "    'dramatically', 'significantly', 'substantially', 'exponentially', 'exceptionally',\n",
    "    'outstanding', 'exceptional', 'remarkable', 'extraordinary', 'tremendous',\n",
    "    'critical', 'crucial', 'vital', 'strategic', 'innovative',\n",
    "    'enterprise', 'comprehensive', 'extensive', 'robust', 'cutting-edge',\n",
    "    'visionary', 'influential', 'instrumental', 'pivotal', 'key'\n",
    "}\n",
    "\n",
    "HUMBLE_WORDS = {\n",
    "    'helped', 'assisted', 'supported', 'contributed', 'participated',\n",
    "    'aided', 'collaborated', 'worked', 'involved', 'engaged',\n",
    "    'some', 'basic', 'minor', 'small', 'routine', 'standard', 'general',\n",
    "    'occasional', 'limited', 'partial', 'modest', 'simple',\n",
    "    'team', 'alongside', 'together', 'group', 'collective',\n",
    "    'somewhat', 'relatively', 'fairly', 'adequately', 'sufficiently'\n",
    "}\n",
    "\n",
    "def count_words(text, word_set):\n",
    "    words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "    return sum(1 for w in words if w in word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WORD INFLUENCE: Power/Humble words in predictions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df['power_count'] = results_df['resume_text'].apply(lambda x: count_words(x, POWER_WORDS))\n",
    "results_df['humble_count'] = results_df['resume_text'].apply(lambda x: count_words(x, HUMBLE_WORDS))\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Prediction':<12} {'Avg Power Words':<18} {'Avg Humble Words'}\")\n",
    "    for pred in ['senior', 'mid', 'junior']:\n",
    "        pred_df = model_df[model_df['prediction'] == pred]\n",
    "        avg_power = pred_df['power_count'].mean()\n",
    "        avg_humble = pred_df['humble_count'].mean()\n",
    "        print(f\"{pred:<12} {avg_power:.1f}{'':>14} {avg_humble:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed word analysis: which specific words appear in senior predictions\n",
    "def get_word_freq(texts, word_set):\n",
    "    counts = Counter()\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "        for w in words:\n",
    "            if w in word_set:\n",
    "                counts[w] += 1\n",
    "    return counts\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP POWER WORDS IN SENIOR PREDICTIONS vs JUNIOR PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    senior_texts = model_df[model_df['prediction'] == 'senior']['resume_text']\n",
    "    junior_texts = model_df[model_df['prediction'] == 'junior']['resume_text']\n",
    "    \n",
    "    senior_words = get_word_freq(senior_texts, POWER_WORDS)\n",
    "    junior_words = get_word_freq(junior_texts, POWER_WORDS)\n",
    "    \n",
    "    print(\"Power words in SENIOR predictions:\", dict(senior_words.most_common(5)))\n",
    "    print(\"Power words in JUNIOR predictions:\", dict(junior_words.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 5: Misclassification Deep Dive\n",
    "\n",
    "When a junior is predicted as senior (or vice versa), what words were present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    # Juniors predicted as Senior (overpromoted)\n",
    "    overpromoted = model_df[(model_df['true_seniority'] == 'junior') & (model_df['prediction'] == 'senior')]\n",
    "    print(f\"\\nJuniors predicted as SENIOR: {len(overpromoted)}\")\n",
    "    if len(overpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {overpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {overpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {overpromoted['humble_count'].mean():.1f}\")\n",
    "    \n",
    "    # Seniors predicted as Junior (underpromoted)\n",
    "    underpromoted = model_df[(model_df['true_seniority'] == 'senior') & (model_df['prediction'] == 'junior')]\n",
    "    print(f\"\\nSeniors predicted as JUNIOR: {len(underpromoted)}\")\n",
    "    if len(underpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {underpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {underpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {underpromoted['humble_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by style for each model\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "x = np.arange(len(styles))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    accs = [model_df[model_df['style'] == s]['correct'].mean() for s in styles]\n",
    "    axes[0].bar(x + i*width, accs, width, label=model_name.upper())\n",
    "\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy by Resume Style')\n",
    "axes[0].set_xticks(x + width/2)\n",
    "axes[0].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Average rank difference (bias) by style\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    diffs = [model_df[model_df['style'] == s]['rank_diff'].mean() for s in styles]\n",
    "    axes[1].bar(x + i*width, diffs, width, label=model_name.upper())\n",
    "\n",
    "axes[1].set_ylabel('Avg Rank Difference (+ = overestimate)')\n",
    "axes[1].set_title('Prediction Bias by Resume Style')\n",
    "axes[1].set_xticks(x + width/2)\n",
    "axes[1].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Findings Summary\n",
    "\n",
    "This section will be filled after running the tests with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    neutral_acc = model_df[model_df['style'] == 'neutral']['correct'].mean()\n",
    "    over_acc = model_df[model_df['style'] == 'overstated']['correct'].mean()\n",
    "    under_acc = model_df[model_df['style'] == 'understated']['correct'].mean()\n",
    "    \n",
    "    print(f\"Neutral accuracy:     {neutral_acc:.1%}\")\n",
    "    print(f\"Overstated accuracy:  {over_acc:.1%}\")\n",
    "    print(f\"Understated accuracy: {under_acc:.1%}\")\n",
    "    \n",
    "    # Bias\n",
    "    over_bias = model_df[model_df['style'] == 'overstated']['rank_diff'].mean()\n",
    "    under_bias = model_df[model_df['style'] == 'understated']['rank_diff'].mean()\n",
    "    \n",
    "    print(f\"\\nOverstated bias:  {over_bias:+.3f} ({'tends to overestimate' if over_bias > 0 else 'tends to underestimate'})\")\n",
    "    print(f\"Understated bias: {under_bias:+.3f} ({'tends to overestimate' if under_bias > 0 else 'tends to underestimate'})\")\n",
    "    \n",
    "    # Inconsistency\n",
    "    inconsistent = 0\n",
    "    for idx in df.index:\n",
    "        preds = model_df[model_df['idx'] == idx]['prediction'].unique()\n",
    "        if len(preds) > 1:\n",
    "            inconsistent += 1\n",
    "    print(f\"\\nInconsistent predictions: {inconsistent}/{len(df)} ({inconsistent/len(df)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
