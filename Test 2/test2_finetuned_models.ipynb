{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Finetuned Models (DistilBERT & RoBERTa)\n",
    "\n",
    "This notebook tests if finetuned transformer models have **bias towards resume writing style**:\n",
    "- Do overstated resumes get higher seniority predictions?\n",
    "- Do understated resumes get lower seniority predictions?\n",
    "- Which words influence the model's decisions?\n",
    "\n",
    "**Models tested:** DistilBERT, RoBERTa (run in parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "print(f\"Loaded {len(df)} resume sets (each has neutral, overstated, understated versions)\")\n",
    "print(f\"Seniority distribution: {df['seniority'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "MODEL_PATHS = {\n",
    "    'distilbert': '../Smaller Models/distilbert_resume_level',\n",
    "    'roberta': '../Smaller Models/roberta_resume_level'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    print(f\"Loading {name}...\", end=\" \")\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device)\n",
    "    models[name].eval()\n",
    "    print(\"Done\")\n",
    "\n",
    "# Label mapping\n",
    "id2label = {0: 'junior', 1: 'mid', 2: 'senior'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, text):\n",
    "    \"\"\"Get prediction from a model.\"\"\"\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return id2label[pred_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predictions (Parallel)\n",
    "\n",
    "Run both models on all resume versions in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions for all resumes and all styles\n",
    "results = []\n",
    "\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "model_names = list(models.keys())\n",
    "\n",
    "total = len(df) * len(styles) * len(model_names)\n",
    "count = 0\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    true_seniority = row['seniority']\n",
    "    \n",
    "    for style in styles:\n",
    "        resume_text = str(row[style])\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            pred = predict(model_name, resume_text)\n",
    "            \n",
    "            results.append({\n",
    "                'idx': idx,\n",
    "                'true_seniority': true_seniority,\n",
    "                'style': style,\n",
    "                'model': model_name,\n",
    "                'prediction': pred,\n",
    "                'correct': pred == true_seniority,\n",
    "                'resume_text': resume_text\n",
    "            })\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)} resumes ({count}/{total} predictions)\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nTotal predictions: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for later analysis\n",
    "results_df.to_csv(\"finetuned_predictions.csv\", index=False)\n",
    "print(\"Saved to finetuned_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 1: Accuracy by Style\n",
    "\n",
    "Does writing style affect prediction accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"ACCURACY BY STYLE AND MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    for style in ['neutral', 'overstated', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        acc = style_df['correct'].mean()\n",
    "        print(f\"  {style:<15}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 2: Style Bias Detection\n",
    "\n",
    "Key question: Do overstated resumes get predicted as higher seniority than they actually are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map seniority to numeric for comparison\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "results_df['true_rank'] = results_df['true_seniority'].map(seniority_rank)\n",
    "results_df['pred_rank'] = results_df['prediction'].map(seniority_rank)\n",
    "results_df['rank_diff'] = results_df['pred_rank'] - results_df['true_rank']  # +ve = overestimate\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION BIAS BY STYLE (rank_diff: +ve = overestimate, -ve = underestimate)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Style':<15} {'Avg Rank Diff':<18} {'Overestimates':<15} {'Underestimates'}\")\n",
    "    for style in ['overstated', 'neutral', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        avg_diff = style_df['rank_diff'].mean()\n",
    "        over = (style_df['rank_diff'] > 0).sum()\n",
    "        under = (style_df['rank_diff'] < 0).sum()\n",
    "        print(f\"{style:<15} {avg_diff:+.3f}{'':>12} {over:<15} {under}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BIAS CHECK: Do overstated resumes get 'promoted'?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    # Check: Junior resumes predicted as senior\n",
    "    for true_level in ['junior', 'mid']:\n",
    "        for style in ['overstated', 'neutral', 'understated']:\n",
    "            subset = model_df[(model_df['true_seniority'] == true_level) & (model_df['style'] == style)]\n",
    "            senior_preds = (subset['prediction'] == 'senior').sum()\n",
    "            total = len(subset)\n",
    "            pct = senior_preds / total * 100 if total > 0 else 0\n",
    "            print(f\"  {true_level.upper()} + {style:<12} â†’ predicted SENIOR: {senior_preds}/{total} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 3: Same Person, Different Predictions?\n",
    "\n",
    "For each person: do their 3 resume versions get different predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION CONSISTENCY: Same person, different styles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    inconsistent = 0\n",
    "    style_changes = []\n",
    "    \n",
    "    for idx in df.index:\n",
    "        person_preds = model_df[model_df['idx'] == idx].set_index('style')['prediction']\n",
    "        \n",
    "        if len(set(person_preds)) > 1:  # Different predictions for same person\n",
    "            inconsistent += 1\n",
    "            style_changes.append({\n",
    "                'idx': idx,\n",
    "                'true': df.loc[idx, 'seniority'],\n",
    "                'neutral': person_preds.get('neutral', '?'),\n",
    "                'overstated': person_preds.get('overstated', '?'),\n",
    "                'understated': person_preds.get('understated', '?')\n",
    "            })\n",
    "    \n",
    "    print(f\"Resumes with INCONSISTENT predictions across styles: {inconsistent}/{len(df)} ({inconsistent/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    if style_changes:\n",
    "        changes_df = pd.DataFrame(style_changes[:10])  # Show first 10\n",
    "        print(\"\\nSample inconsistent predictions:\")\n",
    "        print(changes_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 4: Word Influence Analysis\n",
    "\n",
    "Which words appear more in resumes that got predicted as Senior vs Junior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power words and humble words from our validation metric\n",
    "POWER_WORDS = {\n",
    "    'spearheaded', 'revolutionized', 'pioneered', 'orchestrated', 'architected',\n",
    "    'transformed', 'drove', 'championed', 'accelerated', 'maximized',\n",
    "    'optimized', 'elevated', 'propelled', 'commanded', 'masterminded',\n",
    "    'dramatically', 'significantly', 'substantially', 'exponentially', 'exceptionally',\n",
    "    'outstanding', 'exceptional', 'remarkable', 'extraordinary', 'tremendous',\n",
    "    'critical', 'crucial', 'vital', 'strategic', 'innovative',\n",
    "    'enterprise', 'comprehensive', 'extensive', 'robust', 'cutting-edge',\n",
    "    'visionary', 'influential', 'instrumental', 'pivotal', 'key'\n",
    "}\n",
    "\n",
    "HUMBLE_WORDS = {\n",
    "    'helped', 'assisted', 'supported', 'contributed', 'participated',\n",
    "    'aided', 'collaborated', 'worked', 'involved', 'engaged',\n",
    "    'some', 'basic', 'minor', 'small', 'routine', 'standard', 'general',\n",
    "    'occasional', 'limited', 'partial', 'modest', 'simple',\n",
    "    'team', 'alongside', 'together', 'group', 'collective',\n",
    "    'somewhat', 'relatively', 'fairly', 'adequately', 'sufficiently'\n",
    "}\n",
    "\n",
    "def count_words(text, word_set):\n",
    "    words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "    return sum(1 for w in words if w in word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WORD INFLUENCE: Power/Humble words in predictions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "results_df['power_count'] = results_df['resume_text'].apply(lambda x: count_words(x, POWER_WORDS))\n",
    "results_df['humble_count'] = results_df['resume_text'].apply(lambda x: count_words(x, HUMBLE_WORDS))\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Prediction':<12} {'Avg Power Words':<18} {'Avg Humble Words'}\")\n",
    "    for pred in ['senior', 'mid', 'junior']:\n",
    "        pred_df = model_df[model_df['prediction'] == pred]\n",
    "        avg_power = pred_df['power_count'].mean()\n",
    "        avg_humble = pred_df['humble_count'].mean()\n",
    "        print(f\"{pred:<12} {avg_power:.1f}{'':>14} {avg_humble:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed word analysis: which specific words appear in senior predictions\n",
    "def get_word_freq(texts, word_set):\n",
    "    counts = Counter()\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "        for w in words:\n",
    "            if w in word_set:\n",
    "                counts[w] += 1\n",
    "    return counts\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP POWER WORDS IN SENIOR PREDICTIONS vs JUNIOR PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    senior_texts = model_df[model_df['prediction'] == 'senior']['resume_text']\n",
    "    junior_texts = model_df[model_df['prediction'] == 'junior']['resume_text']\n",
    "    \n",
    "    senior_words = get_word_freq(senior_texts, POWER_WORDS)\n",
    "    junior_words = get_word_freq(junior_texts, POWER_WORDS)\n",
    "    \n",
    "    print(\"Power words in SENIOR predictions:\", dict(senior_words.most_common(5)))\n",
    "    print(\"Power words in JUNIOR predictions:\", dict(junior_words.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 5: Misclassification Deep Dive\n",
    "\n",
    "When a junior is predicted as senior (or vice versa), what words were present?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    # Juniors predicted as Senior (overpromoted)\n",
    "    overpromoted = model_df[(model_df['true_seniority'] == 'junior') & (model_df['prediction'] == 'senior')]\n",
    "    print(f\"\\nJuniors predicted as SENIOR: {len(overpromoted)}\")\n",
    "    if len(overpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {overpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {overpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {overpromoted['humble_count'].mean():.1f}\")\n",
    "    \n",
    "    # Seniors predicted as Junior (underpromoted)\n",
    "    underpromoted = model_df[(model_df['true_seniority'] == 'senior') & (model_df['prediction'] == 'junior')]\n",
    "    print(f\"\\nSeniors predicted as JUNIOR: {len(underpromoted)}\")\n",
    "    if len(underpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {underpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {underpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {underpromoted['humble_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy by style for each model\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "x = np.arange(len(styles))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    accs = [model_df[model_df['style'] == s]['correct'].mean() for s in styles]\n",
    "    axes[0].bar(x + i*width, accs, width, label=model_name.upper())\n",
    "\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy by Resume Style')\n",
    "axes[0].set_xticks(x + width/2)\n",
    "axes[0].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Average rank difference (bias) by style\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    diffs = [model_df[model_df['style'] == s]['rank_diff'].mean() for s in styles]\n",
    "    axes[1].bar(x + i*width, diffs, width, label=model_name.upper())\n",
    "\n",
    "axes[1].set_ylabel('Avg Rank Difference (+ = overestimate)')\n",
    "axes[1].set_title('Prediction Bias by Resume Style')\n",
    "axes[1].set_xticks(x + width/2)\n",
    "axes[1].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Findings Summary\n",
    "\n",
    "This section will be filled after running the tests with the actual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Overall accuracy\n",
    "    neutral_acc = model_df[model_df['style'] == 'neutral']['correct'].mean()\n",
    "    over_acc = model_df[model_df['style'] == 'overstated']['correct'].mean()\n",
    "    under_acc = model_df[model_df['style'] == 'understated']['correct'].mean()\n",
    "    \n",
    "    print(f\"Neutral accuracy:     {neutral_acc:.1%}\")\n",
    "    print(f\"Overstated accuracy:  {over_acc:.1%}\")\n",
    "    print(f\"Understated accuracy: {under_acc:.1%}\")\n",
    "    \n",
    "    # Bias\n",
    "    over_bias = model_df[model_df['style'] == 'overstated']['rank_diff'].mean()\n",
    "    under_bias = model_df[model_df['style'] == 'understated']['rank_diff'].mean()\n",
    "    \n",
    "    print(f\"\\nOverstated bias:  {over_bias:+.3f} ({'tends to overestimate' if over_bias > 0 else 'tends to underestimate'})\")\n",
    "    print(f\"Understated bias: {under_bias:+.3f} ({'tends to overestimate' if under_bias > 0 else 'tends to underestimate'})\")\n",
    "    \n",
    "    # Inconsistency\n",
    "    inconsistent = 0\n",
    "    for idx in df.index:\n",
    "        preds = model_df[model_df['idx'] == idx]['prediction'].unique()\n",
    "        if len(preds) > 1:\n",
    "            inconsistent += 1\n",
    "    print(f\"\\nInconsistent predictions: {inconsistent}/{len(df)} ({inconsistent/len(df)*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
