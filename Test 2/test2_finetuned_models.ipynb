{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Finetuned Models (DistilBERT & RoBERTa)\n",
    "\n",
    "Testing if finetuned transformer models have **bias towards resume writing style**.\n",
    "\n",
    "**Metrics:** Accuracy, Inconsistency Rate, Rank Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "print(f\"Loaded {len(df)} resume sets\")\n",
    "print(f\"Seniority distribution: {df['seniority'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "MODEL_PATHS = {\n",
    "    'distilbert': '../Smaller Models/distilbert_resume_level',\n",
    "    'roberta': '../Smaller Models/roberta_resume_level'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "tokenizers = {}\n",
    "\n",
    "for name, path in MODEL_PATHS.items():\n",
    "    print(f\"Loading {name}...\", end=\" \")\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(path)\n",
    "    models[name] = AutoModelForSequenceClassification.from_pretrained(path).to(device)\n",
    "    models[name].eval()\n",
    "    print(\"Done\")\n",
    "\n",
    "id2label = {0: 'junior', 1: 'mid', 2: 'senior'}\n",
    "model_names = list(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_name, text):\n",
    "    tokenizer = tokenizers[model_name]\n",
    "    model = models[model_name]\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        pred_id = torch.argmax(outputs.logits, dim=1).item()\n",
    "    \n",
    "    return id2label[pred_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predictions\n",
    "results = []\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    for style in styles:\n",
    "        for model_name in model_names:\n",
    "            pred = predict(model_name, str(row[style]))\n",
    "            results.append({\n",
    "                'idx': idx,\n",
    "                'true_seniority': row['seniority'],\n",
    "                'style': style,\n",
    "                'model': model_name,\n",
    "                'prediction': pred,\n",
    "                'correct': pred == row['seniority']\n",
    "            })\n",
    "    if (idx + 1) % 20 == 0:\n",
    "        print(f\"Processed {idx + 1}/{len(df)}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nTotal predictions: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv(\"finetuned_predictions.csv\", index=False)\n",
    "print(\"Saved to finetuned_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add rank columns\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "results_df['true_rank'] = results_df['true_seniority'].map(seniority_rank)\n",
    "results_df['pred_rank'] = results_df['prediction'].map(seniority_rank)\n",
    "results_df['rank_diff'] = results_df['pred_rank'] - results_df['true_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 1: Accuracy by Style\n",
    "\n",
    "How often does the model predict the correct seniority for each resume style?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY BY STYLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    for style in styles:\n",
    "        acc = model_df[model_df['style'] == style]['correct'].mean()\n",
    "        print(f\"  {style:<15}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(styles))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    accs = [model_df[model_df['style'] == s]['correct'].mean() for s in styles]\n",
    "    ax.bar(x + i*width, accs, width, label=model_name.upper())\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by Resume Style')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 2: Inconsistency Rate\n",
    "\n",
    "For the same person, do different resume styles get different predictions? Higher = more biased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INCONSISTENCY RATE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "inconsistency_rates = {}\n",
    "for model_name in model_names:\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    inconsistent = 0\n",
    "    for idx in df.index:\n",
    "        preds = model_df[model_df['idx'] == idx]['prediction'].unique()\n",
    "        if len(preds) > 1:\n",
    "            inconsistent += 1\n",
    "    rate = inconsistent / len(df) * 100\n",
    "    inconsistency_rates[model_name] = rate\n",
    "    print(f\"{model_name.upper()}: {inconsistent}/{len(df)} ({rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inconsistency visualization\n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.bar(model_names, [inconsistency_rates[m] for m in model_names], color=['steelblue', 'coral'])\n",
    "ax.set_ylabel('Inconsistency Rate (%)')\n",
    "ax.set_title('Prediction Inconsistency by Model')\n",
    "ax.set_ylim(0, 100)\n",
    "for i, m in enumerate(model_names):\n",
    "    ax.text(i, inconsistency_rates[m] + 2, f'{inconsistency_rates[m]:.1f}%', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 3: Rank Difference\n",
    "\n",
    "Average difference between predicted and true seniority rank. Positive = overestimates, Negative = underestimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANK DIFFERENCE BY STYLE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    for style in styles:\n",
    "        diff = model_df[model_df['style'] == style]['rank_diff'].mean()\n",
    "        print(f\"  {style:<15}: {diff:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank difference visualization\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(len(styles))\n",
    "width = 0.35\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = results_df[results_df['model'] == model_name]\n",
    "    diffs = [model_df[model_df['style'] == s]['rank_diff'].mean() for s in styles]\n",
    "    ax.bar(x + i*width, diffs, width, label=model_name.upper())\n",
    "\n",
    "ax.set_ylabel('Avg Rank Difference')\n",
    "ax.set_title('Prediction Bias by Style (+ = overestimate)')\n",
    "ax.set_xticks(x + width/2)\n",
    "ax.set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
