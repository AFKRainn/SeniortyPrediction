{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: Style Bias - Visualizations\n",
    "\n",
    "This notebook provides visual analysis of style bias in resume seniority predictions.\n",
    "\n",
    "**Question we're answering:** Does the WAY someone writes their resume affect how AI models judge their seniority?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Set style for all plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Load data\n",
    "df_orig = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "\n",
    "# Try to load both prediction files\n",
    "try:\n",
    "    ft_df = pd.read_csv(\"finetuned_predictions.csv\")\n",
    "    print(f\"Loaded finetuned predictions: {len(ft_df)} rows\")\n",
    "except:\n",
    "    ft_df = None\n",
    "    print(\"Finetuned predictions not found\")\n",
    "\n",
    "try:\n",
    "    llm_df = pd.read_csv(\"llm_predictions.csv\")\n",
    "    llm_df = llm_df[~llm_df['prediction'].isin(['error', 'unknown'])]\n",
    "    print(f\"Loaded LLM predictions: {len(llm_df)} rows\")\n",
    "except:\n",
    "    llm_df = None\n",
    "    print(\"LLM predictions not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "\n",
    "def prepare_df(df):\n",
    "    if df is None:\n",
    "        return None\n",
    "    df = df.copy()\n",
    "    df['true_rank'] = df['true_seniority'].map(seniority_rank)\n",
    "    df['pred_rank'] = df['prediction'].map(seniority_rank)\n",
    "    df['rank_diff'] = df['pred_rank'] - df['true_rank']\n",
    "    return df\n",
    "\n",
    "ft_df = prepare_df(ft_df)\n",
    "llm_df = prepare_df(llm_df)\n",
    "\n",
    "# Get model names\n",
    "ft_models = ft_df['model'].unique().tolist() if ft_df is not None else []\n",
    "llm_models = llm_df['model'].unique().tolist() if llm_df is not None else []\n",
    "all_models = ft_models + llm_models\n",
    "print(f\"Models: {all_models}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 1: Accuracy by Resume Style\n",
    "\n",
    "### What This Analysis Shows\n",
    "This chart displays how accurately each model predicts the correct seniority level (junior, mid, or senior) when given resumes written in different styles.\n",
    "\n",
    "### Why This Matters\n",
    "In a fair system, writing style should NOT affect accuracy. If a model predicts correctly 80% of the time for neutral resumes but only 50% for overstated resumes, it means the confident language is **confusing** the model. The model might be:\n",
    "- Promoting juniors who use power words\n",
    "- Demoting seniors who write modestly\n",
    "\n",
    "### How to Interpret\n",
    "- **Equal bar heights** = Good! The model focuses on actual experience, not writing style.\n",
    "- **Overstated accuracy < Neutral accuracy** = The power words are misleading the model.\n",
    "- **Understated accuracy < Neutral accuracy** = Humble language makes real experience invisible.\n",
    "- **Overstated accuracy < Understated accuracy** = Particularly concerning - confident language actively hurts accuracy.\n",
    "\n",
    "### What to Look For\n",
    "Compare finetuned models (DistilBERT, RoBERTa) vs LLMs (GPT-5, Gemini, Sonnet). Do SOTA LLMs handle style variation better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "colors = {'neutral': '#3498db', 'overstated': '#e74c3c', 'understated': '#2ecc71'}\n",
    "\n",
    "# Finetuned models\n",
    "if ft_df is not None:\n",
    "    x = np.arange(len(ft_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        accs = [ft_df[(ft_df['model'] == m) & (ft_df['style'] == style)]['correct'].mean() \n",
    "                for m in ft_models]\n",
    "        axes[0].bar(x + i*width, accs, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[0].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[0].set_title('Finetuned Models: Accuracy by Style', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x + width)\n",
    "    axes[0].set_xticklabels([m.upper() for m in ft_models])\n",
    "    axes[0].legend(title='Resume Style')\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    axes[0].axhline(y=0.33, color='gray', linestyle='--', alpha=0.5, label='Random guess')\n",
    "\n",
    "# LLM models\n",
    "if llm_df is not None:\n",
    "    x = np.arange(len(llm_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        accs = [llm_df[(llm_df['model'] == m) & (llm_df['style'] == style)]['correct'].mean() \n",
    "                for m in llm_models]\n",
    "        axes[1].bar(x + i*width, accs, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[1].set_ylabel('Accuracy', fontsize=12)\n",
    "    axes[1].set_title('LLMs: Accuracy by Style', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels([m.upper() for m in llm_models])\n",
    "    axes[1].legend(title='Resume Style')\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    axes[1].axhline(y=0.33, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz1_accuracy_by_style.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 2: Prediction Bias (Rank Difference) by Style\n",
    "\n",
    "### What This Analysis Shows\n",
    "This chart shows the **average rank difference** for each style, where:\n",
    "- Rank difference = Predicted seniority rank - True seniority rank\n",
    "- Seniority ranks: Junior=0, Mid=1, Senior=2\n",
    "\n",
    "### Why This Matters\n",
    "Accuracy alone doesn't tell the full story. A model could be 50% accurate but consistently wrong in one direction:\n",
    "- **Always underestimating** (predicting junior when they're senior)\n",
    "- **Always overestimating** (predicting senior when they're junior)\n",
    "\n",
    "This is critical for fairness: If overstated resumes get systematically overestimated, people who exaggerate their achievements get unfair advantages.\n",
    "\n",
    "### How to Interpret\n",
    "- **Bar at 0** = No systematic bias (predictions are scattered evenly around truth)\n",
    "- **Positive bar (+)** = Model overestimates seniority (predicts higher than reality)\n",
    "- **Negative bar (-)** = Model underestimates seniority (predicts lower than reality)\n",
    "- **+1.0** means on average predicting ONE full level too high (all juniors → mid, all mids → senior)\n",
    "\n",
    "### What to Look For\n",
    "- Is the overstated bar higher than neutral? → Style bias exists\n",
    "- Is the understated bar lower than neutral? → Humble language gets penalized\n",
    "- Are LLMs closer to 0 than finetuned models? → LLMs are more robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Finetuned models\n",
    "if ft_df is not None:\n",
    "    x = np.arange(len(ft_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        diffs = [ft_df[(ft_df['model'] == m) & (ft_df['style'] == style)]['rank_diff'].mean() \n",
    "                 for m in ft_models]\n",
    "        axes[0].bar(x + i*width, diffs, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[0].set_ylabel('Avg Rank Difference (+ve = overestimate)', fontsize=12)\n",
    "    axes[0].set_title('Finetuned Models: Prediction Bias by Style', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x + width)\n",
    "    axes[0].set_xticklabels([m.upper() for m in ft_models])\n",
    "    axes[0].legend(title='Resume Style')\n",
    "    axes[0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    axes[0].set_ylim(-1, 1.5)\n",
    "\n",
    "# LLM models\n",
    "if llm_df is not None:\n",
    "    x = np.arange(len(llm_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        diffs = [llm_df[(llm_df['model'] == m) & (llm_df['style'] == style)]['rank_diff'].mean() \n",
    "                 for m in llm_models]\n",
    "        axes[1].bar(x + i*width, diffs, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[1].set_ylabel('Avg Rank Difference (+ve = overestimate)', fontsize=12)\n",
    "    axes[1].set_title('LLMs: Prediction Bias by Style', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels([m.upper() for m in llm_models])\n",
    "    axes[1].legend(title='Resume Style')\n",
    "    axes[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    axes[1].set_ylim(-0.5, 0.6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz2_bias_by_style.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 3: Juniors Predicted as Senior (\"Promotion Rate\")\n",
    "\n",
    "### What This Analysis Shows\n",
    "This is the most direct measure of style bias: What percentage of **actual juniors** get mispredicted as **senior** based on writing style?\n",
    "\n",
    "### Why This Matters\n",
    "This represents the worst-case scenario for hiring bias:\n",
    "- A junior with 1-2 years experience writes confidently → AI thinks they're senior\n",
    "- They might get hired for a senior role they're not qualified for\n",
    "- Meanwhile, a senior who writes modestly might get overlooked\n",
    "\n",
    "This is especially problematic because:\n",
    "1. It rewards people who exaggerate (ethical concern)\n",
    "2. It penalizes people from cultures that value modesty (cultural bias)\n",
    "3. It can lead to bad hires (business impact)\n",
    "\n",
    "### How to Interpret\n",
    "- **Ideal: 0%** for all styles - no juniors should be predicted as senior\n",
    "- **Overstated >> Neutral** = Style bias is actively promoting unqualified candidates\n",
    "- **50%+ for overstated** = Extreme bias - overstating has a coin-flip chance of getting you \"promoted\"\n",
    "\n",
    "### What to Look For\n",
    "Compare the red bars (overstated) vs blue bars (neutral). The gap between them quantifies how much confident writing \"promotes\" juniors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "def calc_promotion_rate(df, model, style):\n",
    "    subset = df[(df['model'] == model) & (df['style'] == style) & (df['true_seniority'] == 'junior')]\n",
    "    if len(subset) == 0:\n",
    "        return 0\n",
    "    return (subset['prediction'] == 'senior').mean() * 100\n",
    "\n",
    "# Finetuned models\n",
    "if ft_df is not None:\n",
    "    x = np.arange(len(ft_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        rates = [calc_promotion_rate(ft_df, m, style) for m in ft_models]\n",
    "        axes[0].bar(x + i*width, rates, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[0].set_ylabel('% of Juniors Predicted as Senior', fontsize=12)\n",
    "    axes[0].set_title('Finetuned Models: Junior → Senior \"Promotion\" Rate', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x + width)\n",
    "    axes[0].set_xticklabels([m.upper() for m in ft_models])\n",
    "    axes[0].legend(title='Resume Style')\n",
    "    axes[0].set_ylim(0, 100)\n",
    "\n",
    "# LLM models  \n",
    "if llm_df is not None:\n",
    "    x = np.arange(len(llm_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, style in enumerate(styles):\n",
    "        rates = [calc_promotion_rate(llm_df, m, style) for m in llm_models]\n",
    "        axes[1].bar(x + i*width, rates, width, label=style.capitalize(), color=colors[style])\n",
    "    \n",
    "    axes[1].set_ylabel('% of Juniors Predicted as Senior', fontsize=12)\n",
    "    axes[1].set_title('LLMs: Junior → Senior \"Promotion\" Rate', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels([m.upper() for m in llm_models])\n",
    "    axes[1].legend(title='Resume Style')\n",
    "    axes[1].set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz3_junior_promotion_rate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 4: Prediction Inconsistency (Same Person, Different Styles)\n",
    "\n",
    "### What This Analysis Shows\n",
    "For each person, we check if their 3 resume versions (neutral, overstated, understated) get the same prediction. This chart shows what percentage of people got **inconsistent predictions** - meaning changing their writing style changed the AI's judgment.\n",
    "\n",
    "### Why This Matters\n",
    "This is the purest test of style bias because:\n",
    "- The resume content is IDENTICAL (same experience, skills, education)\n",
    "- Only the WORDING changed\n",
    "- If predictions differ, the model is reacting to style, not substance\n",
    "\n",
    "An unbiased model should give the SAME prediction regardless of how confidently someone writes. A 40% inconsistency rate means 40% of candidates would get different evaluations just by changing their word choices.\n",
    "\n",
    "### How to Interpret\n",
    "- **0% = Perfect** - Style has zero influence on predictions\n",
    "- **10-20%** = Moderate bias - some influence from writing style\n",
    "- **30%+** = Severe bias - writing style significantly affects outcomes\n",
    "- **50%+** = Critical - might as well flip a coin based on style\n",
    "\n",
    "### What to Look For\n",
    "Which models are most consistent? Lower bars indicate more robust, fair models that judge on content rather than presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_inconsistency(df, model):\n",
    "    model_df = df[df['model'] == model]\n",
    "    inconsistent = 0\n",
    "    total = 0\n",
    "    for idx in model_df['idx'].unique():\n",
    "        preds = model_df[model_df['idx'] == idx]['prediction'].unique()\n",
    "        if len(preds) > 1:\n",
    "            inconsistent += 1\n",
    "        total += 1\n",
    "    return (inconsistent / total * 100) if total > 0 else 0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "all_data = []\n",
    "all_labels = []\n",
    "all_colors = []\n",
    "\n",
    "if ft_df is not None:\n",
    "    for m in ft_models:\n",
    "        all_data.append(calc_inconsistency(ft_df, m))\n",
    "        all_labels.append(m.upper())\n",
    "        all_colors.append('#e74c3c')  # Red for finetuned\n",
    "\n",
    "if llm_df is not None:\n",
    "    for m in llm_models:\n",
    "        all_data.append(calc_inconsistency(llm_df, m))\n",
    "        all_labels.append(m.upper())\n",
    "        all_colors.append('#3498db')  # Blue for LLMs\n",
    "\n",
    "bars = ax.bar(all_labels, all_data, color=all_colors)\n",
    "ax.set_ylabel('Inconsistency Rate (%)', fontsize=12)\n",
    "ax.set_title('Prediction Inconsistency: Same Person, Different Writing Styles', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, all_data):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.1f}%', \n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='#e74c3c', label='Finetuned'),\n",
    "                   Patch(facecolor='#3498db', label='LLM')]\n",
    "ax.legend(handles=legend_elements, loc='upper right')\n",
    "\n",
    "# Add threshold lines\n",
    "ax.axhline(y=10, color='green', linestyle='--', alpha=0.7, label='Good (<10%)')\n",
    "ax.axhline(y=30, color='orange', linestyle='--', alpha=0.7, label='Moderate')\n",
    "ax.axhline(y=50, color='red', linestyle='--', alpha=0.7, label='Severe')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz4_inconsistency_rate.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 5: Word Influence - Power Words in Predictions\n",
    "\n",
    "### What This Analysis Shows\n",
    "This chart shows the average number of \"power words\" (strategic, spearheaded, architected, etc.) found in resumes that received each prediction level.\n",
    "\n",
    "### Why This Matters\n",
    "This reveals WHAT the model is actually responding to. If resumes predicted as \"senior\" have 15 power words on average while \"junior\" predictions have only 2, the model is clearly associating confident language with seniority - regardless of actual experience.\n",
    "\n",
    "Power words include:\n",
    "- Strong verbs: spearheaded, revolutionized, pioneered, architected\n",
    "- Impact words: strategic, critical, comprehensive, innovative\n",
    "- Scale words: enterprise-wide, global, organization-wide\n",
    "\n",
    "### How to Interpret\n",
    "- **Similar heights across predictions** = Good! Model ignores word choice.\n",
    "- **Senior >> Junior** = Model associates power words with seniority (biased)\n",
    "- **Steep gradient (senior to junior)** = Strong word-based bias\n",
    "\n",
    "### What to Look For\n",
    "Compare the gradient (slope from senior to junior) across models. Flatter = more robust to word choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWER_WORDS = {\n",
    "    'spearheaded', 'revolutionized', 'pioneered', 'orchestrated', 'architected',\n",
    "    'transformed', 'drove', 'championed', 'accelerated', 'maximized',\n",
    "    'optimized', 'elevated', 'propelled', 'commanded', 'masterminded',\n",
    "    'dramatically', 'significantly', 'substantially', 'exponentially', 'exceptionally',\n",
    "    'outstanding', 'exceptional', 'remarkable', 'extraordinary', 'tremendous',\n",
    "    'critical', 'crucial', 'vital', 'strategic', 'innovative',\n",
    "    'enterprise', 'comprehensive', 'extensive', 'robust', 'cutting-edge',\n",
    "    'visionary', 'influential', 'instrumental', 'pivotal', 'key'\n",
    "}\n",
    "\n",
    "def count_power_words(text):\n",
    "    words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "    return sum(1 for w in words if w in POWER_WORDS)\n",
    "\n",
    "# Add power word counts if not present\n",
    "if ft_df is not None and 'power_count' not in ft_df.columns:\n",
    "    ft_df['power_count'] = ft_df['resume_text'].apply(count_power_words)\n",
    "if llm_df is not None and 'power_count' not in llm_df.columns:\n",
    "    llm_df['power_count'] = llm_df['resume_text'].apply(count_power_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "pred_levels = ['senior', 'mid', 'junior']\n",
    "pred_colors = {'senior': '#e74c3c', 'mid': '#f39c12', 'junior': '#27ae60'}\n",
    "\n",
    "# Finetuned models\n",
    "if ft_df is not None:\n",
    "    x = np.arange(len(ft_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, pred in enumerate(pred_levels):\n",
    "        counts = [ft_df[(ft_df['model'] == m) & (ft_df['prediction'] == pred)]['power_count'].mean() \n",
    "                  for m in ft_models]\n",
    "        axes[0].bar(x + i*width, counts, width, label=pred.capitalize(), color=pred_colors[pred])\n",
    "    \n",
    "    axes[0].set_ylabel('Avg Power Word Count', fontsize=12)\n",
    "    axes[0].set_title('Finetuned Models: Power Words by Prediction', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x + width)\n",
    "    axes[0].set_xticklabels([m.upper() for m in ft_models])\n",
    "    axes[0].legend(title='Predicted Level')\n",
    "\n",
    "# LLM models\n",
    "if llm_df is not None:\n",
    "    x = np.arange(len(llm_models))\n",
    "    width = 0.25\n",
    "    \n",
    "    for i, pred in enumerate(pred_levels):\n",
    "        counts = [llm_df[(llm_df['model'] == m) & (llm_df['prediction'] == pred)]['power_count'].mean() \n",
    "                  for m in llm_models]\n",
    "        axes[1].bar(x + i*width, counts, width, label=pred.capitalize(), color=pred_colors[pred])\n",
    "    \n",
    "    axes[1].set_ylabel('Avg Power Word Count', fontsize=12)\n",
    "    axes[1].set_title('LLMs: Power Words by Prediction', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticks(x + width)\n",
    "    axes[1].set_xticklabels([m.upper() for m in llm_models])\n",
    "    axes[1].legend(title='Predicted Level')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz5_power_words_by_prediction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 6: Misclassification Analysis - Where Does Bias Hurt Most?\n",
    "\n",
    "### What This Analysis Shows\n",
    "This chart focuses on the two most harmful types of errors:\n",
    "1. **Juniors predicted as Senior** (over-promotion) - unqualified candidates get elevated\n",
    "2. **Seniors predicted as Junior** (under-recognition) - qualified candidates get overlooked\n",
    "\n",
    "For each type, we show which writing style was most associated with the error.\n",
    "\n",
    "### Why This Matters\n",
    "These extreme misclassifications have real consequences:\n",
    "- Juniors hired as seniors may fail in the role, costing the company\n",
    "- Seniors rejected as juniors miss opportunities they deserve\n",
    "\n",
    "By breaking down which style caused the error, we can see:\n",
    "- Are overstated juniors being mistaken for seniors? (reward for exaggeration)\n",
    "- Are understated seniors being mistaken for juniors? (penalty for modesty)\n",
    "\n",
    "### How to Interpret\n",
    "- **Overstated dominates over-promotions** = Confident language unfairly elevates juniors\n",
    "- **Understated dominates under-recognitions** = Modest language unfairly demotes seniors\n",
    "- **Both happening** = The model is fundamentally confused by style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "def get_misclass_breakdown(df, model, true_level, pred_level):\n",
    "    subset = df[(df['model'] == model) & (df['true_seniority'] == true_level) & (df['prediction'] == pred_level)]\n",
    "    if len(subset) == 0:\n",
    "        return {'overstated': 0, 'neutral': 0, 'understated': 0}\n",
    "    return subset['style'].value_counts().to_dict()\n",
    "\n",
    "# Juniors predicted as Senior\n",
    "if ft_df is not None:\n",
    "    for idx, model in enumerate(ft_models):\n",
    "        breakdown = get_misclass_breakdown(ft_df, model, 'junior', 'senior')\n",
    "        total = sum(breakdown.values())\n",
    "        if total > 0:\n",
    "            sizes = [breakdown.get(s, 0) for s in styles]\n",
    "            axes[0, idx].pie(sizes, labels=[f\"{s}\\n({breakdown.get(s,0)})\" for s in styles], \n",
    "                            colors=[colors[s] for s in styles], autopct='%1.0f%%', startangle=90)\n",
    "            axes[0, idx].set_title(f'{model.upper()}: Juniors → Senior (n={total})', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axes[0, idx].text(0.5, 0.5, 'No cases', ha='center', va='center', fontsize=14)\n",
    "            axes[0, idx].set_title(f'{model.upper()}: Juniors → Senior', fontsize=12, fontweight='bold')\n",
    "            axes[0, idx].axis('off')\n",
    "\n",
    "# Seniors predicted as Junior\n",
    "if ft_df is not None:\n",
    "    for idx, model in enumerate(ft_models):\n",
    "        breakdown = get_misclass_breakdown(ft_df, model, 'senior', 'junior')\n",
    "        total = sum(breakdown.values())\n",
    "        if total > 0:\n",
    "            sizes = [breakdown.get(s, 0) for s in styles]\n",
    "            axes[1, idx].pie(sizes, labels=[f\"{s}\\n({breakdown.get(s,0)})\" for s in styles], \n",
    "                            colors=[colors[s] for s in styles], autopct='%1.0f%%', startangle=90)\n",
    "            axes[1, idx].set_title(f'{model.upper()}: Seniors → Junior (n={total})', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            axes[1, idx].text(0.5, 0.5, 'No cases', ha='center', va='center', fontsize=14)\n",
    "            axes[1, idx].set_title(f'{model.upper()}: Seniors → Junior', fontsize=12, fontweight='bold')\n",
    "            axes[1, idx].axis('off')\n",
    "\n",
    "plt.suptitle('Misclassification Breakdown by Style (Finetuned Models)', fontsize=16, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz6_misclassification_breakdown_ft.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 7: Heatmap - Full Prediction Matrix by Style\n",
    "\n",
    "### What This Analysis Shows\n",
    "A heatmap showing the distribution of predictions for each true seniority level, broken down by writing style. Each row is a true level (junior/mid/senior), and colors indicate how predictions were distributed.\n",
    "\n",
    "### Why This Matters\n",
    "This gives the complete picture of how style affects predictions at every level:\n",
    "- Do juniors get different predictions based on style?\n",
    "- Do seniors get different predictions based on style?\n",
    "- Is the bias symmetric or does it affect some levels more?\n",
    "\n",
    "### How to Interpret\n",
    "- **Diagonal dominance (dark on diagonal)** = Good! Model predicts correctly regardless of style.\n",
    "- **Off-diagonal heat for overstated** = Overstated resumes get mispredicted more.\n",
    "- **Upper-right heat for overstated** = Overstated causes over-promotion.\n",
    "- **Lower-left heat for understated** = Understated causes under-recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prediction_heatmap(df, model, ax, title):\n",
    "    model_df = df[df['model'] == model]\n",
    "    \n",
    "    data = []\n",
    "    row_labels = []\n",
    "    \n",
    "    for true_level in ['junior', 'mid', 'senior']:\n",
    "        for style in styles:\n",
    "            subset = model_df[(model_df['true_seniority'] == true_level) & (model_df['style'] == style)]\n",
    "            if len(subset) > 0:\n",
    "                preds = subset['prediction'].value_counts(normalize=True)\n",
    "                row = [preds.get('junior', 0), preds.get('mid', 0), preds.get('senior', 0)]\n",
    "            else:\n",
    "                row = [0, 0, 0]\n",
    "            data.append(row)\n",
    "            row_labels.append(f\"{true_level[:3].upper()}-{style[:4]}\")\n",
    "    \n",
    "    sns.heatmap(data, annot=True, fmt='.0%', cmap='YlOrRd', ax=ax,\n",
    "                xticklabels=['Junior', 'Mid', 'Senior'], yticklabels=row_labels,\n",
    "                vmin=0, vmax=1)\n",
    "    ax.set_title(title, fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Predicted Level')\n",
    "    ax.set_ylabel('True Level - Style')\n",
    "\n",
    "if ft_df is not None:\n",
    "    fig, axes = plt.subplots(1, len(ft_models), figsize=(8*len(ft_models), 10))\n",
    "    if len(ft_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, model in enumerate(ft_models):\n",
    "        create_prediction_heatmap(ft_df, model, axes[idx], f'{model.upper()}')\n",
    "    \n",
    "    plt.suptitle('Prediction Distribution Heatmap (Finetuned Models)', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('viz7_heatmap_finetuned.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if llm_df is not None:\n",
    "    fig, axes = plt.subplots(1, len(llm_models), figsize=(8*len(llm_models), 10))\n",
    "    if len(llm_models) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, model in enumerate(llm_models):\n",
    "        create_prediction_heatmap(llm_df, model, axes[idx], f'{model.upper()}')\n",
    "    \n",
    "    plt.suptitle('Prediction Distribution Heatmap (LLMs)', fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('viz7_heatmap_llm.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Visual 8: Summary Dashboard\n",
    "\n",
    "### What This Analysis Shows\n",
    "A comprehensive comparison of all models on key metrics:\n",
    "- Overall accuracy\n",
    "- Style bias magnitude (overstated vs understated rank difference)\n",
    "- Inconsistency rate\n",
    "\n",
    "### How to Interpret\n",
    "The ideal model has:\n",
    "- High accuracy (tall green bar)\n",
    "- Low style bias (short red bar)\n",
    "- Low inconsistency (short orange bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "metrics = []\n",
    "labels = []\n",
    "\n",
    "def calc_metrics(df, model):\n",
    "    model_df = df[df['model'] == model]\n",
    "    \n",
    "    acc = model_df['correct'].mean()\n",
    "    \n",
    "    over_bias = model_df[model_df['style'] == 'overstated']['rank_diff'].mean()\n",
    "    under_bias = model_df[model_df['style'] == 'understated']['rank_diff'].mean()\n",
    "    style_bias = abs(over_bias - under_bias)\n",
    "    \n",
    "    inconsistent = 0\n",
    "    for idx in model_df['idx'].unique():\n",
    "        if len(model_df[model_df['idx'] == idx]['prediction'].unique()) > 1:\n",
    "            inconsistent += 1\n",
    "    total = len(model_df['idx'].unique())\n",
    "    inconsistency = inconsistent / total if total > 0 else 0\n",
    "    \n",
    "    return acc, style_bias, inconsistency\n",
    "\n",
    "if ft_df is not None:\n",
    "    for m in ft_models:\n",
    "        metrics.append(calc_metrics(ft_df, m))\n",
    "        labels.append(f\"{m.upper()}\\n(Finetuned)\")\n",
    "\n",
    "if llm_df is not None:\n",
    "    for m in llm_models:\n",
    "        metrics.append(calc_metrics(llm_df, m))\n",
    "        labels.append(f\"{m.upper()}\\n(LLM)\")\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "\n",
    "accs = [m[0] for m in metrics]\n",
    "biases = [m[1] for m in metrics]\n",
    "incons = [m[2] for m in metrics]\n",
    "\n",
    "ax.bar(x - width, accs, width, label='Accuracy', color='#27ae60')\n",
    "ax.bar(x, biases, width, label='Style Bias Magnitude', color='#e74c3c')\n",
    "ax.bar(x + width, incons, width, label='Inconsistency Rate', color='#f39c12')\n",
    "\n",
    "ax.set_ylabel('Score (0-1)', fontsize=12)\n",
    "ax.set_title('Model Comparison: Accuracy vs Style Bias vs Inconsistency', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_ylim(0, 1.5)\n",
    "\n",
    "# Add annotations\n",
    "ax.axhline(y=0.33, color='gray', linestyle=':', alpha=0.5)\n",
    "ax.text(len(labels)-0.5, 0.35, 'Random guess', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('viz8_summary_dashboard.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "All visualizations have been saved as PNG files:\n",
    "1. `viz1_accuracy_by_style.png` - How accuracy varies by writing style\n",
    "2. `viz2_bias_by_style.png` - Direction and magnitude of prediction bias\n",
    "3. `viz3_junior_promotion_rate.png` - How often juniors get \"promoted\" to senior\n",
    "4. `viz4_inconsistency_rate.png` - How often style changes the prediction\n",
    "5. `viz5_power_words_by_prediction.png` - Word count analysis\n",
    "6. `viz6_misclassification_breakdown_ft.png` - Which styles cause which errors\n",
    "7. `viz7_heatmap_*.png` - Full prediction distribution\n",
    "8. `viz8_summary_dashboard.png` - Overall model comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
