{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: SOTA LLMs (GPT-5, Gemini 3 Pro, Claude Sonnet 4.5)\n",
    "\n",
    "Testing if state-of-the-art LLMs have **bias towards resume writing style**.\n",
    "\n",
    "**Metrics:** Accuracy, Inconsistency Rate, Rank Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "model_names = ['gpt5', 'gemini3', 'sonnet45']\n",
    "\n",
    "# Check if predictions already exist\n",
    "if os.path.exists(\"llm_predictions.csv\"):\n",
    "    print(\"Loading existing predictions...\")\n",
    "    results_df = pd.read_csv(\"llm_predictions.csv\")\n",
    "    valid_df = results_df[~results_df['prediction'].isin(['error', 'unknown'])].copy()\n",
    "    print(f\"Loaded {len(valid_df)} valid predictions\")\n",
    "else:\n",
    "    print(\"Running predictions...\")\n",
    "    import json\n",
    "    import urllib.request\n",
    "    from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "    import time\n",
    "    \n",
    "    OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "    OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "    if not OPENROUTER_API_KEY:\n",
    "        raise ValueError(\"Set OPENROUTER_API_KEY environment variable\")\n",
    "    \n",
    "    MODELS = {'gpt5': 'openai/gpt-5', 'gemini3': 'google/gemini-3-pro-preview', 'sonnet45': 'anthropic/claude-sonnet-4.5'}\n",
    "    \n",
    "    PROMPT = \"\"\"Classify this resume's seniority level. Respond with ONLY one word: junior, mid, or senior.\\n\\nResume:\\n{resume_text}\\n\\nSeniority level:\"\"\"\n",
    "    \n",
    "    def call_api(model_id, text):\n",
    "        try:\n",
    "            payload = json.dumps({\"model\": model_id, \"messages\": [{\"role\": \"user\", \"content\": PROMPT.format(resume_text=text[:8000])}]}).encode(\"utf-8\")\n",
    "            req = urllib.request.Request(OPENROUTER_URL, data=payload, headers={\"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\", \"Content-Type\": \"application/json\"})\n",
    "            with urllib.request.urlopen(req, timeout=120) as resp:\n",
    "                response = json.loads(resp.read().decode(\"utf-8\"))[\"choices\"][0][\"message\"][\"content\"].strip().lower()\n",
    "            return \"junior\" if \"junior\" in response else \"mid\" if \"mid\" in response else \"senior\" if \"senior\" in response else \"unknown\"\n",
    "        except:\n",
    "            return \"error\"\n",
    "    \n",
    "    tasks = [{'idx': idx, 'true_seniority': row['seniority'], 'style': style, 'model': m, 'model_id': MODELS[m], 'text': str(row[style])}\n",
    "             for idx, row in df.iterrows() for style in styles for m in model_names]\n",
    "    \n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=15) as executor:\n",
    "        futures = {executor.submit(lambda t: {**t, 'prediction': call_api(t['model_id'], t['text']), 'correct': call_api(t['model_id'], t['text']) == t['true_seniority']}, task): task for task in tasks}\n",
    "        for i, future in enumerate(as_completed(futures)):\n",
    "            results.append(future.result())\n",
    "            if (i+1) % 100 == 0: print(f\"Processed {i+1}/{len(tasks)}\")\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(\"llm_predictions.csv\", index=False)\n",
    "    valid_df = results_df[~results_df['prediction'].isin(['error', 'unknown'])].copy()\n",
    "    print(f\"Saved {len(valid_df)} valid predictions\")\n",
    "\n",
    "# Add rank columns\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "valid_df['true_rank'] = valid_df['true_seniority'].map(seniority_rank)\n",
    "valid_df['pred_rank'] = valid_df['prediction'].map(seniority_rank)\n",
    "valid_df['rank_diff'] = valid_df['pred_rank'] - valid_df['true_rank']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 1: Accuracy by Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACCURACY BY STYLE\")\n",
    "print(\"=\" * 50)\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    for style in styles:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        if len(style_df) > 0:\n",
    "            print(f\"  {style:<15}: {style_df['correct'].mean():.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(styles))\n",
    "width = 0.25\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    accs = [model_df[model_df['style'] == s]['correct'].mean() if len(model_df[model_df['style'] == s]) > 0 else 0 for s in styles]\n",
    "    ax.bar(x + i*width, accs, width, label=model_name.upper())\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Accuracy by Resume Style')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 2: Inconsistency Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"INCONSISTENCY RATE\")\n",
    "print(\"=\" * 50)\n",
    "inconsistency_rates = {}\n",
    "for model_name in model_names:\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    inconsistent = sum(1 for idx in df.index if len(model_df[model_df['idx'] == idx]) == 3 and len(model_df[model_df['idx'] == idx]['prediction'].unique()) > 1)\n",
    "    total = model_df['idx'].nunique()\n",
    "    rate = inconsistent / total * 100 if total > 0 else 0\n",
    "    inconsistency_rates[model_name] = rate\n",
    "    print(f\"{model_name.upper()}: {inconsistent}/{total} ({rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.bar(model_names, [inconsistency_rates[m] for m in model_names], color=['steelblue', 'coral', 'seagreen'])\n",
    "ax.set_ylabel('Inconsistency Rate (%)')\n",
    "ax.set_title('Prediction Inconsistency by Model')\n",
    "ax.set_ylim(0, 100)\n",
    "for i, m in enumerate(model_names):\n",
    "    ax.text(i, inconsistency_rates[m] + 2, f'{inconsistency_rates[m]:.1f}%', ha='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metric 3: Rank Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RANK DIFFERENCE BY STYLE\")\n",
    "print(\"=\" * 50)\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    for style in styles:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        if len(style_df) > 0:\n",
    "            print(f\"  {style:<15}: {style_df['rank_diff'].mean():+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = np.arange(len(styles))\n",
    "width = 0.25\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    diffs = [model_df[model_df['style'] == s]['rank_diff'].mean() if len(model_df[model_df['style'] == s]) > 0 else 0 for s in styles]\n",
    "    ax.bar(x + i*width, diffs, width, label=model_name.upper())\n",
    "ax.set_ylabel('Avg Rank Difference')\n",
    "ax.set_title('Prediction Bias by Style (+ = overestimate)')\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
