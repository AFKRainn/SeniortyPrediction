{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 2: SOTA LLMs (GPT-5, Gemini 3 Pro, Claude Sonnet 4.5)\n",
    "\n",
    "This notebook tests if state-of-the-art LLMs have **bias towards resume writing style**:\n",
    "- Do overstated resumes get higher seniority predictions?\n",
    "- Do understated resumes get lower seniority predictions?\n",
    "- Which words influence the model's decisions?\n",
    "\n",
    "**Models tested:** GPT-5, Gemini 3 Pro, Claude Sonnet 4.5 (run in parallel via OpenRouter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "\n",
    "# OpenRouter Config\n",
    "OPENROUTER_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "if not OPENROUTER_API_KEY:\n",
    "    raise ValueError(\"Set OPENROUTER_API_KEY environment variable\")\n",
    "\n",
    "# Models to test\n",
    "MODELS = {\n",
    "    'gpt5': 'openai/gpt-5',\n",
    "    'gemini3': 'google/gemini-3-pro',\n",
    "    'sonnet45': 'anthropic/claude-sonnet-4.5'\n",
    "}\n",
    "\n",
    "print(f\"Testing {len(MODELS)} models: {list(MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df = pd.read_csv(\"../Test 2 Data/test2_resumes.csv\")\n",
    "print(f\"Loaded {len(df)} resume sets (each has neutral, overstated, understated versions)\")\n",
    "print(f\"Seniority distribution: {df['seniority'].value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"Classify this resume's seniority level. Respond with ONLY one word: junior, mid, or senior.\n",
    "\n",
    "Resume:\n",
    "{resume_text}\n",
    "\n",
    "Seniority level:\"\"\"\n",
    "\n",
    "def call_api(model_id, resume_text, max_retries=2):\n",
    "    \"\"\"Call OpenRouter API and return predicted label.\"\"\"\n",
    "    prompt = PROMPT_TEMPLATE.format(resume_text=resume_text[:8000])  # Truncate if too long\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            payload = json.dumps({\n",
    "                \"model\": model_id,\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "                \"max_tokens\": 16\n",
    "            }).encode(\"utf-8\")\n",
    "            \n",
    "            req = urllib.request.Request(\n",
    "                OPENROUTER_URL,\n",
    "                data=payload,\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            with urllib.request.urlopen(req, timeout=120) as resp:\n",
    "                result = json.loads(resp.read().decode(\"utf-8\"))\n",
    "            \n",
    "            response = result[\"choices\"][0][\"message\"][\"content\"].strip().lower()\n",
    "            \n",
    "            # Extract valid label\n",
    "            if \"junior\" in response:\n",
    "                return \"junior\"\n",
    "            elif \"mid\" in response:\n",
    "                return \"mid\"\n",
    "            elif \"senior\" in response:\n",
    "                return \"senior\"\n",
    "            else:\n",
    "                return \"unknown\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return \"error\"\n",
    "    \n",
    "    return \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Predictions (Parallel - 3 Models Simultaneously)\n",
    "\n",
    "For each resume version, we call all 3 LLMs in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORKERS = 15  # 3 models * 5 concurrent requests per model\n",
    "\n",
    "# Build all tasks\n",
    "tasks = []\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    true_seniority = row['seniority']\n",
    "    for style in styles:\n",
    "        resume_text = str(row[style])\n",
    "        for model_name, model_id in MODELS.items():\n",
    "            tasks.append({\n",
    "                'idx': idx,\n",
    "                'true_seniority': true_seniority,\n",
    "                'style': style,\n",
    "                'model': model_name,\n",
    "                'model_id': model_id,\n",
    "                'resume_text': resume_text\n",
    "            })\n",
    "\n",
    "print(f\"Total API calls to make: {len(tasks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_task(task):\n",
    "    \"\"\"Run a single prediction task.\"\"\"\n",
    "    pred = call_api(task['model_id'], task['resume_text'])\n",
    "    return {\n",
    "        'idx': task['idx'],\n",
    "        'true_seniority': task['true_seniority'],\n",
    "        'style': task['style'],\n",
    "        'model': task['model'],\n",
    "        'prediction': pred,\n",
    "        'correct': pred == task['true_seniority'],\n",
    "        'resume_text': task['resume_text']\n",
    "    }\n",
    "\n",
    "# Run all tasks in parallel\n",
    "results = []\n",
    "processed = 0\n",
    "n = len(tasks)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(run_task, task): task for task in tasks}\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            task = futures[future]\n",
    "            results.append({\n",
    "                'idx': task['idx'],\n",
    "                'true_seniority': task['true_seniority'],\n",
    "                'style': task['style'],\n",
    "                'model': task['model'],\n",
    "                'prediction': 'error',\n",
    "                'correct': False,\n",
    "                'resume_text': task['resume_text']\n",
    "            })\n",
    "        \n",
    "        processed += 1\n",
    "        if processed % 100 == 0 or processed == n:\n",
    "            print(f\"Processed {processed}/{n}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nTotal predictions: {len(results_df)}\")\n",
    "print(f\"Errors: {(results_df['prediction'] == 'error').sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv(\"llm_predictions.csv\", index=False)\n",
    "print(\"Saved to llm_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 1: Accuracy by Style\n",
    "\n",
    "Does writing style affect prediction accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out errors\n",
    "valid_df = results_df[~results_df['prediction'].isin(['error', 'unknown'])].copy()\n",
    "print(f\"Valid predictions: {len(valid_df)}/{len(results_df)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ACCURACY BY STYLE AND MODEL\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "model_names = list(MODELS.keys())\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 40)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    for style in ['neutral', 'overstated', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        if len(style_df) > 0:\n",
    "            acc = style_df['correct'].mean()\n",
    "            print(f\"  {style:<15}: {acc:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 2: Style Bias Detection\n",
    "\n",
    "Key question: Do overstated resumes get predicted as higher seniority than they actually are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map seniority to numeric\n",
    "seniority_rank = {'junior': 0, 'mid': 1, 'senior': 2}\n",
    "valid_df['true_rank'] = valid_df['true_seniority'].map(seniority_rank)\n",
    "valid_df['pred_rank'] = valid_df['prediction'].map(seniority_rank)\n",
    "valid_df['rank_diff'] = valid_df['pred_rank'] - valid_df['true_rank']\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION BIAS BY STYLE (rank_diff: +ve = overestimate, -ve = underestimate)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Style':<15} {'Avg Rank Diff':<18} {'Overestimates':<15} {'Underestimates'}\")\n",
    "    for style in ['overstated', 'neutral', 'understated']:\n",
    "        style_df = model_df[model_df['style'] == style]\n",
    "        if len(style_df) > 0:\n",
    "            avg_diff = style_df['rank_diff'].mean()\n",
    "            over = (style_df['rank_diff'] > 0).sum()\n",
    "            under = (style_df['rank_diff'] < 0).sum()\n",
    "            print(f\"{style:<15} {avg_diff:+.3f}{'':>12} {over:<15} {under}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"BIAS CHECK: Do overstated resumes get 'promoted'?\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    for true_level in ['junior', 'mid']:\n",
    "        for style in ['overstated', 'neutral', 'understated']:\n",
    "            subset = model_df[(model_df['true_seniority'] == true_level) & (model_df['style'] == style)]\n",
    "            if len(subset) > 0:\n",
    "                senior_preds = (subset['prediction'] == 'senior').sum()\n",
    "                total = len(subset)\n",
    "                pct = senior_preds / total * 100\n",
    "                print(f\"  {true_level.upper()} + {style:<12} â†’ predicted SENIOR: {senior_preds}/{total} ({pct:.1f}%)\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 3: Same Person, Different Predictions?\n",
    "\n",
    "For each person: do their 3 resume versions get different predictions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PREDICTION CONSISTENCY: Same person, different styles\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    inconsistent = 0\n",
    "    style_changes = []\n",
    "    \n",
    "    for idx in df.index:\n",
    "        person_df = model_df[model_df['idx'] == idx]\n",
    "        if len(person_df) == 3:  # All 3 styles present\n",
    "            preds = person_df.set_index('style')['prediction']\n",
    "            if len(set(preds)) > 1:\n",
    "                inconsistent += 1\n",
    "                style_changes.append({\n",
    "                    'idx': idx,\n",
    "                    'true': df.loc[idx, 'seniority'],\n",
    "                    'neutral': preds.get('neutral', '?'),\n",
    "                    'overstated': preds.get('overstated', '?'),\n",
    "                    'understated': preds.get('understated', '?')\n",
    "                })\n",
    "    \n",
    "    total_valid = len(model_df['idx'].unique())\n",
    "    print(f\"Resumes with INCONSISTENT predictions: {inconsistent}/{total_valid} ({inconsistent/total_valid*100:.1f}%)\")\n",
    "    \n",
    "    if style_changes:\n",
    "        changes_df = pd.DataFrame(style_changes[:10])\n",
    "        print(\"\\nSample inconsistent predictions:\")\n",
    "        print(changes_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 4: Word Influence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWER_WORDS = {\n",
    "    'spearheaded', 'revolutionized', 'pioneered', 'orchestrated', 'architected',\n",
    "    'transformed', 'drove', 'championed', 'accelerated', 'maximized',\n",
    "    'optimized', 'elevated', 'propelled', 'commanded', 'masterminded',\n",
    "    'dramatically', 'significantly', 'substantially', 'exponentially', 'exceptionally',\n",
    "    'outstanding', 'exceptional', 'remarkable', 'extraordinary', 'tremendous',\n",
    "    'critical', 'crucial', 'vital', 'strategic', 'innovative',\n",
    "    'enterprise', 'comprehensive', 'extensive', 'robust', 'cutting-edge',\n",
    "    'visionary', 'influential', 'instrumental', 'pivotal', 'key'\n",
    "}\n",
    "\n",
    "HUMBLE_WORDS = {\n",
    "    'helped', 'assisted', 'supported', 'contributed', 'participated',\n",
    "    'aided', 'collaborated', 'worked', 'involved', 'engaged',\n",
    "    'some', 'basic', 'minor', 'small', 'routine', 'standard', 'general',\n",
    "    'occasional', 'limited', 'partial', 'modest', 'simple',\n",
    "    'team', 'alongside', 'together', 'group', 'collective',\n",
    "    'somewhat', 'relatively', 'fairly', 'adequately', 'sufficiently'\n",
    "}\n",
    "\n",
    "def count_words(text, word_set):\n",
    "    words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "    return sum(1 for w in words if w in word_set)\n",
    "\n",
    "valid_df['power_count'] = valid_df['resume_text'].apply(lambda x: count_words(x, POWER_WORDS))\n",
    "valid_df['humble_count'] = valid_df['resume_text'].apply(lambda x: count_words(x, HUMBLE_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"WORD INFLUENCE: Power/Humble words in predictions\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"{'Prediction':<12} {'Avg Power Words':<18} {'Avg Humble Words'}\")\n",
    "    for pred in ['senior', 'mid', 'junior']:\n",
    "        pred_df = model_df[model_df['prediction'] == pred]\n",
    "        if len(pred_df) > 0:\n",
    "            avg_power = pred_df['power_count'].mean()\n",
    "            avg_humble = pred_df['humble_count'].mean()\n",
    "            print(f\"{pred:<12} {avg_power:.1f}{'':>14} {avg_humble:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_freq(texts, word_set):\n",
    "    counts = Counter()\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b[a-z]+\\b', str(text).lower())\n",
    "        for w in words:\n",
    "            if w in word_set:\n",
    "                counts[w] += 1\n",
    "    return counts\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TOP POWER WORDS IN SENIOR vs JUNIOR PREDICTIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    senior_texts = model_df[model_df['prediction'] == 'senior']['resume_text']\n",
    "    junior_texts = model_df[model_df['prediction'] == 'junior']['resume_text']\n",
    "    \n",
    "    senior_words = get_word_freq(senior_texts, POWER_WORDS)\n",
    "    junior_words = get_word_freq(junior_texts, POWER_WORDS)\n",
    "    \n",
    "    print(\"Power words in SENIOR predictions:\", dict(senior_words.most_common(5)))\n",
    "    print(\"Power words in JUNIOR predictions:\", dict(junior_words.most_common(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis 5: Misclassification Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"MISCLASSIFICATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    # Juniors predicted as Senior\n",
    "    overpromoted = model_df[(model_df['true_seniority'] == 'junior') & (model_df['prediction'] == 'senior')]\n",
    "    print(f\"\\nJuniors predicted as SENIOR: {len(overpromoted)}\")\n",
    "    if len(overpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {overpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {overpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {overpromoted['humble_count'].mean():.1f}\")\n",
    "    \n",
    "    # Seniors predicted as Junior\n",
    "    underpromoted = model_df[(model_df['true_seniority'] == 'senior') & (model_df['prediction'] == 'junior')]\n",
    "    print(f\"\\nSeniors predicted as JUNIOR: {len(underpromoted)}\")\n",
    "    if len(underpromoted) > 0:\n",
    "        print(f\"  Style breakdown: {underpromoted['style'].value_counts().to_dict()}\")\n",
    "        print(f\"  Avg power words: {underpromoted['power_count'].mean():.1f}\")\n",
    "        print(f\"  Avg humble words: {underpromoted['humble_count'].mean():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "styles = ['neutral', 'overstated', 'understated']\n",
    "x = np.arange(len(styles))\n",
    "width = 0.25\n",
    "\n",
    "# Plot 1: Accuracy by style\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    accs = [model_df[model_df['style'] == s]['correct'].mean() if len(model_df[model_df['style'] == s]) > 0 else 0 for s in styles]\n",
    "    axes[0].bar(x + i*width, accs, width, label=model_name.upper())\n",
    "\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('Accuracy by Resume Style')\n",
    "axes[0].set_xticks(x + width)\n",
    "axes[0].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Bias by style\n",
    "for i, model_name in enumerate(model_names):\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    diffs = [model_df[model_df['style'] == s]['rank_diff'].mean() if len(model_df[model_df['style'] == s]) > 0 else 0 for s in styles]\n",
    "    axes[1].bar(x + i*width, diffs, width, label=model_name.upper())\n",
    "\n",
    "axes[1].set_ylabel('Avg Rank Difference (+ = overestimate)')\n",
    "axes[1].set_title('Prediction Bias by Resume Style')\n",
    "axes[1].set_xticks(x + width)\n",
    "axes[1].set_xticklabels(['Neutral', 'Overstated', 'Understated'])\n",
    "axes[1].legend()\n",
    "axes[1].axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cross-Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"CROSS-MODEL COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Model':<12} {'Neutral Acc':<14} {'Over Acc':<12} {'Under Acc':<12} {'Over Bias':<12} {'Under Bias'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    neutral_acc = model_df[model_df['style'] == 'neutral']['correct'].mean()\n",
    "    over_acc = model_df[model_df['style'] == 'overstated']['correct'].mean()\n",
    "    under_acc = model_df[model_df['style'] == 'understated']['correct'].mean()\n",
    "    over_bias = model_df[model_df['style'] == 'overstated']['rank_diff'].mean()\n",
    "    under_bias = model_df[model_df['style'] == 'understated']['rank_diff'].mean()\n",
    "    \n",
    "    print(f\"{model_name:<12} {neutral_acc:.1%}{'':>8} {over_acc:.1%}{'':>6} {under_acc:.1%}{'':>6} {over_bias:+.3f}{'':>6} {under_bias:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"FINAL SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for model_name in model_names:\n",
    "    model_df = valid_df[valid_df['model'] == model_name]\n",
    "    \n",
    "    print(f\"\\n{model_name.upper()}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Accuracy\n",
    "    neutral_acc = model_df[model_df['style'] == 'neutral']['correct'].mean()\n",
    "    over_acc = model_df[model_df['style'] == 'overstated']['correct'].mean()\n",
    "    under_acc = model_df[model_df['style'] == 'understated']['correct'].mean()\n",
    "    \n",
    "    print(f\"Neutral accuracy:     {neutral_acc:.1%}\")\n",
    "    print(f\"Overstated accuracy:  {over_acc:.1%}\")\n",
    "    print(f\"Understated accuracy: {under_acc:.1%}\")\n",
    "    \n",
    "    # Bias\n",
    "    over_bias = model_df[model_df['style'] == 'overstated']['rank_diff'].mean()\n",
    "    under_bias = model_df[model_df['style'] == 'understated']['rank_diff'].mean()\n",
    "    \n",
    "    print(f\"\\nOverstated bias:  {over_bias:+.3f} ({'overestimates' if over_bias > 0 else 'underestimates'})\")\n",
    "    print(f\"Understated bias: {under_bias:+.3f} ({'overestimates' if under_bias > 0 else 'underestimates'})\")\n",
    "    \n",
    "    # Inconsistency\n",
    "    inconsistent = 0\n",
    "    for idx in df.index:\n",
    "        person_df = model_df[model_df['idx'] == idx]\n",
    "        if len(person_df) == 3 and len(person_df['prediction'].unique()) > 1:\n",
    "            inconsistent += 1\n",
    "    total = len(model_df['idx'].unique())\n",
    "    print(f\"\\nInconsistent predictions: {inconsistent}/{total} ({inconsistent/total*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
