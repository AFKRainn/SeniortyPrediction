{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c22e6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Feature columns: ['name', 'email', 'summary', 'linkedin', 'github', 'experience', 'total_years_experience', 'education', 'skills', 'projects', 'certifications']\n",
      "Labels: ['junior', 'mid', 'senior']\n",
      "Total samples: 2100\n",
      "Training samples: 1680, Validation samples: 420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24348/363519851.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0muse_amp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m                     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_encodings_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1739\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24348/363519851.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, column_encodings_batch)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m             \u001b[1;31m# Get [CLS] token embedding for this column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistilbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m             \u001b[0mcls_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# [batch_size, hidden_size]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mcolumn_embeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1739\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1740\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1741\u001b[0m     \u001b[1;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1748\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1751\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    719\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sdpa\"\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhead_mask_is_none\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m                 attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[0m\u001b[0;32m    721\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtgt_len\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m                 )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install dependencies (run once per session):\n",
    "# !pip install torch transformers pandas scikit-learn matplotlib seaborn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    DistilBertTokenizerFast,\n",
    "    DistilBertModel,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# === 1. Load the CSV file ===\n",
    "EXCEL_FILE = \"./cleaned_resumes.csv\"\n",
    "\n",
    "# === 2. Global configuration ===\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "MAX_LEN = 128  # Per column max length\n",
    "LEARNING_RATE = 2e-5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# === 3. Load and preprocess the data ===\n",
    "df = pd.read_csv(EXCEL_FILE)\n",
    "df = df.dropna(subset=['experience_level'])\n",
    "\n",
    "# Define which columns to use as separate inputs\n",
    "FEATURE_COLUMNS = [col for col in df.columns if col != 'experience_level']\n",
    "print(f\"Feature columns: {FEATURE_COLUMNS}\")\n",
    "\n",
    "# === 4. Create tokenizer and label mapping ===\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
    "label_names = sorted(df[\"experience_level\"].unique())\n",
    "label2id = {label: idx for idx, label in enumerate(label_names)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Labels: {label_names}\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "\n",
    "# === 5. Custom Dataset Class - Multi-Column Approach ===\n",
    "class MultiColumnResumeDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len, feature_columns):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.feature_columns = feature_columns\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        \n",
    "        # Tokenize each column separately\n",
    "        column_encodings = []\n",
    "        for col in self.feature_columns:\n",
    "            text = str(row[col]) if pd.notna(row[col]) else \"\"\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_len,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            column_encodings.append({\n",
    "                \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "                \"attention_mask\": encoding[\"attention_mask\"].squeeze(0)\n",
    "            })\n",
    "        \n",
    "        label = label2id[row[\"experience_level\"]]\n",
    "        \n",
    "        return column_encodings, label\n",
    "\n",
    "# === 6. Custom Model - Processes Each Column Separately ===\n",
    "class MultiColumnDistilBERT(nn.Module):\n",
    "    def __init__(self, num_columns, num_labels, hidden_size=768):\n",
    "        super(MultiColumnDistilBERT, self).__init__()\n",
    "        self.num_columns = num_columns\n",
    "        \n",
    "        # Shared DistilBERT encoder for all columns\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        \n",
    "        # Alternative: Use separate encoders per column (more parameters)\n",
    "        # self.encoders = nn.ModuleList([\n",
    "        #     DistilBertModel.from_pretrained(\"distilbert-base-uncased\") \n",
    "        #     for _ in range(num_columns)\n",
    "        # ])\n",
    "        \n",
    "        # Attention mechanism to weight column importance\n",
    "        self.column_attention = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_size // 2, num_labels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, column_encodings_batch):\n",
    "        batch_size = len(column_encodings_batch)\n",
    "        \n",
    "        # Process each column through DistilBERT\n",
    "        column_embeddings = []\n",
    "        \n",
    "        for col_idx in range(self.num_columns):\n",
    "            # Extract input_ids and attention_mask for this column across the batch\n",
    "            input_ids = torch.stack([column_encodings_batch[b][col_idx][\"input_ids\"] \n",
    "                                    for b in range(batch_size)])\n",
    "            attention_mask = torch.stack([column_encodings_batch[b][col_idx][\"attention_mask\"] \n",
    "                                         for b in range(batch_size)])\n",
    "            \n",
    "            # Get [CLS] token embedding for this column\n",
    "            outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_size]\n",
    "            column_embeddings.append(cls_embedding)\n",
    "        \n",
    "        # Stack all column embeddings: [batch_size, num_columns, hidden_size]\n",
    "        stacked_embeddings = torch.stack(column_embeddings, dim=1)\n",
    "        \n",
    "        # Apply attention to weight columns\n",
    "        attention_scores = self.column_attention(stacked_embeddings)  # [batch_size, num_columns, 1]\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        \n",
    "        # Weighted sum of column embeddings\n",
    "        weighted_embedding = torch.sum(stacked_embeddings * attention_weights, dim=1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(weighted_embedding)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# === 7. Custom collate function ===\n",
    "def collate_fn(batch):\n",
    "    column_encodings_batch = [item[0] for item in batch]\n",
    "    labels = torch.tensor([item[1] for item in batch])\n",
    "    return column_encodings_batch, labels\n",
    "\n",
    "# === 8. Split dataset into Training and Validation ===\n",
    "full_dataset = MultiColumnResumeDataset(df, tokenizer, MAX_LEN, FEATURE_COLUMNS)\n",
    "n_total = len(full_dataset)\n",
    "n_train = int(0.8 * n_total)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "train_ds, val_ds = random_split(full_dataset, [n_train, n_val])\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Training samples: {n_train}, Validation samples: {n_val}\")\n",
    "\n",
    "# === 9. Initialize model and optimizer ===\n",
    "model = MultiColumnDistilBERT(\n",
    "    num_columns=len(FEATURE_COLUMNS),\n",
    "    num_labels=len(label2id)\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# === 10. Set up Learning Rate Scheduler with warm-up ===\n",
    "total_steps = EPOCHS * len(train_loader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.1 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# === 11. Mixed Precision (FP16) setup ===\n",
    "use_amp = True if torch.cuda.is_available() else False\n",
    "scaler = torch.amp.GradScaler('cuda') if use_amp else None\n",
    "\n",
    "# === 12. Training Loop ===\n",
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "\n",
    "    for column_encodings_batch, labels in train_loader:\n",
    "        # Move column encodings to device\n",
    "        for batch_idx in range(len(column_encodings_batch)):\n",
    "            for col_idx in range(len(column_encodings_batch[batch_idx])):\n",
    "                column_encodings_batch[batch_idx][col_idx][\"input_ids\"] = \\\n",
    "                    column_encodings_batch[batch_idx][col_idx][\"input_ids\"].to(DEVICE)\n",
    "                column_encodings_batch[batch_idx][col_idx][\"attention_mask\"] = \\\n",
    "                    column_encodings_batch[batch_idx][col_idx][\"attention_mask\"].to(DEVICE)\n",
    "        \n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if use_amp:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                logits = model(column_encodings_batch)\n",
    "                loss = loss_fn(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            logits = model(column_encodings_batch)\n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # === Validation Phase ===\n",
    "    model.eval()\n",
    "    val_loss_total = 0.0\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for column_encodings_batch, labels in val_loader:\n",
    "            # Move column encodings to device\n",
    "            for batch_idx in range(len(column_encodings_batch)):\n",
    "                for col_idx in range(len(column_encodings_batch[batch_idx])):\n",
    "                    column_encodings_batch[batch_idx][col_idx][\"input_ids\"] = \\\n",
    "                        column_encodings_batch[batch_idx][col_idx][\"input_ids\"].to(DEVICE)\n",
    "                    column_encodings_batch[batch_idx][col_idx][\"attention_mask\"] = \\\n",
    "                        column_encodings_batch[batch_idx][col_idx][\"attention_mask\"].to(DEVICE)\n",
    "            \n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            if use_amp:\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    logits = model(column_encodings_batch)\n",
    "                    loss = loss_fn(logits, labels)\n",
    "            else:\n",
    "                logits = model(column_encodings_batch)\n",
    "                loss = loss_fn(logits, labels)\n",
    "\n",
    "            val_loss_total += loss.item()\n",
    "            preds = torch.argmax(logits, dim=1).cpu().tolist()\n",
    "            all_pred.extend(preds)\n",
    "            all_true.extend(labels.cpu().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss_total / len(val_loader)\n",
    "    val_acc = accuracy_score(all_true, all_pred) * 100\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "        f\"Train Loss: {avg_train_loss:.4f} | \"\n",
    "        f\"Val Loss: {avg_val_loss:.4f} | \"\n",
    "        f\"Val Acc: {val_acc:.2f}%\"\n",
    "    )\n",
    "\n",
    "    # === Save best model by Val Accuracy ===\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_distilbert_multicolumn.pt\")\n",
    "\n",
    "# === 13. Final Confusion Matrix ===\n",
    "conf_mat = confusion_matrix(all_true, all_pred, labels=list(label2id.values()))\n",
    "print(f\"\\n✅ Best Multi-Column DistilBERT Val Accuracy: {best_val_acc:.2f}%\")\n",
    "print(\"Final Confusion Matrix (Validation):\")\n",
    "print(conf_mat)\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_names,\n",
    "    yticklabels=label_names\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Multi-Column DistilBERT – Confusion Matrix (Validation)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c2b740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
