{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi‑Column RoBERTa Fine‑Tuning (no text concatenation)\n",
        "\n",
        "This notebook fine‑tunes **`roberta-base`** to predict **`experience_level`** (`junior` / `mid` / `senior`) from **`cleaned_resumes.csv`** (in the same folder).\n",
        "\n",
        "Core design choices:\n",
        "- Each CSV column is **tokenized separately** (we never concatenate columns into one long string).\n",
        "- Each column gets its own RoBERTa encoding, then we **aggregate column embeddings with column‑level self‑attention**.\n",
        "- Training prints accuracy + a confusion matrix for quick comparison between runs.\n",
        "\n",
        "Next cell: imports + experiment settings (hyperparameters, device, seeds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths / columns\n",
        "DATA_PATH = Path(\"cleaned_resumes.csv\")\n",
        "LABEL_COL = \"experience_level\"\n",
        "\n",
        "# Model / tokenization\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Training\n",
        "SEED = 42\n",
        "TRAIN_RATIO = 0.8\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"device:\", DEVICE)\n",
        "print(\"data:\", DATA_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the CSV and quickly sanity‑check it\n",
        "\n",
        "This cell reads `cleaned_resumes.csv`, verifies the label column (`experience_level`) exists, and prints a quick snapshot: number of rows, feature columns, label balance, and which columns are mostly empty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _is_empty(v: object) -> bool:\n",
        "    return v is None or str(v).strip() == \"\"\n",
        "\n",
        "\n",
        "def read_csv_rows(path: Path) -> tuple[list[str], list[dict[str, str]]]:\n",
        "    with path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if reader.fieldnames is None:\n",
        "            raise ValueError(\"CSV has no header row\")\n",
        "        rows = list(reader)\n",
        "        return list(reader.fieldnames), rows\n",
        "\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing file: {DATA_PATH.resolve()}\")\n",
        "\n",
        "columns, rows = read_csv_rows(DATA_PATH)\n",
        "\n",
        "if LABEL_COL not in columns:\n",
        "    raise ValueError(f\"Label column '{LABEL_COL}' not found. Columns: {columns}\")\n",
        "\n",
        "FEATURE_COLUMNS = [c for c in columns if c != LABEL_COL]\n",
        "\n",
        "labels = [str(r[LABEL_COL]).strip().lower() for r in rows]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "print(\"rows:\", len(rows))\n",
        "print(\"feature_columns (count):\", len(FEATURE_COLUMNS))\n",
        "print(\"feature_columns:\", FEATURE_COLUMNS)\n",
        "print(\"label_counts:\", dict(label_counts))\n",
        "\n",
        "empty_frac = {\n",
        "    c: sum(_is_empty(r.get(c, \"\")) for r in rows) / max(1, len(rows))\n",
        "    for c in columns\n",
        "}\n",
        "\n",
        "mostly_empty = sorted(empty_frac.items(), key=lambda kv: kv[1], reverse=True)[:8]\n",
        "print(\"\\nmost_empty_columns (top 8):\")\n",
        "for c, frac in mostly_empty:\n",
        "    print(f\"- {c}: {frac:.1%}\")\n",
        "\n",
        "print(\"\\nexample rows (label + 3 columns):\")\n",
        "preview_cols = FEATURE_COLUMNS[:3]\n",
        "for i in range(min(2, len(rows))):\n",
        "    r = rows[i]\n",
        "    preview = {c: str(r.get(c, \"\"))[:80] for c in preview_cols}\n",
        "    print(f\"[{i}] label={r[LABEL_COL]!r} preview={preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize *each column separately* and build tensors\n",
        "\n",
        "This cell initializes the tokenizer and tokenizes every feature column independently into `input_ids`/`attention_mask` with shape **[N, num_columns, MAX_LEN]**. It also creates a **stratified** train/validation split so class balance stays intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "label_names = sorted(label_counts.keys())\n",
        "label2id = {name: i for i, name in enumerate(label_names)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "y = torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n",
        "\n",
        "texts_by_col = {\n",
        "    c: [str(r.get(c, \"\") or \"\") for r in rows]\n",
        "    for c in FEATURE_COLUMNS\n",
        "}\n",
        "\n",
        "input_ids_cols = []\n",
        "attention_mask_cols = []\n",
        "\n",
        "for c in FEATURE_COLUMNS:\n",
        "    enc = tokenizer(\n",
        "        texts_by_col[c],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "    input_ids_cols.append(torch.tensor(enc[\"input_ids\"], dtype=torch.long))\n",
        "    attention_mask_cols.append(torch.tensor(enc[\"attention_mask\"], dtype=torch.long))\n",
        "\n",
        "input_ids = torch.stack(input_ids_cols, dim=1)\n",
        "attention_mask = torch.stack(attention_mask_cols, dim=1)\n",
        "\n",
        "print(\"input_ids:\", tuple(input_ids.shape))\n",
        "print(\"attention_mask:\", tuple(attention_mask.shape))\n",
        "print(\"labels:\", tuple(y.shape))\n",
        "\n",
        "rng = random.Random(SEED)\n",
        "indices_by_label = {name: [] for name in label_names}\n",
        "for i, lab in enumerate(labels):\n",
        "    indices_by_label[lab].append(i)\n",
        "\n",
        "train_idx: list[int] = []\n",
        "val_idx: list[int] = []\n",
        "\n",
        "for lab, idxs in indices_by_label.items():\n",
        "    rng.shuffle(idxs)\n",
        "    cut = int(len(idxs) * TRAIN_RATIO)\n",
        "    train_idx.extend(idxs[:cut])\n",
        "    val_idx.extend(idxs[cut:])\n",
        "\n",
        "rng.shuffle(train_idx)\n",
        "rng.shuffle(val_idx)\n",
        "\n",
        "train_input_ids = input_ids[train_idx]\n",
        "train_attention_mask = attention_mask[train_idx]\n",
        "train_y = y[train_idx]\n",
        "\n",
        "val_input_ids = input_ids[val_idx]\n",
        "val_attention_mask = attention_mask[val_idx]\n",
        "val_y = y[val_idx]\n",
        "\n",
        "print(\"\\ntrain size:\", len(train_idx), Counter([labels[i] for i in train_idx]))\n",
        "print(\"val size:\", len(val_idx), Counter([labels[i] for i in val_idx]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create datasets + dataloaders\n",
        "\n",
        "This cell wraps the tokenized tensors into `Dataset` objects and builds `DataLoader`s that return batches shaped **[batch, num_columns, MAX_LEN]** for both `input_ids` and `attention_mask`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResumeTensorDataset(Dataset):\n",
        "    def __init__(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(self.labels.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_ds = ResumeTensorDataset(train_input_ids, train_attention_mask, train_y)\n",
        "val_ds = ResumeTensorDataset(val_input_ids, val_attention_mask, val_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "batch_input_ids, batch_attention_mask, batch_labels = next(iter(train_loader))\n",
        "print(\"batch input_ids:\", tuple(batch_input_ids.shape))\n",
        "print(\"batch attention_mask:\", tuple(batch_attention_mask.shape))\n",
        "print(\"batch labels:\", tuple(batch_labels.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the multi‑column RoBERTa model (column‑level self‑attention)\n",
        "\n",
        "This cell defines a model that:\n",
        "- Runs RoBERTa on **all columns in one batched forward pass** (faster than looping columns)\n",
        "- Treats the resulting per‑column embeddings as a short sequence and applies **self‑attention across columns**\n",
        "- Pools across columns and predicts `experience_level`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ColumnSelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, hidden_size: int, num_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attn_out, _ = self.mha(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + self.drop(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.drop(ff_out))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiColumnRobertaClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_columns: int,\n",
        "        num_labels: int,\n",
        "        col_attn_heads: int = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
        "        hidden = int(self.encoder.config.hidden_size)\n",
        "\n",
        "        self.col_pos = nn.Embedding(num_columns, hidden)\n",
        "        self.col_block = ColumnSelfAttentionBlock(hidden, col_attn_heads, dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden // 2, num_labels),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        b, c, l = input_ids.shape\n",
        "\n",
        "        flat_ids = input_ids.view(b * c, l)\n",
        "        flat_mask = attention_mask.view(b * c, l)\n",
        "\n",
        "        out = self.encoder(input_ids=flat_ids, attention_mask=flat_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]\n",
        "        cols = cls.view(b, c, -1)\n",
        "\n",
        "        pos = self.col_pos(torch.arange(c, device=input_ids.device)).unsqueeze(0)\n",
        "        x = cols + pos\n",
        "        x = self.col_block(x)\n",
        "        pooled = x.mean(dim=1)\n",
        "\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "\n",
        "NUM_COLUMNS = int(train_input_ids.shape[1])\n",
        "NUM_LABELS = len(label2id)\n",
        "\n",
        "model = MultiColumnRobertaClassifier(\n",
        "    model_name=MODEL_NAME,\n",
        "    num_columns=NUM_COLUMNS,\n",
        "    num_labels=NUM_LABELS,\n",
        ").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_input_ids, batch_attention_mask, _ = next(iter(train_loader))\n",
        "    logits = model(batch_input_ids.to(DEVICE), batch_attention_mask.to(DEVICE))\n",
        "    print(\"logits:\", tuple(logits.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and validate\n",
        "\n",
        "This cell fine‑tunes the whole model end‑to‑end (RoBERTa + column‑attention layers), evaluates on the validation split each epoch, and saves the best checkpoint by validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 69\u001b[0m\n\u001b[1;32m     66\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(input_ids_b, attention_mask_b)\n\u001b[1;32m     67\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, labels_b)\n\u001b[0;32m---> 69\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), GRAD_CLIP_NORM)\n\u001b[1;32m     71\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    617\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    618\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "total_steps = max(1, EPOCHS * len(train_loader))\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(m: nn.Module, loader: DataLoader, num_labels: int):\n",
        "    m.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_n = 0\n",
        "    conf = torch.zeros((num_labels, num_labels), dtype=torch.long)\n",
        "\n",
        "    for input_ids_b, attention_mask_b, labels_b in loader:\n",
        "        input_ids_b = input_ids_b.to(DEVICE)\n",
        "        attention_mask_b = attention_mask_b.to(DEVICE)\n",
        "        labels_b = labels_b.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "            logits = m(input_ids_b, attention_mask_b)\n",
        "            loss = loss_fn(logits, labels_b)\n",
        "\n",
        "        total_loss += float(loss.item()) * int(labels_b.shape[0])\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += int((preds == labels_b).sum().item())\n",
        "        total_n += int(labels_b.shape[0])\n",
        "\n",
        "        for t, p in zip(labels_b.view(-1), preds.view(-1)):\n",
        "            conf[int(t), int(p)] += 1\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_n)\n",
        "    acc = total_correct / max(1, total_n)\n",
        "    return avg_loss, acc, conf\n",
        "\n",
        "\n",
        "BEST_CKPT_PATH = Path(\"best_multicolumn_roberta.pt\")\n",
        "\n",
        "best_val_acc = -1.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_n = 0\n",
        "\n",
        "    for input_ids_b, attention_mask_b, labels_b in train_loader:\n",
        "        input_ids_b = input_ids_b.to(DEVICE)\n",
        "        attention_mask_b = attention_mask_b.to(DEVICE)\n",
        "        labels_b = labels_b.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "            logits = model(input_ids_b, attention_mask_b)\n",
        "            loss = loss_fn(logits, labels_b)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += float(loss.item()) * int(labels_b.shape[0])\n",
        "        preds = logits.argmax(dim=1)\n",
        "        running_correct += int((preds == labels_b).sum().item())\n",
        "        running_n += int(labels_b.shape[0])\n",
        "\n",
        "    train_loss = running_loss / max(1, running_n)\n",
        "    train_acc = running_correct / max(1, running_n)\n",
        "\n",
        "    val_loss, val_acc, _ = evaluate(model, val_loader, NUM_LABELS)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} | \"\n",
        "        f\"val_loss={val_loss:.4f} val_acc={val_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"label2id\": label2id,\n",
        "                \"feature_columns\": FEATURE_COLUMNS,\n",
        "                \"model_name\": MODEL_NAME,\n",
        "                \"max_len\": MAX_LEN,\n",
        "            },\n",
        "            BEST_CKPT_PATH,\n",
        "        )\n",
        "\n",
        "print(\"\\nbest_val_acc:\", best_val_acc)\n",
        "print(\"saved:\", BEST_CKPT_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load best checkpoint + show confusion matrix\n",
        "\n",
        "This final cell reloads the best saved model (by validation accuracy), re-runs evaluation on the validation set, and visualizes the confusion matrix over `junior/mid/senior`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt = torch.load(BEST_CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "val_loss, val_acc, conf = evaluate(model, val_loader, NUM_LABELS)\n",
        "\n",
        "print(\"val_loss:\", val_loss)\n",
        "print(\"val_acc:\", val_acc)\n",
        "print(\"confusion_matrix:\\n\", conf)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "im = ax.imshow(conf.numpy(), cmap=\"Blues\")\n",
        "\n",
        "ax.set_xticks(range(NUM_LABELS))\n",
        "ax.set_yticks(range(NUM_LABELS))\n",
        "ax.set_xticklabels([id2label[i] for i in range(NUM_LABELS)], rotation=45, ha=\"right\")\n",
        "ax.set_yticklabels([id2label[i] for i in range(NUM_LABELS)])\n",
        "\n",
        "for i in range(NUM_LABELS):\n",
        "    for j in range(NUM_LABELS):\n",
        "        ax.text(j, i, int(conf[i, j]), ha=\"center\", va=\"center\")\n",
        "\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Multi‑Column RoBERTa – Confusion Matrix (Validation)\")\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
