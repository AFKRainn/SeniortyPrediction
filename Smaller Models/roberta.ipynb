{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi‑Column RoBERTa Fine‑Tuning (no text concatenation)\n",
        "\n",
        "This notebook fine‑tunes **`roberta-base`** to predict **`experience_level`** (`junior` / `mid` / `senior`) from **`cleaned_resumes.csv`** (in the same folder).\n",
        "\n",
        "Core design choices:\n",
        "- Each CSV column is **tokenized separately** (we never concatenate columns into one long string).\n",
        "- Each column gets its own RoBERTa encoding, then we **aggregate column embeddings with column‑level self‑attention**.\n",
        "- Training prints accuracy + a confusion matrix for quick comparison between runs.\n",
        "\n",
        "Next cell: imports + experiment settings (hyperparameters, device, seeds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\Rane\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n",
            "Skipping import of cpp extensions due to incompatible torch version 2.6.0+cu124 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "device: cuda\n",
            "data: C:\\Users\\Rane\\Desktop\\GenAI Baseline\\SeniortyPrediction\\Smaller Models\\cleaned_resumes.csv\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths / columns\n",
        "DATA_PATH = Path(\"cleaned_resumes.csv\")\n",
        "LABEL_COL = \"experience_level\"\n",
        "\n",
        "# Model / tokenization\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Training\n",
        "SEED = 42\n",
        "TRAIN_RATIO = 0.8\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"device:\", DEVICE)\n",
        "print(\"data:\", DATA_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the CSV and quickly sanity‑check it\n",
        "\n",
        "This cell reads `cleaned_resumes.csv`, verifies the label column (`experience_level`) exists, and prints a quick snapshot: number of rows, feature columns, label balance, and which columns are mostly empty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rows: 2100\n",
            "feature_columns (count): 16\n",
            "feature_columns: ['name', 'email', 'summary', 'linkedin', 'github', 'experience', 'education', 'skills', 'projects', 'certifications', 'summary_count', 'last_experience_only', 'total_experience_time', 'last_experience_time', 'job title', 'target_experience_text']\n",
            "label_counts: {'senior': 700, 'mid': 700, 'junior': 700}\n",
            "\n",
            "most_empty_columns (top 8):\n",
            "- name: 0.0%\n",
            "- email: 0.0%\n",
            "- summary: 0.0%\n",
            "- linkedin: 0.0%\n",
            "- github: 0.0%\n",
            "- experience: 0.0%\n",
            "- education: 0.0%\n",
            "- skills: 0.0%\n",
            "\n",
            "example rows (label + 3 columns):\n",
            "[0] label='senior' preview={'name': 'Brenda Garza', 'email': 'williamsrichard@example.org', 'summary': 'Passionate Deep Learning Engineer with expertise in neural network design, train'}\n",
            "[1] label='senior' preview={'name': 'Michele Clark', 'email': 'kennethpark@example.org', 'summary': 'Blockchain Developer with experience in smart contract development and integrati'}\n"
          ]
        }
      ],
      "source": [
        "def _is_empty(v: object) -> bool:\n",
        "    return v is None or str(v).strip() == \"\"\n",
        "\n",
        "\n",
        "def read_csv_rows(path: Path) -> tuple[list[str], list[dict[str, str]]]:\n",
        "    with path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if reader.fieldnames is None:\n",
        "            raise ValueError(\"CSV has no header row\")\n",
        "        rows = list(reader)\n",
        "        return list(reader.fieldnames), rows\n",
        "\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing file: {DATA_PATH.resolve()}\")\n",
        "\n",
        "columns, rows = read_csv_rows(DATA_PATH)\n",
        "\n",
        "if LABEL_COL not in columns:\n",
        "    raise ValueError(f\"Label column '{LABEL_COL}' not found. Columns: {columns}\")\n",
        "\n",
        "FEATURE_COLUMNS = [c for c in columns if c != LABEL_COL]\n",
        "\n",
        "labels = [str(r[LABEL_COL]).strip().lower() for r in rows]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "print(\"rows:\", len(rows))\n",
        "print(\"feature_columns (count):\", len(FEATURE_COLUMNS))\n",
        "print(\"feature_columns:\", FEATURE_COLUMNS)\n",
        "print(\"label_counts:\", dict(label_counts))\n",
        "\n",
        "empty_frac = {\n",
        "    c: sum(_is_empty(r.get(c, \"\")) for r in rows) / max(1, len(rows))\n",
        "    for c in columns\n",
        "}\n",
        "\n",
        "mostly_empty = sorted(empty_frac.items(), key=lambda kv: kv[1], reverse=True)[:8]\n",
        "print(\"\\nmost_empty_columns (top 8):\")\n",
        "for c, frac in mostly_empty:\n",
        "    print(f\"- {c}: {frac:.1%}\")\n",
        "\n",
        "print(\"\\nexample rows (label + 3 columns):\")\n",
        "preview_cols = FEATURE_COLUMNS[:3]\n",
        "for i in range(min(2, len(rows))):\n",
        "    r = rows[i]\n",
        "    preview = {c: str(r.get(c, \"\"))[:80] for c in preview_cols}\n",
        "    print(f\"[{i}] label={r[LABEL_COL]!r} preview={preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize *each column separately* and build tensors\n",
        "\n",
        "This cell initializes the tokenizer and tokenizes every feature column independently into `input_ids`/`attention_mask` with shape **[N, num_columns, MAX_LEN]**. It also creates a **stratified** train/validation split so class balance stays intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "358f2c48f45746539648f5335e23fd31",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "87e1c134ae1f4c7da0303907e3fa76cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9790b981916f4abd8c74c4e1d6965b16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "201cc0d0f4e648f692217ce7cc9263b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c7776234bb74c6a91fbc018dc94aa3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids: (2100, 16, 128)\n",
            "attention_mask: (2100, 16, 128)\n",
            "labels: (2100,)\n",
            "\n",
            "train size: 1680 Counter({'junior': 560, 'mid': 560, 'senior': 560})\n",
            "val size: 420 Counter({'senior': 140, 'mid': 140, 'junior': 140})\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "label_names = sorted(label_counts.keys())\n",
        "label2id = {name: i for i, name in enumerate(label_names)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "y = torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n",
        "\n",
        "texts_by_col = {\n",
        "    c: [str(r.get(c, \"\") or \"\") for r in rows]\n",
        "    for c in FEATURE_COLUMNS\n",
        "}\n",
        "\n",
        "input_ids_cols = []\n",
        "attention_mask_cols = []\n",
        "\n",
        "for c in FEATURE_COLUMNS:\n",
        "    enc = tokenizer(\n",
        "        texts_by_col[c],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "    input_ids_cols.append(torch.tensor(enc[\"input_ids\"], dtype=torch.long))\n",
        "    attention_mask_cols.append(torch.tensor(enc[\"attention_mask\"], dtype=torch.long))\n",
        "\n",
        "input_ids = torch.stack(input_ids_cols, dim=1)\n",
        "attention_mask = torch.stack(attention_mask_cols, dim=1)\n",
        "\n",
        "print(\"input_ids:\", tuple(input_ids.shape))\n",
        "print(\"attention_mask:\", tuple(attention_mask.shape))\n",
        "print(\"labels:\", tuple(y.shape))\n",
        "\n",
        "rng = random.Random(SEED)\n",
        "indices_by_label = {name: [] for name in label_names}\n",
        "for i, lab in enumerate(labels):\n",
        "    indices_by_label[lab].append(i)\n",
        "\n",
        "train_idx: list[int] = []\n",
        "val_idx: list[int] = []\n",
        "\n",
        "for lab, idxs in indices_by_label.items():\n",
        "    rng.shuffle(idxs)\n",
        "    cut = int(len(idxs) * TRAIN_RATIO)\n",
        "    train_idx.extend(idxs[:cut])\n",
        "    val_idx.extend(idxs[cut:])\n",
        "\n",
        "rng.shuffle(train_idx)\n",
        "rng.shuffle(val_idx)\n",
        "\n",
        "train_input_ids = input_ids[train_idx]\n",
        "train_attention_mask = attention_mask[train_idx]\n",
        "train_y = y[train_idx]\n",
        "\n",
        "val_input_ids = input_ids[val_idx]\n",
        "val_attention_mask = attention_mask[val_idx]\n",
        "val_y = y[val_idx]\n",
        "\n",
        "print(\"\\ntrain size:\", len(train_idx), Counter([labels[i] for i in train_idx]))\n",
        "print(\"val size:\", len(val_idx), Counter([labels[i] for i in val_idx]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create datasets + dataloaders\n",
        "\n",
        "This cell wraps the tokenized tensors into `Dataset` objects and builds `DataLoader`s that return batches shaped **[batch, num_columns, MAX_LEN]** for both `input_ids` and `attention_mask`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "batch input_ids: (4, 16, 128)\n",
            "batch attention_mask: (4, 16, 128)\n",
            "batch labels: (4,)\n"
          ]
        }
      ],
      "source": [
        "class ResumeTensorDataset(Dataset):\n",
        "    def __init__(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(self.labels.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_ds = ResumeTensorDataset(train_input_ids, train_attention_mask, train_y)\n",
        "val_ds = ResumeTensorDataset(val_input_ids, val_attention_mask, val_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "batch_input_ids, batch_attention_mask, batch_labels = next(iter(train_loader))\n",
        "print(\"batch input_ids:\", tuple(batch_input_ids.shape))\n",
        "print(\"batch attention_mask:\", tuple(batch_attention_mask.shape))\n",
        "print(\"batch labels:\", tuple(batch_labels.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define the multi‑column RoBERTa model (column‑level self‑attention)\n",
        "\n",
        "This cell defines a model that:\n",
        "- Runs RoBERTa on **all columns in one batched forward pass** (faster than looping columns)\n",
        "- Treats the resulting per‑column embeddings as a short sequence and applies **self‑attention across columns**\n",
        "- Pools across columns and predicts `experience_level`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fcd22e298bf8429bac1bb3b4f7e293e2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "logits: (4, 3)\n"
          ]
        }
      ],
      "source": [
        "class ColumnSelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, hidden_size: int, num_heads: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "        )\n",
        "        self.ln1 = nn.LayerNorm(hidden_size)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_size * 4, hidden_size),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(hidden_size)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        attn_out, _ = self.mha(x, x, x, need_weights=False)\n",
        "        x = self.ln1(x + self.drop(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.drop(ff_out))\n",
        "        return x\n",
        "\n",
        "\n",
        "class MultiColumnRobertaClassifier(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name: str,\n",
        "        num_columns: int,\n",
        "        num_labels: int,\n",
        "        col_attn_heads: int = 4,\n",
        "        dropout: float = 0.1,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = RobertaModel.from_pretrained(model_name)\n",
        "        hidden = int(self.encoder.config.hidden_size)\n",
        "\n",
        "        self.col_pos = nn.Embedding(num_columns, hidden)\n",
        "        self.col_block = ColumnSelfAttentionBlock(hidden, col_attn_heads, dropout)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, hidden // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden // 2, num_labels),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        b, c, l = input_ids.shape\n",
        "\n",
        "        flat_ids = input_ids.view(b * c, l)\n",
        "        flat_mask = attention_mask.view(b * c, l)\n",
        "\n",
        "        out = self.encoder(input_ids=flat_ids, attention_mask=flat_mask)\n",
        "        cls = out.last_hidden_state[:, 0, :]\n",
        "        cols = cls.view(b, c, -1)\n",
        "\n",
        "        pos = self.col_pos(torch.arange(c, device=input_ids.device)).unsqueeze(0)\n",
        "        x = cols + pos\n",
        "        x = self.col_block(x)\n",
        "        pooled = x.mean(dim=1)\n",
        "\n",
        "        return self.classifier(pooled)\n",
        "\n",
        "\n",
        "NUM_COLUMNS = int(train_input_ids.shape[1])\n",
        "NUM_LABELS = len(label2id)\n",
        "\n",
        "model = MultiColumnRobertaClassifier(\n",
        "    model_name=MODEL_NAME,\n",
        "    num_columns=NUM_COLUMNS,\n",
        "    num_labels=NUM_LABELS,\n",
        ").to(DEVICE)\n",
        "\n",
        "with torch.no_grad():\n",
        "    batch_input_ids, batch_attention_mask, _ = next(iter(train_loader))\n",
        "    logits = model(batch_input_ids.to(DEVICE), batch_attention_mask.to(DEVICE))\n",
        "    print(\"logits:\", tuple(logits.shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train and validate\n",
        "\n",
        "This cell fine‑tunes the whole model end‑to‑end (RoBERTa + column‑attention layers), evaluates on the validation split each epoch, and saves the best checkpoint by validation accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Rane\\AppData\\Local\\Temp/ipykernel_13088/2619655549.py:13: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
            "C:\\Users\\Rane\\AppData\\Local\\Temp/ipykernel_13088/2619655549.py:65: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
            "C:\\Users\\Rane\\AppData\\Local\\Temp/ipykernel_13088/2619655549.py:30: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 | train_loss=1.1033 train_acc=0.319 | val_loss=1.1018 val_acc=0.333\n",
            "Epoch 2/5 | train_loss=1.1024 train_acc=0.314 | val_loss=1.1008 val_acc=0.333\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13088/2619655549.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mrunning_correct\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "total_steps = max(1, EPOCHS * len(train_loader))\n",
        "warmup_steps = int(WARMUP_RATIO * total_steps)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps,\n",
        ")\n",
        "\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(m: nn.Module, loader: DataLoader, num_labels: int):\n",
        "    m.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_n = 0\n",
        "    conf = torch.zeros((num_labels, num_labels), dtype=torch.long)\n",
        "\n",
        "    for input_ids_b, attention_mask_b, labels_b in loader:\n",
        "        input_ids_b = input_ids_b.to(DEVICE)\n",
        "        attention_mask_b = attention_mask_b.to(DEVICE)\n",
        "        labels_b = labels_b.to(DEVICE)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "            logits = m(input_ids_b, attention_mask_b)\n",
        "            loss = loss_fn(logits, labels_b)\n",
        "\n",
        "        total_loss += float(loss.item()) * int(labels_b.shape[0])\n",
        "        preds = logits.argmax(dim=1)\n",
        "        total_correct += int((preds == labels_b).sum().item())\n",
        "        total_n += int(labels_b.shape[0])\n",
        "\n",
        "        for t, p in zip(labels_b.view(-1), preds.view(-1)):\n",
        "            conf[int(t), int(p)] += 1\n",
        "\n",
        "    avg_loss = total_loss / max(1, total_n)\n",
        "    acc = total_correct / max(1, total_n)\n",
        "    return avg_loss, acc, conf\n",
        "\n",
        "\n",
        "BEST_CKPT_PATH = Path(\"best_multicolumn_roberta.pt\")\n",
        "\n",
        "best_val_acc = -1.0\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_correct = 0\n",
        "    running_n = 0\n",
        "\n",
        "    for input_ids_b, attention_mask_b, labels_b in train_loader:\n",
        "        input_ids_b = input_ids_b.to(DEVICE)\n",
        "        attention_mask_b = attention_mask_b.to(DEVICE)\n",
        "        labels_b = labels_b.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=scaler.is_enabled()):\n",
        "            logits = model(input_ids_b, attention_mask_b)\n",
        "            loss = loss_fn(logits, labels_b)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        running_loss += float(loss.item()) * int(labels_b.shape[0])\n",
        "        preds = logits.argmax(dim=1)\n",
        "        running_correct += int((preds == labels_b).sum().item())\n",
        "        running_n += int(labels_b.shape[0])\n",
        "\n",
        "    train_loss = running_loss / max(1, running_n)\n",
        "    train_acc = running_correct / max(1, running_n)\n",
        "\n",
        "    val_loss, val_acc, _ = evaluate(model, val_loader, NUM_LABELS)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch}/{EPOCHS} | \"\n",
        "        f\"train_loss={train_loss:.4f} train_acc={train_acc:.3f} | \"\n",
        "        f\"val_loss={val_loss:.4f} val_acc={val_acc:.3f}\"\n",
        "    )\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(\n",
        "            {\n",
        "                \"model_state_dict\": model.state_dict(),\n",
        "                \"label2id\": label2id,\n",
        "                \"feature_columns\": FEATURE_COLUMNS,\n",
        "                \"model_name\": MODEL_NAME,\n",
        "                \"max_len\": MAX_LEN,\n",
        "            },\n",
        "            BEST_CKPT_PATH,\n",
        "        )\n",
        "\n",
        "print(\"\\nbest_val_acc:\", best_val_acc)\n",
        "print(\"saved:\", BEST_CKPT_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load best checkpoint + show confusion matrix\n",
        "\n",
        "This final cell reloads the best saved model (by validation accuracy), re-runs evaluation on the validation set, and visualizes the confusion matrix over `junior/mid/senior`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ckpt = torch.load(BEST_CKPT_PATH, map_location=DEVICE)\n",
        "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "val_loss, val_acc, conf = evaluate(model, val_loader, NUM_LABELS)\n",
        "\n",
        "print(\"val_loss:\", val_loss)\n",
        "print(\"val_acc:\", val_acc)\n",
        "print(\"confusion_matrix:\\n\", conf)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "im = ax.imshow(conf.numpy(), cmap=\"Blues\")\n",
        "\n",
        "ax.set_xticks(range(NUM_LABELS))\n",
        "ax.set_yticks(range(NUM_LABELS))\n",
        "ax.set_xticklabels([id2label[i] for i in range(NUM_LABELS)], rotation=45, ha=\"right\")\n",
        "ax.set_yticklabels([id2label[i] for i in range(NUM_LABELS)])\n",
        "\n",
        "for i in range(NUM_LABELS):\n",
        "    for j in range(NUM_LABELS):\n",
        "        ax.text(j, i, int(conf[i, j]), ha=\"center\", va=\"center\")\n",
        "\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"True\")\n",
        "ax.set_title(\"Multi‑Column RoBERTa – Confusion Matrix (Validation)\")\n",
        "fig.colorbar(im, ax=ax)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
