{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi‑Column RoBERTa Fine‑Tuning (no text concatenation)\n",
        "\n",
        "This notebook fine‑tunes **`roberta-base`** to predict **`experience_level`** (`junior` / `mid` / `senior`) from **`cleaned_resumes.csv`** (in the same folder).\n",
        "\n",
        "Core design choices:\n",
        "- Each CSV column is **tokenized separately** (we never concatenate columns into one long string).\n",
        "- Each column gets its own RoBERTa encoding, then we **aggregate column embeddings with column‑level self‑attention**.\n",
        "- Training prints accuracy + a confusion matrix for quick comparison between runs.\n",
        "\n",
        "Next cell: imports + experiment settings (hyperparameters, device, seeds).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import AutoTokenizer, RobertaModel, get_linear_schedule_with_warmup\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths / columns\n",
        "DATA_PATH = Path(\"cleaned_resumes.csv\")\n",
        "LABEL_COL = \"experience_level\"\n",
        "\n",
        "# Model / tokenization\n",
        "MODEL_NAME = \"roberta-base\"\n",
        "MAX_LEN = 128\n",
        "\n",
        "# Training\n",
        "SEED = 42\n",
        "TRAIN_RATIO = 0.8\n",
        "BATCH_SIZE = 4\n",
        "EPOCHS = 5\n",
        "LR = 2e-5\n",
        "WEIGHT_DECAY = 0.01\n",
        "WARMUP_RATIO = 0.1\n",
        "GRAD_CLIP_NORM = 1.0\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "print(\"device:\", DEVICE)\n",
        "print(\"data:\", DATA_PATH.resolve())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the CSV and quickly sanity‑check it\n",
        "\n",
        "This cell reads `cleaned_resumes.csv`, verifies the label column (`experience_level`) exists, and prints a quick snapshot: number of rows, feature columns, label balance, and which columns are mostly empty.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _is_empty(v: object) -> bool:\n",
        "    return v is None or str(v).strip() == \"\"\n",
        "\n",
        "\n",
        "def read_csv_rows(path: Path) -> tuple[list[str], list[dict[str, str]]]:\n",
        "    with path.open(newline=\"\", encoding=\"utf-8\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        if reader.fieldnames is None:\n",
        "            raise ValueError(\"CSV has no header row\")\n",
        "        rows = list(reader)\n",
        "        return list(reader.fieldnames), rows\n",
        "\n",
        "\n",
        "if not DATA_PATH.exists():\n",
        "    raise FileNotFoundError(f\"Missing file: {DATA_PATH.resolve()}\")\n",
        "\n",
        "columns, rows = read_csv_rows(DATA_PATH)\n",
        "\n",
        "if LABEL_COL not in columns:\n",
        "    raise ValueError(f\"Label column '{LABEL_COL}' not found. Columns: {columns}\")\n",
        "\n",
        "FEATURE_COLUMNS = [c for c in columns if c != LABEL_COL]\n",
        "\n",
        "labels = [str(r[LABEL_COL]).strip().lower() for r in rows]\n",
        "label_counts = Counter(labels)\n",
        "\n",
        "print(\"rows:\", len(rows))\n",
        "print(\"feature_columns (count):\", len(FEATURE_COLUMNS))\n",
        "print(\"feature_columns:\", FEATURE_COLUMNS)\n",
        "print(\"label_counts:\", dict(label_counts))\n",
        "\n",
        "empty_frac = {\n",
        "    c: sum(_is_empty(r.get(c, \"\")) for r in rows) / max(1, len(rows))\n",
        "    for c in columns\n",
        "}\n",
        "\n",
        "mostly_empty = sorted(empty_frac.items(), key=lambda kv: kv[1], reverse=True)[:8]\n",
        "print(\"\\nmost_empty_columns (top 8):\")\n",
        "for c, frac in mostly_empty:\n",
        "    print(f\"- {c}: {frac:.1%}\")\n",
        "\n",
        "print(\"\\nexample rows (label + 3 columns):\")\n",
        "preview_cols = FEATURE_COLUMNS[:3]\n",
        "for i in range(min(2, len(rows))):\n",
        "    r = rows[i]\n",
        "    preview = {c: str(r.get(c, \"\"))[:80] for c in preview_cols}\n",
        "    print(f\"[{i}] label={r[LABEL_COL]!r} preview={preview}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenize *each column separately* and build tensors\n",
        "\n",
        "This cell initializes the tokenizer and tokenizes every feature column independently into `input_ids`/`attention_mask` with shape **[N, num_columns, MAX_LEN]**. It also creates a **stratified** train/validation split so class balance stays intact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "\n",
        "label_names = sorted(label_counts.keys())\n",
        "label2id = {name: i for i, name in enumerate(label_names)}\n",
        "id2label = {i: name for name, i in label2id.items()}\n",
        "\n",
        "y = torch.tensor([label2id[l] for l in labels], dtype=torch.long)\n",
        "\n",
        "texts_by_col = {\n",
        "    c: [str(r.get(c, \"\") or \"\") for r in rows]\n",
        "    for c in FEATURE_COLUMNS\n",
        "}\n",
        "\n",
        "input_ids_cols = []\n",
        "attention_mask_cols = []\n",
        "\n",
        "for c in FEATURE_COLUMNS:\n",
        "    enc = tokenizer(\n",
        "        texts_by_col[c],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "    input_ids_cols.append(torch.tensor(enc[\"input_ids\"], dtype=torch.long))\n",
        "    attention_mask_cols.append(torch.tensor(enc[\"attention_mask\"], dtype=torch.long))\n",
        "\n",
        "input_ids = torch.stack(input_ids_cols, dim=1)\n",
        "attention_mask = torch.stack(attention_mask_cols, dim=1)\n",
        "\n",
        "print(\"input_ids:\", tuple(input_ids.shape))\n",
        "print(\"attention_mask:\", tuple(attention_mask.shape))\n",
        "print(\"labels:\", tuple(y.shape))\n",
        "\n",
        "rng = random.Random(SEED)\n",
        "indices_by_label = {name: [] for name in label_names}\n",
        "for i, lab in enumerate(labels):\n",
        "    indices_by_label[lab].append(i)\n",
        "\n",
        "train_idx: list[int] = []\n",
        "val_idx: list[int] = []\n",
        "\n",
        "for lab, idxs in indices_by_label.items():\n",
        "    rng.shuffle(idxs)\n",
        "    cut = int(len(idxs) * TRAIN_RATIO)\n",
        "    train_idx.extend(idxs[:cut])\n",
        "    val_idx.extend(idxs[cut:])\n",
        "\n",
        "rng.shuffle(train_idx)\n",
        "rng.shuffle(val_idx)\n",
        "\n",
        "train_input_ids = input_ids[train_idx]\n",
        "train_attention_mask = attention_mask[train_idx]\n",
        "train_y = y[train_idx]\n",
        "\n",
        "val_input_ids = input_ids[val_idx]\n",
        "val_attention_mask = attention_mask[val_idx]\n",
        "val_y = y[val_idx]\n",
        "\n",
        "print(\"\\ntrain size:\", len(train_idx), Counter([labels[i] for i in train_idx]))\n",
        "print(\"val size:\", len(val_idx), Counter([labels[i] for i in val_idx]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create datasets + dataloaders\n",
        "\n",
        "This cell wraps the tokenized tensors into `Dataset` objects and builds `DataLoader`s that return batches shaped **[batch, num_columns, MAX_LEN]** for both `input_ids` and `attention_mask`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResumeTensorDataset(Dataset):\n",
        "    def __init__(self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: torch.Tensor):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return int(self.labels.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        return self.input_ids[idx], self.attention_mask[idx], self.labels[idx]\n",
        "\n",
        "\n",
        "train_ds = ResumeTensorDataset(train_input_ids, train_attention_mask, train_y)\n",
        "val_ds = ResumeTensorDataset(val_input_ids, val_attention_mask, val_y)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "batch_input_ids, batch_attention_mask, batch_labels = next(iter(train_loader))\n",
        "print(\"batch input_ids:\", tuple(batch_input_ids.shape))\n",
        "print(\"batch attention_mask:\", tuple(batch_attention_mask.shape))\n",
        "print(\"batch labels:\", tuple(batch_labels.shape))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
