{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune DistilBERT & RoBERTa for Resume Classification\n",
    "\n",
    "This notebook finetunes both models on `cleaned_resumes.csv` and **saves them properly** for use in Test 2.\n",
    "\n",
    "**Output directories:**\n",
    "- `distilbert_resume_level/` - DistilBERT model + tokenizer\n",
    "- `roberta_resume_level/` - RoBERTa model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv(\"../cleaned_resumes.csv\", engine=\"python\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Target distribution:\\n{df['experience_level'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare text field\n",
    "TARGET_COL = \"experience_level\"\n",
    "\n",
    "def clean_value(v):\n",
    "    if pd.isna(v):\n",
    "        return \"\"\n",
    "    s = str(v)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def row_to_text(row, target_col):\n",
    "    parts = []\n",
    "    for col, val in row.items():\n",
    "        if col == target_col:\n",
    "            continue\n",
    "        s = clean_value(val)\n",
    "        if s:\n",
    "            parts.append(f\"[{col}] {s}\")\n",
    "    return \" \".join(parts)\n",
    "\n",
    "df[\"text\"] = df.apply(lambda r: row_to_text(r, TARGET_COL), axis=1)\n",
    "print(f\"Average text length: {int(df['text'].str.len().mean())} chars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label mappings\n",
    "labels = sorted(df[TARGET_COL].dropna().unique().tolist())\n",
    "label2id = {l: i for i, l in enumerate(labels)}\n",
    "id2label = {i: l for l, i in label2id.items()}\n",
    "df[\"label\"] = df[TARGET_COL].map(label2id)\n",
    "\n",
    "print(f\"Labels: {labels}\")\n",
    "print(f\"label2id: {label2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "print(f\"Train: {train_df.shape[0]}, Test: {test_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResumeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, y_true = eval_pred\n",
    "    y_pred = np.argmax(logits, axis=1)\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(model_name, output_dir, num_epochs=5):\n",
    "    \"\"\"\n",
    "    Train a model and save both weights and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_name: HuggingFace model name (e.g., 'distilbert-base-uncased')\n",
    "        output_dir: Directory to save the model\n",
    "        num_epochs: Number of training epochs\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {model_name}\")\n",
    "    print(f\"Output: {output_dir}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Tokenize data\n",
    "    MAX_LEN = 512\n",
    "    def tokenize(texts):\n",
    "        return tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "    \n",
    "    train_enc = tokenize(train_df[\"text\"].tolist())\n",
    "    test_enc = tokenize(test_df[\"text\"].tolist())\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = ResumeDataset(train_enc, train_df[\"label\"].tolist())\n",
    "    test_dataset = ResumeDataset(test_enc, test_df[\"label\"].tolist())\n",
    "    \n",
    "    # Load model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    \n",
    "    # Training arguments\n",
    "    args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1_macro\",\n",
    "        learning_rate=5e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=32,\n",
    "        num_train_epochs=num_epochs,\n",
    "        logging_steps=50,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    # Create trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nStarting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # IMPORTANT: Save model weights AND tokenizer\n",
    "    print(f\"\\nSaving model to {output_dir}...\")\n",
    "    trainer.save_model(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Verify files were saved\n",
    "    saved_files = os.listdir(output_dir)\n",
    "    print(f\"Saved files: {saved_files}\")\n",
    "    \n",
    "    # Check for model weights\n",
    "    has_weights = any(f.endswith('.bin') or f.endswith('.safetensors') for f in saved_files)\n",
    "    if has_weights:\n",
    "        print(\"Model weights saved successfully!\")\n",
    "    else:\n",
    "        print(\"WARNING: Model weights may not have been saved!\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(\"\\nFinal evaluation on test set:\")\n",
    "    results = trainer.evaluate()\n",
    "    print(f\"Accuracy: {results['eval_accuracy']:.4f}\")\n",
    "    print(f\"F1 Macro: {results['eval_f1_macro']:.4f}\")\n",
    "    \n",
    "    return trainer, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_trainer, distilbert_results = train_and_save_model(\n",
    "    model_name=\"distilbert-base-uncased\",\n",
    "    output_dir=\"distilbert_resume_level\",\n",
    "    num_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_trainer, roberta_results = train_and_save_model(\n",
    "    model_name=\"roberta-base\",\n",
    "    output_dir=\"roberta_resume_level\",\n",
    "    num_epochs=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Verify Models Can Be Loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying models can be loaded...\\n\")\n",
    "\n",
    "for name, path in [(\"DistilBERT\", \"distilbert_resume_level\"), (\"RoBERTa\", \"roberta_resume_level\")]:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(path)\n",
    "        print(f\"{name}: Loaded successfully!\")\n",
    "        print(f\"  - Config: {model.config.id2label}\")\n",
    "    except Exception as e:\n",
    "        print(f\"{name}: FAILED to load - {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDistilBERT:\")\n",
    "print(f\"  Accuracy: {distilbert_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {distilbert_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Saved to: distilbert_resume_level/\")\n",
    "\n",
    "print(f\"\\nRoBERTa:\")\n",
    "print(f\"  Accuracy: {roberta_results['eval_accuracy']:.4f}\")\n",
    "print(f\"  F1 Macro: {roberta_results['eval_f1_macro']:.4f}\")\n",
    "print(f\"  Saved to: roberta_resume_level/\")\n",
    "\n",
    "print(\"\\nModels are ready for Test 2!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
